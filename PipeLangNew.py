#!/usr/bin/env python3
"""
PipeHaystack_LangGraph v3.0 - Orquestra√ß√£o 100% LangGraph

FILOSOFIA:
- LangGraph gerencia ITERA√á√ïES (discovery ‚Üí scrape ‚Üí reduce ‚Üí analyze ‚Üí judge)
- Pipe gerencia apenas CICLO DE FASES (criar novas fases quando Judge decide)
- N√≥s s√£o WRAPPERS FINOS que delegam para c√≥digo existente
- Router √© PURO (decis√£o baseada apenas em state, sem side-effects)

ESTRUTURA:
1. State TypedDict: Estado COMPLETO da pesquisa (todos os campos do Orchestrator)
2. N√≥s LangGraph: Wrappers para discovery, scrape, reduce, analyze, judge
3. Router: Decis√£o pura baseada em state (done/refine/new_phase)
4. Pipe: Wrapper OpenWebUI (gerencia fases, n√£o loops)
"""

import asyncio
import json
import logging
import time
import uuid
import hashlib
import os
import re
import numpy as np
from dataclasses import dataclass, field, asdict
from threading import Lock
from typing import Any, AsyncGenerator, Callable, Dict, List, Optional, TypedDict, Literal, Tuple
from datetime import datetime

# LangGraph imports (conditional)
try:
    from langgraph.graph import StateGraph as LG_StateGraph, END as LG_END
    from langgraph.checkpoint.memory import MemorySaver as LG_MemorySaver
    LANGGRAPH_AVAILABLE = True
    # Alias to unified names (type ignores to satisfy static checker across branches)
    StateGraph = LG_StateGraph  # type: ignore[assignment]
    MemorySaver = LG_MemorySaver  # type: ignore[assignment]
    END = LG_END  # type: ignore[assignment]
except ImportError:
    LANGGRAPH_AVAILABLE = False
    # Fallback for when LangGraph is not available
    class StateGraph:
        def __init__(self, *args, **kwargs):
            # Called without args in fallback usage
            pass

        def add_node(self, *args, **kwargs):
            return None

        def set_entry_point(self, *args, **kwargs):
            return None

        def add_edge(self, *args, **kwargs):
            return None

        def add_conditional_edges(self, *args, **kwargs):
            return None

        def compile(self, *args, **kwargs):
            return self

        async def ainvoke(self, *args, **kwargs):
            raise ImportError("LangGraph not available. Install with: pip install langgraph")

    class MemorySaver:
        pass

    END = "END"

import httpx
from pydantic import BaseModel, Field, validator

# Conditional imports
try:
    from datasketch import MinHash, MinHashLSH
except ImportError:
    pass

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    pass

try:
    from haystack import Document
    from haystack.components.embedders import SentenceTransformersDocumentEmbedder
    HAYSTACK_AVAILABLE = True
except ImportError:
    HAYSTACK_AVAILABLE = False

logger = logging.getLogger(__name__)

# Global LLM cache
_LLM_CACHE: Dict[str, Optional['AsyncOpenAIClient']] = {}
_LLM_CACHE_LOCK: Lock = Lock()


# ============================================================================
# INFRASTRUCTURE CLASSES AND FUNCTIONS
# ============================================================================

@dataclass
class StepTelemetry:
    """Telemetria padronizada por etapa"""
    step: str  # "discovery", "scraper", "analyst", "judge", "synthesis"
    correlation_id: str
    start_ms: float
    end_ms: Optional[float] = None
    inputs_brief: str = ""
    outputs_brief: str = ""
    counters: Optional[Dict[str, int]] = None
    notes: Optional[List[str]] = None
    
    @property
    def elapsed_ms(self) -> float:
        if self.end_ms:
            return self.end_ms - self.start_ms
        return time.time() * 1000 - self.start_ms
    
    def to_dict(self) -> Dict:
        return {
            "step": self.step,
            "correlation_id": self.correlation_id,
            "start_ms": self.start_ms,
            "end_ms": self.end_ms,
            "elapsed_ms": self.elapsed_ms,
            "inputs": self.inputs_brief,
            "outputs": self.outputs_brief,
            "counters": self.counters or {},
            "notes": self.notes or [],
        }


def _emit_decision_snapshot(step: str, vector: Dict[str, Any], reason: str = "", contract_diff: Optional[Dict[str, Any]] = None) -> None:
    """Emit structured decision snapshot without modifying StepTelemetry.
    Safe to call even if values are large; truncates where appropriate.
    """
    try:
        # Lightweight truncation for large fields
        safe_vector = {}
        for k, v in (vector or {}).items():
            if isinstance(v, str) and len(v) > 800:
                safe_vector[k] = v[:800] + "‚Ä¶"
            else:
                safe_vector[k] = v
        payload = {
            "step": step,
            "decision_vector": safe_vector,
            "decision_reason": reason,
            "contract_diff": contract_diff or {},
            "ts": datetime.utcnow().isoformat() + "Z",
        }
        logger.info(f"[DECISION]{json.dumps(payload, ensure_ascii=False)}")
    except Exception as e:
        logger.debug(f"Failed to emit decision snapshot: {e}")


async def _safe_emit(emitter: Optional[Callable], payload: str) -> None:
    """Safely emit payload via optional callable. Supports sync or async emitters."""
    if not emitter:
        return
    try:
        res = emitter(payload)
        # Await if returns an awaitable (async emitter)
        import inspect
        if inspect.isawaitable(res):
            await res
    except Exception:
        # Never break pipeline due to emitter failures
        pass


# ==================== CUSTOM EXCEPTIONS ====================
# Exce√ß√µes espec√≠ficas para melhor rastreabilidade e recovery strategies

class PipeExecutionError(Exception):
    """Erro base para execu√ß√£o do Pipe - permite captura global de erros do pipeline"""
    pass


class LLMConfigurationError(PipeExecutionError):
    """Raised when LLM client configuration is missing or invalid"""
    pass


class ContractValidationError(PipeExecutionError):
    """Erro na valida√ß√£o do contract - estrutura inv√°lida ou incompat√≠vel"""
    pass


class ContractGenerationError(PipeExecutionError):
    """Erro na gera√ß√£o do contract pelo LLM Planner"""
    pass


class ToolExecutionError(PipeExecutionError):
    """Erro na execu√ß√£o de ferramentas (discovery, scraper, context_reducer)"""
    pass


# ==================== PYDANTIC MODELS FOR CONTRACT VALIDATION ====================
# Valida√ß√£o formal de contracts para prevenir estruturas inv√°lidas

# ===== CONTRACT SCHEMAS (Fim-a-fim) =====

class EntitiesModel(BaseModel):
    """Entidades can√¥nicas e aliases"""
    canonical: List[str]
    aliases: List[str] = []


class StrategistPayloadModel(BaseModel):
    """Contrato de sa√≠da do Estrategista (Call 1) - JSON-only, sem CoT exposto"""
    intent_profile: str
    intent: str
    executive_objectives: List[str]
    constraints: List[str] = []
    initial_hypotheses: List[str] = []
    stakeholders: List[str] = []
    key_questions: List[str]
    entities: EntitiesModel
    language_bias: List[str] = Field(default_factory=lambda: ["pt-BR", "en"])
    geo_bias: List[str] = Field(default_factory=lambda: ["BR", "global"])
    notes: Optional[str] = None


class TimeHintModel(BaseModel):
    """Time hint configuration for phase recency requirements"""
    recency: str  # "90d", "1y", "3y"
    strict: bool = False

    @validator("recency")
    def validate_recency(cls, v):
        if v not in ["90d", "1y", "3y"]:
            raise ValueError(f"recency must be 90d, 1y, or 3y, got {v}")
        return v


class EvidenceGoalModel(BaseModel):
    """Evidence requirements for quality rails"""
    official_or_two_independent: bool = True
    min_domains: int = Field(ge=2, le=10)


class PhaseTypeModel(BaseModel):
    """Phase type with strict validation"""
    phase_type: str

    @validator("phase_type")
    def validate_phase_type(cls, v):
        valid_types = [
            "industry",
            "profiles",
            "news",
            "regulatory",
            "financials",
            "tech",
        ]
        if v not in valid_types:
            raise ValueError(f"phase_type must be one of {valid_types}, got {v}")
        return v


class PlannerPhaseModel(BaseModel):
    """Phase model for Planner contract - com phase_type e valida√ß√£o estrita"""
    name: str
    phase_type: str  # "industry"|"profiles"|"news"|"regulatory"|"financials"|"tech"
    objective: str
    seed_query: str = Field(min_length=3, max_length=50)
    seed_core: Optional[str] = Field(
        default="",
        max_length=200,
        description="Seed query rica (1 frase) sem operadores - usado pelo Discovery",
    )
    seed_core_source: Optional[str] = Field(
        default="planner_heuristic",
        description="Origem: planner_llm|planner_heuristic|user",
    )
    seed_family_hint: Optional[str] = Field(
        default="entity-centric",
        description="Fam√≠lia de explora√ß√£o: entity-centric|problem-centric|outcome-centric|regulatory|counterfactual",
    )
    must_terms: List[str] = []
    avoid_terms: List[str] = []
    time_hint: TimeHintModel
    source_bias: List[str] = ["oficial", "primaria", "secundaria"]
    evidence_goal: EvidenceGoalModel
    lang_bias: List[str] = ["pt-BR", "en"]
    geo_bias: List[str] = ["BR", "global"]
    suggested_domains: List[str] = Field(
        default=[], description="Dom√≠nios sugeridos para prioriza√ß√£o"
    )
    suggested_filetypes: List[str] = Field(
        default=[], description="Tipos de arquivo sugeridos (html, pdf, etc)"
    )

    @validator("phase_type")
    def validate_phase_type(cls, v):
        valid_types = [
            "industry",
            "profiles",
            "news",
            "regulatory",
            "financials",
            "tech",
        ]
        if v not in valid_types:
            raise ValueError(f"phase_type must be one of {valid_types}, got {v}")
        return v

    @validator("seed_query")
    def validate_seed_query(cls, v):
        # Forbid operators (except @noticias which is handled separately)
        forbidden = ["site:", "filetype:", "after:", "before:", " AND ", " OR ", '"']
        for op in forbidden:
            if op in v:
                raise ValueError(f"seed_query cannot contain operator: {op}")
        # ‚úÖ REMOVED: Word count validation (3-6 words) - bloqueava queries v√°lidas com entidades compostas
        # Discovery's internal Planner will optimize the query regardless of initial seed length
        return v

    @validator("seed_core")
    def validate_seed_core(cls, v):
        """Valida seed_core: ‚â•3 palavras, ‚â§200 chars, sem operadores"""
        if not v or not v.strip():
            return ""  # Opcional, pode estar vazio

        # Forbid operators
        forbidden = ["site:", "filetype:", "after:", "before:", "AND", "OR"]
        for op in forbidden:
            if op in v:
                raise ValueError(f"seed_core cannot contain operator: {op}")

        # Validate length
        if len(v) > 200:
            raise ValueError(f"seed_core must be ‚â§200 chars, got {len(v)}")

        # Validate word count (m√≠nimo 3 palavras)
        words = v.split()
        if len(words) < 3:
            raise ValueError(f"seed_core must have ‚â•3 words, got {len(words)}")

        return v

    @validator("seed_family_hint")
    def validate_seed_family(cls, v):
        """Valida seed_family_hint: enum de fam√≠lias suportadas"""
        valid_families = [
            "entity-centric",
            "problem-centric",
            "outcome-centric",
            "regulatory",
            "counterfactual",
        ]
        if v and v not in valid_families:
            raise ValueError(
                f"seed_family_hint must be one of {valid_families}, got {v}"
            )
        return v or "entity-centric"  # Default

    @validator("must_terms")
    def validate_must_terms_policy(cls, v, values):
        """Pol√≠tica: industry n√£o deve ter todos os players; profiles/news devem ter"""
        phase_type = values.get("phase_type")
        if phase_type == "industry" and len(v) > 5:
            raise ValueError(
                f"industry phase should not have all players in must_terms (got {len(v)})"
            )
        return v


class PlannerPayloadModel(BaseModel):
    """Contrato de sa√≠da do Planner (Call 2) - JSON-only, sem CoT"""
    plan_intent: str
    assumptions_to_validate: List[str] = []
    phases: List[PlannerPhaseModel]
    quality_rails: Dict[str, Any] = {
        "min_unique_domains": 3,
        "need_official_or_two_independent": True,
    }
    budget: Dict[str, int] = {"max_rounds": 2}


class PhaseModel(BaseModel):
    name: str
    objective: str
    seed_query: str
    seed_core: Optional[str] = None
    must_terms: List[str] = []
    avoid_terms: List[str] = []
    time_hint: Dict[str, Any] = {}
    source_bias: List[str] = []
    evidence_goal: Dict[str, Any] = {}
    lang_bias: List[str] = []
    geo_bias: List[str] = []


class QualityRailsModel(BaseModel):
    """Quality rails configuration (split from PhaseModel)"""
    min_unique_domains: int = Field(ge=2)
    need_official_or_two_independent: bool = True
    official_domains: List[str] = []


# ==================== CONSTANTS AND UTILITIES ====================

class PipeConstants:
    """
    Configura√ß√µes e constantes centralizadas do pipeline.
    Valores padr√£o que podem ser sobrescritos via Valves UI.
    """

    # ===== LIMITES DE EXECU√á√ÉO =====
    MIN_PHASES = 3  # M√≠nimo de fases recomendadas
    MAX_PHASES_LIMIT = 15  # Limite absoluto de fases
    MAX_FUNCTION_LENGTH = 100  # Tamanho m√°ximo recomendado para m√©todos (linhas)

    # ===== TIMEOUTS (segundos) =====
    CONTEXT_DETECTION_TIMEOUT = 30  # Timeout para detec√ß√£o de contexto
    LLM_CALL_TIMEOUT = 60  # Timeout para chamadas LLM
    TOOL_EXECUTION_TIMEOUT = 120  # Timeout para execu√ß√£o de ferramentas

    # ===== DEFAULTS DE PERFIL =====
    DEFAULT_PROFILE = "company_profile"
    AVAILABLE_PROFILES = [
        "company_profile",
        "regulation_review",
        "technical_spec",
        "literature_review",
        "history_review",
    ]

    # ===== QUALITY RAILS =====
    MIN_EVIDENCE_COVERAGE = 1.0  # 100% de fatos com ‚â•1 evid√™ncia

    # ===== CONTEXT DETECTION =====
    DETECTOR_COT_ENABLED = True  # Habilitar Chain-of-Thought no Context Detection
    MIN_UNIQUE_DOMAINS = 2  # M√≠nimo de dom√≠nios √∫nicos por fase
    STALENESS_DAYS_DEFAULT = 90  # Dias para considerar conte√∫do stale
    NOVELTY_THRESHOLD = 0.3  # Threshold para ratio de novidade

    # ===== DEDUPLICA√á√ÉO =====
    MAX_DEDUP_PARAGRAPHS = 200  # M√°ximo de par√°grafos ap√≥s deduplica√ß√£o (alinhado com valve)
    DEDUP_SIMILARITY_THRESHOLD = 0.85  # Threshold de similaridade (0.0-1.0, maior = menos agressivo) - v4.4: ajustado 0.9‚Üí0.85
    DEDUP_RELEVANCE_WEIGHT = 0.7  # Peso da relev√¢ncia vs diversidade (0.6-0.8)

    # ===== LLM CONFIGURATION =====
    LLM_TEMPERATURE = 0.7  # Temperatura padr√£o para LLM
    LLM_MAX_TOKENS = 2048  # Max tokens para respostas LLM
    LLM_MIN_TOKENS = 100  # Min tokens aceit√°vel
    LLM_MAX_TOKENS_LIMIT = 4000  # Limite absoluto de tokens

    # ===== CONTEXT MANAGEMENT =====
    MAX_CONTEXT_CHARS = 150000  # M√°ximo de caracteres no contexto
    MAX_HISTORY_MESSAGES = 10  # M√°ximo de mensagens do hist√≥rico

    # ===== S√çNTESE =====
    SYNTHESIS_MIN_PARAGRAPHS = 3  # M√≠nimo de par√°grafos no relat√≥rio
    SYNTHESIS_PREFERRED_SECTIONS = 5  # N√∫mero preferido de se√ß√µes no relat√≥rio


# ===== Deduplication Utilities =====
def _shingles(s: str, n: int = 3) -> set:
    """Generate n-grams (shingles) from text for similarity comparison

    v4.4: Reduzido n=5‚Üí3 (tri-grams) - industry standard, +40% detec√ß√£o de similaridade
    """
    tokens = [t for t in s.lower().split() if t]
    return set(tuple(tokens[i : i + n]) for i in range(max(0, len(tokens) - n + 1)))


def _jaccard(a: set, b: set) -> float:
    """Jaccard similarity between two sets"""
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union > 0 else 0.0


def _is_geographic_term(term: str) -> bool:
    """Detecta se um termo √© geogr√°fico baseado em caracter√≠sticas estruturais"""
    term_lower = term.strip().lower()
    
    # Termos muito curtos (< 3 chars) s√£o provavelmente c√≥digos geogr√°ficos
    if len(term_lower) < 3:
        return True
    
    # Pa√≠ses conhecidos (lista m√≠nima)
    countries = {"brasil", "brazil", "portugal", "argentina", "chile", "colombia", "mexico"}
    if term_lower in countries:
        return True
    
    # C√≥digos de pa√≠s comuns
    geo_codes = {"br", "pt", "ar", "cl", "co", "mx", "us", "uk", "fr", "de", "es", "it"}
    if term_lower in geo_codes:
        return True
    
    # Termos geogr√°ficos gen√©ricos
    geo_generics = {"global", "mundial", "nacional", "internacional", "latam", "america", "europa"}
    if term_lower in geo_generics:
        return True
    
    return False


# ===== Helper Functions =====
def normalize_base_url(base_url: str) -> str:
    """Normaliza base_url para diferentes variantes de API"""
    base_url = base_url.strip().rstrip("/")
    # Normalizar variantes /api, /v1beta, etc.
    if base_url.endswith(("/v1", "/v1beta", "/api")):
        return base_url
    return base_url + "/v1"


def build_chat_endpoint(base_url: str) -> str:
    """Constr√≥i endpoint de chat completions de forma robusta"""
    from urllib.parse import urljoin

    norm = normalize_base_url(base_url)
    return urljoin(norm + "/", "chat/completions")


def _hash_contract(contract: Dict[str, Any]) -> str:
    s = json.dumps(contract, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def parse_json_resilient(
    text: str, mode: str = "balanced", allow_arrays: bool = True
) -> Optional[dict]:
    """CONSOLIDADO (v4.3.1): √önica fun√ß√£o de parsing JSON com 3 modos

    Substitui 3 fun√ß√µes antigas:
    - _extract_json_from_text() ‚Üí mode='balanced', allow_arrays=True
    - parse_llm_json_strict() ‚Üí mode='strict', allow_arrays=False
    - _soft_json_cleanup() ‚Üí usado internamente em mode='soft'

    Args:
        text: Texto contendo JSON (possivelmente com ru√≠do)
        mode: 'strict' (apenas objetos, raise em erro) |
              'soft' (cleanup trailing commas) |
              'balanced' (markdown + cleanup + balanceamento) [DEFAULT]
        allow_arrays: Se True, aceita arrays na raiz; se False, apenas objetos

    Returns:
        Dict/List parseado ou None (mode='balanced') / raises (mode='strict')

    Raises:
        ContractValidationError: Se mode='strict' e JSON inv√°lido
    """
    if not text or not isinstance(text, str):
        if mode == "strict":
            raise ContractValidationError("Empty or invalid LLM response")
        return None

    s = text.strip()

    # STEP 1: Remove markdown fences (todos os modos)
    s = re.sub(r"^```(?:json)?\s*", "", s, flags=re.MULTILINE)
    s = re.sub(r"\s*```\s*$", "", s, flags=re.MULTILINE)
    s = s.strip()

    # MODE: STRICT (contracts - apenas objetos)
    if mode == "strict":
        m_open = s.find("{")
        m_close = s.rfind("}")
        if m_open == -1 or m_close == -1 or m_close <= m_open:
            raise ContractValidationError(
                f"JSON object not found in LLM response (len={len(s)})"
            )

        raw = s[m_open : m_close + 1]

        # Tentativa 1: parse direto
        try:
            return json.loads(raw)
        except json.JSONDecodeError:
            pass

        # Tentativa 2: limpar bytes de controle
        cleaned = re.sub(r"[\x00-\x1f\x7f]", "", raw)
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass

        # Tentativa 3: trailing commas
        cleaned2 = re.sub(r",\s*}", "}", cleaned)
        cleaned2 = re.sub(r",\s*\]", "]", cleaned2)
        try:
            return json.loads(cleaned2)
        except json.JSONDecodeError as e:
            preview = cleaned2[:200] + "..." if len(cleaned2) > 200 else cleaned2
            raise ContractValidationError(
                f"Invalid JSON from LLM (after cleaning): {e}\nPreview: {preview}"
            )

    # MODE: SOFT (apenas trailing commas cleanup)
    if mode == "soft":
        s2 = re.sub(r",\s*}\s*", r"}", s)
        s2 = re.sub(r",\s*\]\s*", r"]", s2)
        try:
            return json.loads(s2)
        except json.JSONDecodeError:
            return None

    # MODE: BALANCED (default - m√°ximo esfor√ßo)
    # Tentativa 1: parse direto
    try:
        return json.loads(s)
    except json.JSONDecodeError:
        pass

    # Tentativa 2: encontrar JSON (objeto ou array se permitido)
    if allow_arrays:
        json_pattern = re.compile(r"(\{[\s\S]*\}|\[[\s\S]*\])")
    else:
        json_pattern = re.compile(r"(\{[\s\S]*\})")

    m = json_pattern.search(s)
    if not m:
        logger.warning("No JSON pattern found in text")
        return None

    snippet = m.group(1).strip()

    # ‚úÖ AGGRESSIVE CLEANUP (added for Analyst robustness)
    # Remove trailing commas, control chars, and fix common issues
    cleaned = re.sub(r"[\x00-\x1f\x7f]", "", snippet)
    cleaned = re.sub(r",\s*}", "}", cleaned)
    cleaned = re.sub(r",\s*\]", "]", cleaned)

    # Tentativa 3: parse cleaned
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError:
        pass

    # Tentativa 4: balanceamento de chaves (para JSON malformado)
    try:
        # Count braces and brackets
        open_braces = cleaned.count("{")
        close_braces = cleaned.count("}")
        open_brackets = cleaned.count("[")
        close_brackets = cleaned.count("]")

        # Try to balance
        if open_braces > close_braces:
            cleaned += "}" * (open_braces - close_braces)
        if open_brackets > close_brackets:
            cleaned += "]" * (open_brackets - close_brackets)

        return json.loads(cleaned)
    except json.JSONDecodeError:
        pass

    logger.warning(f"Failed to parse JSON after all attempts. Preview: {snippet[:200]}...")
    return None
def _extract_quality_metrics(facts_list: List[dict]) -> dict:
    """CONSOLIDADO (v4.3.1 - P1C): Extrai m√©tricas de qualidade de lista de fatos

    Usado por:
    - JudgeLLM._validate_quality_rails()
    - Orchestrator._calculate_evidence_metrics_new()

    Returns:
        Dict com: domains, facts_with_evidence, facts_with_multiple_sources,
                 high_confidence_facts, contradictions, total_evidence
    """
    if not facts_list or not isinstance(facts_list, list):
        return {
            "domains": set(),
            "facts_with_evidence": 0,
            "facts_with_multiple_sources": 0,
            "high_confidence_facts": 0,
            "contradictions": 0,
            "total_evidence": 0,
        }

    domains = set()
    facts_with_evidence = 0
    facts_with_multiple_sources = 0
    high_confidence_facts = 0
    contradictions = 0
    total_evidence = 0

    for fact in facts_list:
        if not isinstance(fact, dict):
            continue

        evidencias = fact.get("evidencias", [])
        if evidencias and len(evidencias) > 0:
            facts_with_evidence += 1
            total_evidence += len(evidencias)

            # Extrair dom√≠nios
            fact_domains = set()
            for ev in evidencias:
                if not isinstance(ev, dict):
                    continue
                try:
                    url = ev.get("url", "")
                    if url:
                        domain = url.split("/")[2]
                        domains.add(domain)
                        fact_domains.add(domain)
                except:
                    pass

            # Contar m√∫ltiplas fontes
            if len(fact_domains) >= 2:
                facts_with_multiple_sources += 1

        # Contar alta confian√ßa
        if fact.get("confian√ßa") == "alta":
            high_confidence_facts += 1

        # Contar contradi√ß√µes
        if fact.get("contradicao", False):
            contradictions += 1

    return {
        "domains": domains,
        "facts_with_evidence": facts_with_evidence,
        "facts_with_multiple_sources": facts_with_multiple_sources,
        "high_confidence_facts": high_confidence_facts,
        "contradictions": contradictions,
        "total_evidence": total_evidence,
    }


def get_safe_llm_params(model_name: str, base_params: dict = None) -> dict:
    """Retorna par√¢metros seguros para o modelo, removendo incompat√≠veis

    GPT-5/GPT-4.5/O1/O3: N√ÉO suportam temperature, max_tokens
    GPT-4/GPT-3.5: Suportam todos os par√¢metros

    Args:
        model_name: Nome do modelo (ex: "gpt-5-mini")
        base_params: Par√¢metros desejados (podem ser filtrados)

    Returns:
        Dict com par√¢metros seguros para o modelo
    """
    if not base_params:
        base_params = {}

    model_lower = (model_name or "").lower()
    is_new_gen = any(
        [
            "gpt-4.1" in model_lower,
            "gpt-4.5" in model_lower,
            "gpt-5" in model_lower,
            "o1" in model_lower,
            "o3" in model_lower,
            model_lower.startswith("chatgpt-"),
        ]
    )

    safe_params = {}

    # response_format: suportado por todos
    if "response_format" in base_params:
        safe_params["response_format"] = base_params["response_format"]

    # temperature: N√ÉO suportado por modelos novos
    if "temperature" in base_params and not is_new_gen:
        safe_params["temperature"] = base_params["temperature"]

    # request_timeout: sempre seguro (n√£o vai no body da API)
    if "request_timeout" in base_params:
        safe_params["request_timeout"] = base_params["request_timeout"]

    # max_tokens/max_completion_tokens: N√ÉO enviar (deixar model defaults)
    # OpenAI rejeita para alguns modelos

    return safe_params


def _extract_phase_count(prompt: str) -> Optional[int]:
    if not prompt:
        return None
    patterns = [
        r"(?:com|use|em|fazer|faz)\s+(\d+)\s*(?:fases?|etapas?|passos?)",
        r"(\d+)\s*(?:fases?|etapas?|passos?)",
        r"(?:dividir|separar|organizar)\s+em\s+(\d+)",
        r"(?:primeiro|segundo|terceiro|quarto|quinto)",
    ]
    for pattern in patterns:
        match = re.search(pattern, prompt.lower())
        if match:
            try:
                if pattern == r"(?:primeiro|segundo|terceiro|quarto|quinto)":
                    # Count ordinal numbers
                    ordinals = ["primeiro", "segundo", "terceiro", "quarto", "quinto"]
                    count = 0
                    for ordinal in ordinals:
                        if ordinal in prompt.lower():
                            count += 1
                    if count >= 2:
                        return count
                else:
                    num = int(match.group(1))
                    if 2 <= num <= 10:
                        return num
            except:
                pass
    return None


def _get_llm(
    valves, generation_kwargs: Optional[dict] = None, model_name: Optional[str] = None
) -> Optional['AsyncOpenAIClient']:
    model = model_name or getattr(valves, "LLM_MODEL", "") or os.getenv("LLM_MODEL", "") or ""
    base = getattr(valves, "OPENAI_BASE_URL", "") or os.getenv("OPENAI_BASE_URL", "") or ""
    key = f"{model}::{base}"
    # Fast path (read): if present and not None, return; else build under lock
    cached = _LLM_CACHE.get(key)
    if cached is not None:
        return cached

    with _LLM_CACHE_LOCK:
        # Double-check inside lock
        cached = _LLM_CACHE.get(key)
        if cached is not None:
            return cached

        api_key = (
            getattr(valves, "OPENAI_API_KEY", "") or os.getenv("OPENAI_API_KEY") or ""
        )

        # Debug info
        logger.debug("Model: %s", model)
        logger.debug("Base URL: %s", base)
        logger.debug("API Key: %s", "SET" if api_key else "NOT SET")

        if not all([base, api_key, model]):
            logger.error(
                "Missing configuration: base=%s, api_key=%s, model=%s",
                bool(base),
                bool(api_key),
                bool(model),
            )
            raise LLMConfigurationError(
                "LLM configuration missing: set OPENAI_BASE_URL, OPENAI_API_KEY and LLM_MODEL or corresponding valves"
            )

        # Import AsyncOpenAIClient here to avoid circular imports
        llm = AsyncOpenAIClient(base, api_key, model, valves)
        _LLM_CACHE[key] = llm
        return llm


async def _safe_llm_run_with_retry(
    llm_obj,
    prompt_text: str,
    generation_kwargs: Optional[dict] = None,
    timeout: int = 60,
    max_retries: int = 3,
) -> Optional[dict]:
    """LLM call with exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            # Ensure generation_kwargs is a dict to avoid passing None
            payload = generation_kwargs or {}
            result = await llm_obj.run(
                prompt=prompt_text,
                generation_kwargs=payload,
            )
            return result
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"LLM call failed after {max_retries} attempts: {e}")
                raise
            else:
                wait_time = 2 ** attempt  # Exponential backoff
                logger.warning(f"LLM call attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
                await asyncio.sleep(wait_time)

    return None


# ==================== DEDUPLICATION ENGINE ====================

def _mmr_select(
    chunks: List[str],
    k: int = 50,
    lambda_div: float = 0.7,
    preserve_order: bool = True,
    similarity_threshold: float = 0.6,
    randomize: bool = False,
    reference_chunks: List[str] = None,
) -> List[str]:
    """MMR com sele√ß√£o justa e preserva√ß√£o de narrativa

    Args:
        chunks: Lista de par√°grafos/chunks
        k: N√∫mero m√°ximo de chunks a selecionar
        lambda_div: Peso diversidade (0.0-1.0, maior = mais conservador)
        preserve_order: True = shuffle ‚Üí select ‚Üí reorder (narrativa), False = order by size
        similarity_threshold: Threshold para considerar similar (0.0-1.0)
        randomize: Se True, embaralha chunks antes de selecionar
        reference_chunks: Chunks de refer√™ncia (dedupe candidates CONTRA estes)

    Returns:
        Lista de chunks selecionados (preservando ordem original se preserve_order=True)
    """
    # Preparar √≠ndices originais para preservar ordem depois
    indexed_chunks = [(i, chunk) for i, chunk in enumerate(chunks)]

    # Estrat√©gia de sele√ß√£o: pode embaralhar os antigos para sele√ß√£o justa
    # - Se preserve_order=True e randomize=True: shuffle nos antigos, reordena ao final
    # - Se preserve_order=True e randomize=False: varre em ordem original
    # - Se preserve_order=False: prioriza chunks maiores primeiro
    if preserve_order:
        pool = list(indexed_chunks)
        if randomize:
            import random
            random.shuffle(pool)
    else:
        pool = sorted(indexed_chunks, key=lambda x: len(x[1]), reverse=True)

    selected_with_indices = []
    selected_sh: List[set] = []

    # Se h√° reference_chunks, inicializar selected_sh com eles (dedupe CONTRA refer√™ncia)
    if reference_chunks:
        for ref_chunk in reference_chunks:
            selected_sh.append(_shingles(ref_chunk))

    for original_idx, chunk in pool:
        if len(selected_with_indices) >= k:
            break

        s_sh = _shingles(chunk)

        # Calcular similaridade m√°xima com selecionados (inclui reference se houver)
        sim = max((0.0,) + tuple(_jaccard(s_sh, prev_sh) for prev_sh in selected_sh))

        # Score MMR: relev√¢ncia (tamanho) - penalidade de similaridade
        score = lambda_div * (len(chunk) / 1000.0) - (1 - lambda_div) * sim

        # Aceitar se: baixa similaridade OU score positivo
        if sim < similarity_threshold or score > 0:
            selected_with_indices.append((original_idx, chunk))
            selected_sh.append(s_sh)

    # Reordenar para preservar narrativa (se habilitado)
    if preserve_order and selected_with_indices:
        selected_with_indices.sort(key=lambda x: x[0])  # j√° est√° em ordem, mas garantir

    return [chunk for _, chunk in selected_with_indices]
class Deduplicator:
    """Deduplica√ß√£o centralizada com m√∫ltiplos algoritmos (v4.4)

    Algoritmos dispon√≠veis:
    - mmr: Maximal Marginal Relevance (padr√£o, O(n¬≤))
    - minhash: MinHash LSH (r√°pido, O(n), requer datasketch)
    - tfidf: TF-IDF + Cosine Similarity (sem√¢ntico, requer sklearn)

    Features:
    - Preserva√ß√£o de ordem original (narrativa)
    - M√©tricas de qualidade (reduction %, tokens saved)
    - Fallback autom√°tico se biblioteca n√£o dispon√≠vel
    """

    def __init__(self, valves):
        self.valves = valves

    def dedupe(
        self,
        chunks: List[str],
        max_chunks: int,
        algorithm: str = None,
        threshold: float = None,
        preserve_order: bool = True,
        preserve_recent_pct: float = 0.0,
        shuffle_older: bool = False,
        reference_first: bool = False,
        # NOVO: Context-aware parameters
        must_terms: Optional[List[str]] = None,
        key_questions: Optional[List[str]] = None,
        enable_context_aware: Optional[bool] = None,
    ) -> Dict[str, Any]:
        """Deduplica√ß√£o unificada com escolha de algoritmo

        Args:
            chunks: Lista de par√°grafos/chunks
            max_chunks: N√∫mero m√°ximo a retornar
            algorithm: 'mmr' | 'minhash' | 'tfidf' | 'semantic' (None = usa valve)
            threshold: Similaridade threshold (None = usa valve)
            preserve_order: Reordenar para manter narrativa
            preserve_recent_pct: % de chunks recentes a preservar intactos (0.0-1.0)
            shuffle_older: embaralhar sele√ß√£o dos CHUNKS ANTIGOS (e reordenar ao final)
            reference_first: se True, recent s√£o REFER√äNCIA (dedupe older CONTRA recent)
            must_terms: Termos que devem ser preservados (context-aware)
            key_questions: Quest√µes-chave para matching (context-aware)
            enable_context_aware: Ativar preserva√ß√£o de chunks cr√≠ticos (None = usa valve)

        Returns:
            Dict com: chunks (deduped), original_count, deduped_count, reduction_pct, tokens_saved

        DIVERSITY CAPS ENFORCEMENT (context-aware):
        Quando habilitado, tenta garantir cobertura m√≠nima por categoria:
        - min_new_domains: dom√≠nios √∫nicos (evita echo chamber)
        - min_official: fontes oficiais (gov, reguladores)
        - min_independent: fontes independentes (imprensa, academia)

        Estrat√©gia:
        1) Context-aware prioritization (must_terms, key_questions)
        2) Deduplica√ß√£o do restante (low priority)
        3) First pass: preencher quotas de diversidade
        4) Second pass: completar slots restantes por score
        5) Restaurar ordem original dos chunks selecionados
        """
        if not chunks:
            return {
                "chunks": [],
                "original_count": 0,
                "deduped_count": 0,
                "reduction_pct": 0.0,
                "tokens_saved": 0,
                "algorithm_used": "none",
            }

        algorithm = algorithm or getattr(self.valves, "DEDUP_ALGORITHM", "mmr")
        threshold = (
            threshold
            if threshold is not None
            else getattr(self.valves, "DEDUP_SIMILARITY_THRESHOLD", 0.85)
        )

        original_count = len(chunks)

        # Context-aware prioritization (se ativado)
        enable_context_aware = (
            enable_context_aware 
            if enable_context_aware is not None 
            else getattr(self.valves, "ENABLE_CONTEXT_AWARE_DEDUP", False)
        )
        
        if enable_context_aware and (must_terms or key_questions):
            # Usar context-aware prioritization
            preserve_top_pct = getattr(self.valves, "CONTEXT_AWARE_PRESERVE_PCT", 0.3)
            
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[CONTEXT_AWARE] Ativado: preserve_top_pct={preserve_top_pct}")
                print(f"[CONTEXT_AWARE] Must terms: {must_terms}")
                print(f"[CONTEXT_AWARE] Key questions: {key_questions}")
                print(f"[CONTEXT_AWARE] Input: {len(chunks)} chunks -> Target: {max_chunks}")
            
            high_priority, low_priority = self._context_aware_prioritize(
                chunks, must_terms, key_questions, preserve_top_pct
            )  # Agora retorna [(idx, chunk), ...]

            # Capear high_count antes de calcular available_slots (P0 fix)
            high_count = len(high_priority)
            high_count = min(high_count, max_chunks)
            available_slots = max_chunks - high_count

            # Priorizar must_terms ‚Üí key_questions ‚Üí position quando high > max
            if len(high_priority) > max_chunks:
                # Ordenar high_priority por tipo de score (must > question > position)
                high_priority_scored = []
                for idx, chunk in high_priority:
                    chunk_lower = chunk.lower()
                    must_score = sum(chunk_lower.count(t.lower()) * 2.0 for t in (must_terms or []))
                    q_score = sum(
                        len(set(q.lower().split()).intersection(set(chunk_lower.split()))) * 1.5
                        for q in (key_questions or [])
                    )
                    pos_score = idx / len(chunks) * 0.1
                    high_priority_scored.append((must_score, q_score, pos_score, idx, chunk))
                
                # Sort: must_terms primeiro, depois questions, depois position
                high_priority_scored.sort(key=lambda x: (x[0], x[1], x[2]), reverse=True)
                high_priority = [(x[3], x[4]) for x in high_priority_scored[:max_chunks]]
                available_slots = 0
            else:
                high_priority = high_priority[:high_count]

            # Dedupear low_priority se houver slots dispon√≠veis
            final_tuples = list(high_priority)  # [(idx, chunk), ...]

            # ===== Diversity caps enforcement (best-effort) =====
            # Determine profile and caps from valves
            intent_profile = getattr(self, "_intent_profile", "default") or "default"
            caps_by_profile = getattr(self.valves, "DIVERSITY_CAPS_BY_PROFILE", {}) or {}
            caps = caps_by_profile.get(intent_profile, caps_by_profile.get("default", {}))
            min_new_domains = int(caps.get("min_new_domains", 0))
            min_official = int(caps.get("min_official", 0))
            min_independent = int(caps.get("min_independent", 0))

            def _domain_from_chunk(text: str) -> Optional[str]:
                # Very lightweight domain extraction from "URL: ..." lines
                try:
                    for line in text.splitlines():
                        if line.startswith("URL:"):
                            url = line.split("URL:", 1)[1].strip()
                            # Extract host
                            host = url.split("//", 1)[-1].split("/", 1)[0]
                            return host.lower()
                except Exception:
                    return None
                return None

            # Build current category counters for already selected
            selected_domains = set()
            selected_official = 0
            selected_independent = 0

            official_domains = set(
                (getattr(self.valves, "OFFICIAL_DOMAINS", {}) or {})
                    .get(getattr(self, "_intent_profile", "default"), [])
            )

            for _, ch in final_tuples:
                dom = _domain_from_chunk(ch)
                if not dom:
                    continue
                selected_domains.add(dom)
                if any(off in dom for off in official_domains):
                    selected_official += 1
                else:
                    selected_independent += 1

            if available_slots > 0 and low_priority:
                # Criar dicion√°rio chunk ‚Üí [indices] para mapeamento robusto
                low_chunks = [ch for _, ch in low_priority]
                chunk_to_indices = {}
                for idx, chunk in low_priority:
                    if chunk not in chunk_to_indices:
                        chunk_to_indices[chunk] = []
                    chunk_to_indices[chunk].append(idx)
                
                # Dedupear apenas os chunks (sem √≠ndices)
                deduped_low = self._dedupe_chunks(low_chunks, available_slots, algorithm, threshold)

                # First pass: satisfy diversity caps
                if min_new_domains or min_official or min_independent:
                    added_now = 0
                    for chunk in list(deduped_low):
                        if added_now >= available_slots:
                            break
                        dom = _domain_from_chunk(chunk)
                        if not dom:
                            continue
                        is_official = any(off in dom for off in official_domains)
                        improves_new = dom not in selected_domains and len(selected_domains) < min_new_domains
                        improves_official = is_official and selected_official < min_official
                        improves_indep = (not is_official) and selected_independent < min_independent
                        if improves_new or improves_official or improves_indep:
                            if chunk in chunk_to_indices and chunk_to_indices[chunk]:
                                idx = chunk_to_indices[chunk].pop(0)
                                final_tuples.append((idx, chunk))
                                selected_domains.add(dom)
                                if is_official:
                                    selected_official += 1
                                else:
                                    selected_independent += 1
                                added_now += 1
                                available_slots -= 1
                    # Remove items already used in first pass
                    for _, items in list(chunk_to_indices.items()):
                        while items and any(t[0] == items[0] for t in final_tuples):
                            items.pop(0)
                
                # Reconstruir com √≠ndices originais
                for chunk in deduped_low:
                    if available_slots <= 0:
                        break
                    if chunk in chunk_to_indices and chunk_to_indices[chunk]:
                        idx = chunk_to_indices[chunk].pop(0)  # Pegar primeiro √≠ndice dispon√≠vel
                        final_tuples.append((idx, chunk))
                        available_slots -= 1

            # SEMPRE restaurar ordem original (1b: for√ßar preserve_order=True)
            final_tuples.sort(key=lambda x: x[0])  # Ordenar por √≠ndice original
            final_chunks = [chunk for _, chunk in final_tuples]

            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[CONTEXT_AWARE] Ordem restaurada: {len(final_chunks)} chunks preservam timeline original")
                # Mostrar preview de chunks com URL
                for i, chunk in enumerate(final_chunks[:3]):
                    if chunk.startswith("URL:"):
                        url_line = chunk.split('\n')[0]
                        print(f"[CONTEXT_AWARE]   [{i}] {url_line[:80]}...")

            # M√©tricas
            deduped_count = len(final_chunks)
                
            # Retornar resultado context-aware
            result = {
                "chunks": final_chunks,
                "original_count": original_count,
                "deduped_count": deduped_count,
                "reduction_pct": (original_count - deduped_count) / original_count * 100,
                "tokens_saved": 0,  # TODO: calcular tokens saved
                "algorithm_used": f"context_aware_{algorithm}",
                "fallback_occurred": False,  # Context-aware n√£o usa fallback
                "dependencies_checked": True,
            }
            
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[CONTEXT_AWARE] Final: {original_count} -> {deduped_count} chunks ({result['reduction_pct']:.1f}% reduction)")
            
            return result
        
        # Separar chunks recentes se solicitado (para Analyst)
        if preserve_recent_pct > 0:
            recent_count = max(10, int(len(chunks) * preserve_recent_pct))
            recent_chunks = chunks[-recent_count:]
            older_chunks = chunks[:-recent_count]
            effective_max = max_chunks - len(recent_chunks)
        else:
            recent_chunks = []
            older_chunks = chunks
            effective_max = max_chunks

        # Aplicar algoritmo escolhido
        # Se reference_first=True, dedupear older CONTRA recent (recent como refer√™ncia)
        reference_chunks_for_mmr = recent_chunks if reference_first else []

        try:
            if algorithm == "minhash":
                deduped_older = self._minhash_dedupe(
                    older_chunks,
                    threshold,
                    effective_max,
                    reference_chunks=reference_chunks_for_mmr,
                )
                algo_used = "minhash"
            elif algorithm == "tfidf":
                deduped_older = self._tfidf_dedupe(
                    older_chunks,
                    threshold,
                    effective_max,
                    reference_chunks=reference_chunks_for_mmr,
                )
                algo_used = "tfidf"
            elif algorithm == "semantic":
                try:
                    model_name = getattr(self.valves, "SEMANTIC_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
                    print(f"[DEDUP] üß† Usando algoritmo SEMANTIC com modelo {model_name}")
                    deduped_older = self._semantic_dedupe(
                        older_chunks,
                        threshold,
                        effective_max,
                        reference_chunks=reference_chunks_for_mmr,
                        model_name=model_name,
                    )
                    algo_used = "semantic"
                except (ImportError, AttributeError) as e:
                    print(f"[DEDUP] ‚ö†Ô∏è Fallback para MMR: {e}")
                    # Fallback para MMR se Haystack n√£o dispon√≠vel
                    deduped_older = _mmr_select(
                        chunks=older_chunks,
                        k=effective_max,
                        lambda_div=getattr(self.valves, "DEDUP_RELEVANCE_WEIGHT", 0.7),
                        preserve_order=True,
                        similarity_threshold=threshold,
                        randomize=shuffle_older,
                        reference_chunks=reference_chunks_for_mmr,
                    )
                    algo_used = "mmr_fallback"
            else:  # mmr (default)
                print(f"[DEDUP] Usando algoritmo MMR (default)")
                deduped_older = _mmr_select(
                    chunks=older_chunks,
                    k=effective_max,
                    lambda_div=getattr(self.valves, "DEDUP_RELEVANCE_WEIGHT", 0.7),
                    preserve_order=True,  # Preserve a ordem durante a sele√ß√£o
                    similarity_threshold=threshold,
                    randomize=shuffle_older,
                    reference_chunks=reference_chunks_for_mmr,  # Dedupe older CONTRA recent
                )
                algo_used = "mmr"
        except ImportError as e:
            # Fallback para MMR se biblioteca n√£o dispon√≠vel
            logger.warning(
                f"Algorithm '{algorithm}' not available ({e}), falling back to MMR"
            )
            deduped_older = _mmr_select(
                chunks=older_chunks,
                k=effective_max,
                lambda_div=getattr(self.valves, "DEDUP_RELEVANCE_WEIGHT", 0.7),
                preserve_order=True,
                similarity_threshold=threshold,
                randomize=shuffle_older,
                reference_chunks=reference_chunks_for_mmr,
            )
            algo_used = "mmr_fallback"

        # Combinar: Se reference_first, recent V√äM PRIMEIRO
        if reference_first:
            result = (recent_chunks + deduped_older)[:max_chunks]
        else:
            result = (deduped_older + recent_chunks)[:max_chunks]

        # Preservar ordem original se solicitado
        if preserve_order:
            result = self._restore_order(result, chunks)

        deduped_count = len(result)
        reduction_pct = (
            ((original_count - deduped_count) / original_count * 100)
            if original_count > 0
            else 0
        )
        tokens_saved = (original_count - deduped_count) * 30  # ~30 tokens/par√°grafo

        # Detectar se houve fallback
        fallback_occurred = algo_used.endswith("_fallback")
        
        # Log de telemetria para monitoramento
        if fallback_occurred:
            print(f"[DEDUP] TELEMETRIA: Fallback detectado - algoritmo solicitado vs usado: {algorithm} -> {algo_used}")
        
        return {
            "chunks": result,
            "original_count": original_count,
            "deduped_count": deduped_count,
            "reduction_pct": reduction_pct,
            "tokens_saved": tokens_saved,
            "algorithm_used": algo_used,
            "fallback_occurred": fallback_occurred,
            "dependencies_checked": True,
        }

    def _minhash_dedupe(
        self,
        chunks: List[str],
        threshold: float,
        max_chunks: int,
        reference_chunks: List[str] = None,
    ) -> List[str]:
        """MinHash LSH deduplica√ß√£o - O(n) - requer datasketch

        Args:
            reference_chunks: Se fornecido, dedupe chunks CONTRA estes (j√° no LSH)
        """
        try:
            from datasketch import MinHash, MinHashLSH
        except ImportError:
            raise ImportError("datasketch required: pip install datasketch")

        lsh = MinHashLSH(threshold=threshold, num_perm=128)
        unique_chunks = []

        # Se h√° reference_chunks, inserir no LSH primeiro (dedupe CONTRA eles)
        if reference_chunks:
            for i, ref_chunk in enumerate(reference_chunks):
                m = MinHash(num_perm=128)
                for word in ref_chunk.split():
                    m.update(word.encode("utf8"))
                lsh.insert(f"ref_{i}", m)

        for i, chunk in enumerate(chunks):
            if len(unique_chunks) >= max_chunks:
                break

            # Criar MinHash signature
            m = MinHash(num_perm=128)
            for word in chunk.split():
                m.update(word.encode("utf8"))

            # Query LSH para similares
            result = lsh.query(m)
            if not result:  # Nenhum similar encontrado
                lsh.insert(f"chunk_{i}", m)
                unique_chunks.append(chunk)

        return unique_chunks

    def _tfidf_dedupe(
        self,
        chunks: List[str],
        threshold: float,
        max_chunks: int,
        reference_chunks: List[str] = None,
    ) -> List[str]:
        """TF-IDF + Cosine Similarity deduplica√ß√£o - requer sklearn

        Args:
            reference_chunks: Se fornecido, dedupe chunks CONTRA estes
        """
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
        except ImportError:
            raise ImportError("sklearn required: pip install scikit-learn")

        if not chunks:
            return []

        # Se h√° reference_chunks, processar junto para TF-IDF consistente
        all_chunks = (reference_chunks or []) + chunks
        ref_count = len(reference_chunks) if reference_chunks else 0

        # Criar TF-IDF matrix
        vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=1, max_df=0.95)
        tfidf_matrix = vectorizer.fit_transform(all_chunks)

        selected = []
        selected_indices = []

        # Se h√° reference, considerar todos eles como "j√° selecionados"
        if ref_count > 0:
            selected_indices = list(range(ref_count))

        # Iterar apenas sobre chunks (n√£o reference)
        for i in range(ref_count, len(all_chunks)):
            chunk_idx_in_original = i - ref_count
            chunk = chunks[chunk_idx_in_original]

            if len(selected) >= max_chunks:
                break

            if not selected_indices:
                selected.append(chunk)
                selected_indices.append(i)
                continue

            # Calcular similaridade com j√° selecionados (inclui reference)
            chunk_vec = tfidf_matrix[i : i + 1]
            if selected_indices:
                selected_vecs = tfidf_matrix[selected_indices, :]
                similarities = cosine_similarity(chunk_vec, selected_vecs)
                max_sim = similarities.max()
            else:
                max_sim = 0.0
            if max_sim < threshold:
                selected.append(chunk)
                selected_indices.append(i)

        return selected

    def _restore_order(self, deduped: List[str], original: List[str]) -> List[str]:
        """Restaura ordem original dos chunks dedupados"""
        if not deduped or not original:
            return deduped

        # Criar mapa: chunk ‚Üí √≠ndice original
        original_indices = {chunk: i for i, chunk in enumerate(original)}

        # Ordenar deduped pela ordem original
        ordered = sorted(deduped, key=lambda x: original_indices.get(x, 999999))
        return ordered

    def _semantic_dedupe(
        self,
        chunks: List[str],
        threshold: float,
        max_chunks: int,
        reference_chunks: List[str] = None,
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
    ) -> List[str]:
        """Deduplica√ß√£o sem√¢ntica usando embeddings (Haystack)
        
        Args:
            chunks: Par√°grafos a dedupear
            threshold: Cosine similarity threshold (0.0-1.0, default 0.85)
            max_chunks: N√∫mero m√°ximo a retornar
            reference_chunks: Se fornecido, dedupe chunks CONTRA estes
            model_name: Modelo de embeddings (lightweight por padr√£o)
        
        Returns:
            Lista de chunks √∫nicos semanticamente
            
        Raises:
            ImportError: Se Haystack n√£o estiver dispon√≠vel (fallback para MMR)
        """
        if not HAYSTACK_AVAILABLE:
            print(f"[DEDUP] DEPENDENCIA FALTANDO: Haystack/sentence-transformers n√£o instalado")
            print(f"[DEDUP] INSTALAR: pip install haystack sentence-transformers scikit-learn")
            raise ImportError("Haystack/sentence-transformers required for semantic dedup")
        
        if not chunks:
            return []
        
        # Preparar documentos para Haystack
        docs = [Document(content=chunk, meta={"original_idx": i}) 
                for i, chunk in enumerate(chunks)]
        
        # Reference documents (se fornecido)
        if reference_chunks:
            ref_docs = [Document(content=ref, meta={"is_reference": True}) 
                        for ref in reference_chunks]
            all_docs = ref_docs + docs
        else:
            all_docs = docs
        
        # Embedder (in-memory, sem persist√™ncia)
        try:
            embedder = SentenceTransformersDocumentEmbedder(model=model_name)
            embedder.warm_up()
            embedded_docs = embedder.run(all_docs)["documents"]
        except Exception as e:
            print(f"[DEDUP] Semantic embedder failed: {e}")
            raise ImportError(f"Semantic model load failed: {e}")
        
        # Extrair embeddings como numpy array
        embeddings = np.array([doc.embedding for doc in embedded_docs])
        
        # Calcular matriz de similaridade (cosine)
        from sklearn.metrics.pairwise import cosine_similarity
        similarity_matrix = cosine_similarity(embeddings)
        
        # Selecionar chunks √∫nicos por clustering simples
        selected_indices = []
        excluded_indices = set()
        
        # Se h√° refer√™ncias, marcar como j√° selecionadas
        if reference_chunks:
            num_refs = len(reference_chunks)
            excluded_indices.update(range(num_refs))
            start_idx = num_refs
        else:
            start_idx = 0
        
        for i in range(start_idx, len(similarity_matrix)):
            if i in excluded_indices:
                continue
            
            # Verificar se similar a algum j√° selecionado ou refer√™ncia
            is_duplicate = False
            for j in selected_indices + list(range(start_idx)):
                if i != j and similarity_matrix[i][j] >= threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                selected_indices.append(i)
                if len(selected_indices) >= max_chunks:
                    break
        
        # Mapear de volta para chunks originais
        result = [chunks[embedded_docs[i].meta["original_idx"]] 
                  for i in selected_indices if i >= start_idx]
        
        return result

    def _context_aware_prioritize(
        self,
        chunks: List[str],
        must_terms: Optional[List[str]] = None,
        key_questions: Optional[List[str]] = None,
        preserve_top_pct: float = 0.3
    ) -> Tuple[List[Tuple[int, str]], List[Tuple[int, str]]]:
        """
        Separa chunks em alta e baixa prioridade baseado em contexto.
        
        Args:
            chunks: Lista de chunks para priorizar
            must_terms: Termos que devem ser preservados (weight: 2.0)
            key_questions: Quest√µes-chave para matching (weight: 1.5)
            preserve_top_pct: % de chunks para alta prioridade (default: 0.3)
            
        Returns:
            (high_priority_chunks, low_priority_chunks) where each is List[Tuple[int, str]] (index, chunk)
        """
        if not chunks:
            return [], []
            
        if not must_terms and not key_questions:
            # Se n√£o h√° contexto, retornar chunks recentes como high priority
            high_count = max(1, int(len(chunks) * preserve_top_pct))
            return [(len(chunks)-high_count+i, ch) for i, ch in enumerate(chunks[-high_count:])], \
                   [(i, ch) for i, ch in enumerate(chunks[:-high_count])]
        
        # Calcular score para cada chunk
        chunk_scores = []
        for i, chunk in enumerate(chunks):
            # LLM-first: Deixar o LLM decidir qualidade atrav√©s do scoring inteligente
            chunk_lower = chunk.lower()
            score = 0.0
            must_score = 0.0
            question_score = 0.0
            
            # 1. Must terms (weight: 2.0) - LLM-first: scoring inteligente
            if must_terms:
                for term in must_terms:
                    term_lower = term.lower()
                    
                    # Ignorar termos geogr√°ficos usando detec√ß√£o estrutural
                    if _is_geographic_term(term):
                        if getattr(self.valves, "VERBOSE_DEBUG", False):
                            print(f"[CONTEXT_AWARE] Skipping geo term: '{term}'")
                        continue
                    
                    # Contar ocorr√™ncias (case-insensitive)
                    count = chunk_lower.count(term_lower)
                    
                    # LLM-first: Bonus para co-ocorr√™ncia com contexto setorial
                    setorial_bonus = 0.0
                    if any(setor in chunk_lower for setor in [
                        "executive search", "headhunting", "recrutamento executivo", 
                        "search executivo", "consultoria executiva"
                    ]):
                        setorial_bonus = 1.0  # Bonus por contexto setorial correto
                    
                    term_score = (count * 2.0) + setorial_bonus
                    must_score += term_score
                    
            # 2. Key questions overlap (weight: 1.5)
            if key_questions:
                for question in key_questions:
                    question_lower = question.lower()
                    # Verificar se chunk cont√©m palavras-chave da quest√£o
                    question_words = set(question_lower.split())
                    chunk_words = set(chunk_lower.split())
                    overlap = len(question_words.intersection(chunk_words))
                    if overlap > 0:
                        q_score = overlap * 1.5
                        question_score += q_score
            
            # 3. Posi√ß√£o no documento (recent > old, weight: 0.1)
            position_score = (i / len(chunks)) * 0.1
            score = must_score + question_score + position_score
            
            chunk_scores.append((score, i, chunk))
            
            # Debug logging para chunks com score alto
            if getattr(self.valves, "VERBOSE_DEBUG", False) and score > 1.0:
                print(f"[CONTEXT_AWARE] Chunk {i}: score={score:.2f} (must={must_score:.2f}, q={question_score:.2f}, pos={position_score:.2f})")
                print(f"[CONTEXT_AWARE]    Preview: {chunk[:100]}...")
        
        # Ordenar por score (maior primeiro)
        chunk_scores.sort(key=lambda x: x[0], reverse=True)
        
        # Separar em high/low priority
        high_count = max(1, int(len(chunks) * preserve_top_pct))
        high_priority = [(chunk_scores[i][1], chunk_scores[i][2]) for i in range(high_count)]  # (index, chunk)
        low_priority = [(chunk_scores[i][1], chunk_scores[i][2]) for i in range(high_count, len(chunks))]
        
        return high_priority, low_priority

    def _dedupe_chunks(
        self,
        chunks: List[str],
        max_chunks: int,
        algorithm: str,
        threshold: float
    ) -> List[str]:
        """
        M√©todo auxiliar para dedupear chunks (usado pelo context-aware).
        Aplica o algoritmo especificado aos chunks fornecidos.
        """
        if not chunks or max_chunks <= 0:
            return []
            
        try:
            if algorithm == "minhash":
                return self._minhash_dedupe(chunks, threshold, max_chunks)
            elif algorithm == "tfidf":
                return self._tfidf_dedupe(chunks, threshold, max_chunks)
            elif algorithm == "semantic":
                model_name = getattr(self.valves, "SEMANTIC_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
                try:
                    return self._semantic_dedupe(chunks, threshold, max_chunks, model_name=model_name)
                except ImportError as e:
                    print(f"[DEDUP] Semantic unavailable: {e} ‚Üí fallback to MMR")
                    return _mmr_select(chunks, max_chunks, similarity_threshold=threshold)
            else:  # default to mmr
                return _mmr_select(chunks, max_chunks, similarity_threshold=threshold)
        except Exception as e:
            # Fallback para MMR em caso de erro
            print(f"[DEDUP] FALLBACK CRITICO: {algorithm} -> MMR devido a: {e}")
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[DEDUP] Verifique dependencias: pip install scikit-learn datasketch sentence-transformers haystack")
            return _mmr_select(chunks, max_chunks, similarity_threshold=threshold)


# ==================== LLM CLIENT ====================

class AsyncOpenAIClient:
    """Async OpenAI client using httpx for better performance"""

    def __init__(self, base_url: str, api_key: str, model: str, valves=None):
        self.base_url = (base_url or "").rstrip("/")
        self.api_key = api_key
        self.model = model
        self.valves = valves  # Store valves for timeout configuration
        self._client = None  # Lazy initialization

    def _ensure_client(self):
        """Lazy initialization of httpx client"""
        if self._client is None:
            try:
                import httpx

                # Use valve timeout if available, otherwise default to 180s
                read_timeout = (
                    getattr(self.valves, "HTTPX_READ_TIMEOUT", 180)
                    if self.valves
                    else 180
                )
                self._client = httpx.AsyncClient(
                    timeout=httpx.Timeout(
                        240.0, connect=10.0, read=float(read_timeout)
                    ),  # Increased: 120‚Üí240, 5‚Üí10, 115‚Üí180
                    limits=httpx.Limits(
                        max_keepalive_connections=10, max_connections=20
                    ),
                    http2=True,  # Enable HTTP/2 for multiplexing
                    follow_redirects=True,
                )
            except ImportError:
                raise RuntimeError("httpx library required: pip install httpx")
        return self._client

    async def _call_api(
        self, prompt: str, generation_kwargs: Optional[dict] = None
    ) -> str:
        try:
            import httpx
        except ImportError:
            raise RuntimeError("httpx library required: pip install httpx")

        client = self._ensure_client()
        url = build_chat_endpoint(self.base_url)

        # Log do tamanho do prompt ANTES de enviar (debug cr√≠tico para truncamento)
        prompt_len = len(prompt)
        prompt_tokens_est = prompt_len // 4
        if prompt_tokens_est > 12000:
            logger.warning(
                f"[API] Prompt grande sendo enviado: {prompt_len:,} chars (~{prompt_tokens_est:,} tokens). Modelo '{self.model}' pode ter limite de input que cause truncamento!"
            )

        body = {"model": self.model, "messages": [{"role": "user", "content": prompt}]}
        gen_kwargs = generation_kwargs or {}

        # Detect capabilities by model name
        model_lower = (self.model or "").lower()
        is_new_gen_model = any(
            [
                "gpt-4.1" in model_lower,
                "gpt-4.5" in model_lower,
                "gpt-5" in model_lower,
                "o1" in model_lower,
                "o3" in model_lower,
                model_lower.startswith("chatgpt-"),
            ]
        )

        # Temperature: newer models often only support default (1.0)
        if "temperature" in gen_kwargs and not is_new_gen_model:
            body["temperature"] = gen_kwargs["temperature"]

        # Forward response_format when caller enforces JSON mode
        resp_fmt = gen_kwargs.get("response_format")
        if resp_fmt:
            body["response_format"] = resp_fmt

        # Do NOT send any token limits - let model defaults handle it
        # OpenAI API rejects max_tokens/max_completion_tokens for some models

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        # ‚úÖ FIX TIMEOUT HIERARCHY: Unificar cliente/per-request usando MAX
        default_read = (
            getattr(self.valves, "HTTPX_READ_TIMEOUT", 180)
            if hasattr(self, "valves")
            else 180
        )
        request_timeout = float(gen_kwargs.get("request_timeout", default_read))
        effective_read_timeout = max(default_read, request_timeout, 60.0)

        # ‚úÖ Criar timeout per-request (sobrescreve timeout do cliente)
        per_request_timeout = httpx.Timeout(
            240.0, connect=10.0, read=effective_read_timeout
        )

        if effective_read_timeout > 300:
            logger.info(
                f"[API] Long operation timeout: {effective_read_timeout}s (synthesis/large prompt)"
            )

        try:
            # ‚úÖ Async HTTP POST com timeout PER-REQUEST expl√≠cito
            resp = await client.post(
                url, json=body, headers=headers, timeout=per_request_timeout
            )
            resp.raise_for_status()
            j = resp.json()
            text_parts = []
            for ch in j.get("choices", []):
                if isinstance(ch.get("message"), dict):
                    text_parts.append(ch["message"].get("content") or "")
                else:
                    text_parts.append(ch.get("text") or "")
            return "".join(text_parts).strip()
        except httpx.TimeoutException as e:
            logger.error(f"[API] HTTP timeout after {effective_read_timeout}s: {e}")
            raise TimeoutError(
                f"HTTP request timeout after {effective_read_timeout}s"
            ) from e
        except httpx.HTTPStatusError as e:
            error_detail = ""
            try:
                error_detail = f" - {e.response.text}"
            except:
                pass
            logger.error("HTTP Error %s: %s%s", e.response.status_code, e, error_detail)
            logger.debug("URL: %s", url)
            logger.debug("Body: %s", json.dumps(body, indent=2))
            raise RuntimeError(f"HTTP {e.response.status_code}: {e}") from e
        except Exception as e:
            logger.error("HTTP Error: %s", e)
            raise

    async def run(self, prompt: str, generation_kwargs: Optional[dict] = None) -> dict:
        result = await self._call_api(prompt, generation_kwargs)
        return {"replies": [result]}


# ==================== LLM COMPONENTS ====================

class AnalystLLM:
    """Analyst que processa contexto acumulado completo"""

    def __init__(self, valves):
        self.valves = valves
        # Usar modelo espec√≠fico se configurado, sen√£o usa modelo padr√£o
        model = valves.LLM_MODEL_ANALYST or valves.LLM_MODEL
        self.model_name = model
        self.llm = _get_llm(valves, model_name=model)
        # Base kwargs: ser√£o filtrados por get_safe_llm_params (GPT-5 n√£o aceita temperature)
        self.generation_kwargs = {"temperature": valves.LLM_TEMPERATURE}

    async def run(
        self, query: str, accumulated_context: str, phase_context: Dict = None
    ) -> Dict[str, Any]:
        """Analisa contexto acumulado COMPLETO (todas as fases at√© agora)"""

        # üî¥ DEFESA P0: Validar inputs e estado do LLM
        if not self.llm:
            logger.error("[ANALYST] LLM n√£o configurado")
            return {"summary": "", "facts": [], "lacunas": ["LLM n√£o configurado"]}

        if not accumulated_context or len(accumulated_context.strip()) == 0:
            logger.warning("[ANALYST] Contexto vazio - sem dados para analisar")
            return {
                "summary": "Sem contexto para analisar",
                "facts": [],
                "lacunas": ["Contexto vazio"],
            }

        try:
            # Extrair informa√ß√µes da fase atual
            phase_info = ""
            if phase_context:
                phase_name = phase_context.get("name", "Fase atual")
                # Contract usa "objetivo" (PT), n√£o "objective" (EN)
                phase_objective = phase_context.get("objetivo") or phase_context.get(
                    "objective", ""
                )
                phase_info = f"\n**FASE ATUAL:** {phase_name}\n**Objetivo da Fase:** {phase_objective}"

            sys_prompt = _build_analyst_prompt(query, phase_context)

            user_prompt = f"""**Objetivo da Fase:** {query}{phase_info}
**Contexto Acumulado (todas as fases at√© agora):**
{accumulated_context}"""

            timeout_analyst = min(self.valves.LLM_TIMEOUT_ANALYST, 120)  # Cap at 120s to prevent truncation
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(
                    f"[DEBUG][ANALYST] Using timeout: {timeout_analyst}s (context: {len(accumulated_context):,} chars)"
                )
            # Use retry function if enabled, otherwise single attempt
            # Filtrar par√¢metros incompat√≠veis com GPT-5/O1
            # ‚úÖ FORCE JSON MODE for Analyst robustness
            base_params = {
                "temperature": self.generation_kwargs.get("temperature", 0.2),
                "response_format": {"type": "json_object"}  # FORCE JSON MODE
            }
            safe_params = get_safe_llm_params(self.model_name, base_params)

            if getattr(self.valves, "ENABLE_LLM_RETRY", True):
                max_retries = int(getattr(self.valves, "LLM_MAX_RETRIES", 3) or 3)
                out = await _safe_llm_run_with_retry(
                    self.llm,
                    f"{sys_prompt}\n\n{user_prompt}",
                    safe_params,
                    timeout=timeout_analyst,
                    max_retries=max_retries,
                )
            else:
                out = await _safe_llm_run_with_retry(
                    self.llm,
                    f"{sys_prompt}\n\n{user_prompt}",
                    safe_params,
                    timeout=timeout_analyst,
                    max_retries=1,
                )

            if not out:
                return {"summary": "", "facts": [], "lacunas": []}

            raw_reply = out.get("replies", [""])[0] if out else ""
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(
                    f"[DEBUG][ANALYST] Analyst raw reply length: {len(raw_reply)} chars"
                )
                print(
                    f"[DEBUG][ANALYST] Analyst raw reply preview: {raw_reply[:200]}..."
                )

            # üîß FIX v2: Strip agressivo para remover \n " no in√≠cio (erro comum do LLM)
            cleaned_reply = raw_reply.strip()

            # Remover newlines e whitespace no in√≠cio recursivamente
            while cleaned_reply and cleaned_reply[0] in "\n\r\t ":
                cleaned_reply = cleaned_reply[1:]

            # Se come√ßa com " mas n√£o √© JSON v√°lido, remover aspas soltas
            if cleaned_reply.startswith('"') and not cleaned_reply.startswith('{"'):
                # Remover todas as aspas duplas consecutivas no in√≠cio
                cleaned_reply = cleaned_reply.lstrip('"').lstrip()

            # Se ainda n√£o come√ßa com { ou [, tentar envolver em objeto
            if (
                cleaned_reply
                and not cleaned_reply.startswith("{")
                and not cleaned_reply.startswith("[")
            ):
                # Caso especial: LLM retornou apenas os campos sem o envelope {}
                # Tentar envolver em chaves
                if '"summary"' in cleaned_reply or '"facts"' in cleaned_reply:
                    cleaned_reply = "{" + cleaned_reply + "}"
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(
                            "[DEBUG][ANALYST] Wrapped response in {} (LLM forgot envelope)"
                        )

            # v4.4: Usar parse_json_resilient direto (modo balanced - m√°ximo esfor√ßo)
            parsed = parse_json_resilient(
                cleaned_reply, mode="balanced", allow_arrays=False
            )

            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[DEBUG] Analyst parsed: {parsed is not None}")
                if parsed:
                    print(f"[DEBUG] Analyst parsed keys: {list(parsed.keys())}")
                    print(
                        f"[DEBUG] Analyst facts count before validation: {len(parsed.get('facts', []))}"
                    )
                else:
                    # Log o motivo da falha
                    print(
                        f"[DEBUG] Analyst parsing FAILED. Raw reply length: {len(raw_reply)}"
                    )
                    print(f"[DEBUG] First 500 chars: {raw_reply[:500]}")
                    print(f"[DEBUG] Last 500 chars: {raw_reply[-500:]}")

            # Se falhou, tentar com mode='soft' (apenas cleanup)
            if not parsed and raw_reply:
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print("[DEBUG] Trying mode='soft' (cleanup only)...")
                parsed = parse_json_resilient(
                    raw_reply, mode="soft", allow_arrays=False
                )
                if parsed:
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print("[DEBUG] Analyst reparsed successfully with mode='soft'")

            # Re-ask √∫nico e curto exigindo JSON v√°lido
            try_reask = not parsed
            if try_reask:
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(
                        "[DEBUG] Analyst initial parse failed, re-asking with stricter JSON-only instructions..."
                    )

                reask_instr = (
                    "RETORNE APENAS JSON PURO (sem markdown, sem explica√ß√£o, sem texto extra).\n\n"
                    "SCHEMA OBRIGAT√ìRIO:\n"
                    "{\n"
                    '  "summary": "string resumo",\n'
                    '  "facts": [{ "texto": "fato X", "confian√ßa": "alta|m√©dia|baixa", "evidencias": [{"url":"...","trecho":"..."}] }],\n'
                    '  "lacunas": ["lacuna 1", "lacuna 2"],\n'
                    '  "self_assessment": { "coverage_score": 0.7, "confidence": "m√©dia", "gaps_critical": true, "suggest_refine": false, "suggest_pivot": true, "reasoning": "brevemente por qu√™" }\n'
                    "}\n\n"
                    "‚ö†Ô∏è IMPORTANTE:\n"
                    "- coverage_score: 0.0-1.0 (quanto % do objetivo foi coberto)\n"
                    "- gaps_critical: True se lacunas impedem resposta ao objetivo\n"
                    "- suggest_pivot: True se lacuna precisa de √¢ngulo/temporal diferente\n\n"
                    "N√ÉO adicione coment√°rios, N√ÉO use ```json, N√ÉO explique nada fora do JSON."
                )
                limited_context = accumulated_context[:20000]
                reask_prompt = f"{_build_analyst_prompt(query, phase_context)}\n\n{reask_instr}\n\n**Objetivo da Fase:** {query}{phase_info}\n\n**Contexto Acumulado:**\n{limited_context}"

                # For√ßar JSON response_format quando suportado (evitar para modelos que n√£o aceitam)
                base_reask = {"temperature": 0.1}
                if not any(x in self.model_name.lower() for x in ["o1", "o3", "gpt-5"]):
                    base_reask["response_format"] = {"type": "json_object"}
                safe_reask = get_safe_llm_params(self.model_name, base_reask)

                reask_out = await _safe_llm_run_with_retry(
                    self.llm,
                    reask_prompt,
                    safe_reask,
                    timeout=timeout_analyst,
                    max_retries=1,
                )
                re_raw = reask_out.get("replies", [""])[0] if reask_out else ""

                # Parse estrito primeiro; se falhar, tentar balanced
                reparsed = parse_json_resilient(
                    re_raw, mode="strict", allow_arrays=False
                )
                if not reparsed and re_raw:
                    reparsed = parse_json_resilient(
                        re_raw, mode="balanced", allow_arrays=False
                    )
                if reparsed:
                    parsed = reparsed
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print("[DEBUG] Analyst parsed successfully on re-ask")

            # Valida√ß√£o de evid√™ncia rica (P0.4) - TEMPORARIAMENTE RELAXADA PARA DEBUG
            if parsed:
                facts_before_validation = parsed.get("facts", [])
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(
                        f"[DEBUG] Facts before validation: {len(facts_before_validation)}"
                    )

                # Log detalhado de cada fato antes da valida√ß√£o
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    for i, fact in enumerate(facts_before_validation[:3]):
                        print(f"[DEBUG] Fact {i}: {fact}")

                validated = self._validate_analyst_output(parsed)
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[DEBUG] Analyst validation result: {validated}")

                if not validated["valid"]:
                    # v4.6: Valida√ß√£o RE-HABILITADA (era temporariamente relaxada para debug)
                    logger.warning(f"[ANALYST] Output inv√°lido: {validated['reason']}")
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(
                            f"[WARNING] Analyst output invalid: {validated['reason']}"
                        )
                    # Retornar output vazio com lacuna explicativa
                    return {
                        "summary": "",
                        "facts": [],
                        "lacunas": [validated["reason"]],
                    }

        except Exception as e:
            logger.error(f"[ANALYST] Exce√ß√£o n√£o tratada: {e}")
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                import traceback
                traceback.print_exc()
            return {
                "summary": "",
                "facts": [],
                "lacunas": [f"Erro interno: {str(e)[:100]}"],
            }

        return parsed or {"summary": "", "facts": [], "lacunas": []}

    def _validate_analyst_output(self, parsed):
        """Valida sa√≠da do Analyst - VERS√ÉO COMPLETA RE-HABILITADA (v4.6)

        Valida√ß√µes:
        1. Facts s√£o dicts com campos obrigat√≥rios
        2. Evid√™ncias t√™m URL v√°lida
        3. Self-assessment presente e bem-formado
        """
        facts = parsed.get("facts", [])

        # Sem fatos N√ÉO √© v√°lido: retorna lacuna explicativa
        if not facts:
            return {"valid": False, "reason": "Nenhum fato extra√≠do (contexto vazio ou irrelevante)"}

        # Valida√ß√£o de estrutura dos fatos
        for i, fact in enumerate(facts):
            if not isinstance(fact, dict):
                return {"valid": False, "reason": f"Fato {i} n√£o √© dict"}

            # Campos obrigat√≥rios
            if "texto" not in fact:
                return {"valid": False, "reason": f"Fato {i} sem campo 'texto'"}

            if not fact.get("texto") or not fact["texto"].strip():
                return {"valid": False, "reason": f"Fato {i} com texto vazio"}

            # Confian√ßa obrigat√≥ria
            if "confian√ßa" not in fact:
                return {"valid": False, "reason": f"Fato {i} sem campo 'confian√ßa'"}

            if fact["confian√ßa"] not in ["alta", "m√©dia", "baixa"]:
                return {
                    "valid": False,
                    "reason": f"Fato {i} com confian√ßa inv√°lida: {fact['confian√ßa']}",
                }

            # Evid√™ncias (opcional mas recomendado)
            evidencias = fact.get("evidencias", [])
            if evidencias:
                for j, ev in enumerate(evidencias):
                    if not isinstance(ev, dict):
                        return {
                            "valid": False,
                            "reason": f"Fato {i}, evid√™ncia {j} n√£o √© dict",
                        }

                    if "url" not in ev:
                        return {
                            "valid": False,
                            "reason": f"Fato {i}, evid√™ncia {j} sem URL",
                        }

        # Valida√ß√£o de self_assessment (obrigat√≥rio)
        sa = parsed.get("self_assessment", {})
        if not sa:
            return {"valid": False, "reason": "self_assessment ausente"}

        # Campos obrigat√≥rios de self_assessment
        required_sa_fields = ["coverage_score", "confidence", "gaps_critical"]
        for field in required_sa_fields:
            if field not in sa:
                return {
                    "valid": False,
                    "reason": f"self_assessment sem campo '{field}'",
                }

        # Validar coverage_score range
        coverage = sa.get("coverage_score")
        if not isinstance(coverage, (int, float)) or not (0.0 <= coverage <= 1.0):
            return {
                "valid": False,
                "reason": f"coverage_score inv√°lido: {coverage} (deve ser 0.0-1.0)",
            }

        # Validar confidence
        if sa.get("confidence") not in ["alta", "m√©dia", "baixa"]:
            return {
                "valid": False,
                "reason": f"confidence inv√°lida: {sa.get('confidence')}",
            }

        # Validar gaps_critical
        if not isinstance(sa.get("gaps_critical"), bool):
            return {
                "valid": False,
                "reason": f"gaps_critical deve ser bool, got {type(sa.get('gaps_critical'))}",
            }

        return {"valid": True}


def _build_seed_query_rules() -> str:
    """Retorna regras compactas de seed_query (usar 1x no prompt)"""
    return """
**SEED_QUERY (3-8 palavras, SEM operadores):**
- Estrutura: TEMA_CENTRAL + ASPECTO + GEO
- Se 1-3 entidades: incluir TODOS os nomes na seed
- Se 4+ entidades: seed gen√©rica + TODOS em must_terms
- @noticias: adicionar 3-6 palavras espec√≠ficas (eventos, tipos, a√ß√µes)

Exemplos:
‚úÖ "RedeDr S√≥ Sa√∫de oncologia Brasil" (1-3 entidades)
‚úÖ "volume autos el√©tricos Brasil" (4+ entidades)
‚úÖ "@noticias recalls ve√≠culos el√©tricos Brasil" (breaking news)
‚ùå "volume fees Brasil" (falta tema!)
‚ùå "buscar dados verific√°veis" (gen√©rico demais)
"""


def _build_time_windows_table() -> str:
    """Tabela compacta de janelas temporais"""
    return """
**JANELAS TEMPORAIS:**

| Recency | Uso | Exemplo |
|---------|-----|---------|
| **90d** | Breaking news expl√≠cito | "√∫ltimos 90 dias", "breaking news" |
| **1y** | Tend√™ncias/estado atual (DEFAULT news) | "eventos recentes", "aquisi√ß√µes ano" |
| **3y** | Panorama/contexto hist√≥rico | "evolu√ß√£o setorial", "baseline" |
**Regra Pr√°tica:**
- News SEM prazo expl√≠cito ‚Üí 1y (captura 12 meses)
- News COM "90 dias" ‚Üí 90d (breaking only)
- Estudos de mercado ‚Üí 3y (contexto) + 1y (tend√™ncias) [OBRIGAT√ìRIO]
"""
def _extract_json_from_text(text: str) -> Optional[dict]:
    """LEGACY WRAPPER: Delega para parse_json_resilient(mode='balanced')"""
    return parse_json_resilient(text, mode="balanced", allow_arrays=True)
def _patch_seed_if_needed(
    phase: dict, strict_mode: bool, metrics: dict, logger
) -> None:
    """Patch seed_query se estiver muito magra (modo relax apenas)"""
    if strict_mode:
        return  # Modo strict: n√£o patch

    obj = phase.get("objective") or phase.get("objetivo") or ""
    sq = phase.get("seed_query", "")

    if not obj or not sq:
        return

    # Verifica se seed_query cont√©m algum token significativo do objetivo
    obj_tokens = [t for t in obj.lower().split() if len(t) > 4]
    sq_lower = sq.lower()

    has_obj_token = any(t in sq_lower for t in obj_tokens)

    if not has_obj_token:
        # Seed n√£o tem nenhum token do objetivo - patch
        k = _first_content_token(obj)
        if k and k not in sq_lower:
            phase["seed_query"] = f"{sq} {k}".strip()
            if metrics is not None:
                metrics["seed_patched_count"] = metrics.get("seed_patched_count", 0) + 1
            logger.info(f"[SEED] Patched seed_query '{sq}' ‚Üí '{phase['seed_query']}'")


def _check_mece_basic(fases: List[dict], key_questions: List[str]) -> List[str]:
    """Verifica MECE b√°sico: key_questions √≥rf√£s (sem cobertura)"""
    if not key_questions:
        return []

    uncovered = []
    for kq in key_questions:
        tokens = [t for t in kq.lower().split() if len(t) > 4]
        covered = False
        for f in fases:
            obj = (f.get("objetivo") or f.get("objective") or "").lower()
            if any(t in obj for t in tokens):
                covered = True
                break
        if not covered:
            uncovered.append(kq)

    return uncovered


def _append_phase(contract: dict, candidate: dict) -> None:
    """Adiciona fase candidata ao contract (incremental, sem replanejar)"""
    # Normalizar chaves (name/nome, objective/objetivo)
    if "nome" in candidate and "name" not in candidate:
        candidate["name"] = candidate.pop("nome")
    if "objective" not in candidate and "objetivo" in candidate:
        candidate["objective"] = candidate.pop("objetivo")

    # Sanity checks b√°sicos
    seed_query = candidate.get("seed_query", "")
    if not (3 <= len(seed_query.split()) <= 8):
        logger.warning(f"[APPEND] seed_query inv√°lida: {seed_query}")
        return

    seed_core = candidate.get("seed_core", "")
    if seed_core and seed_core == seed_query:
        logger.warning(f"[APPEND] seed_core igual a seed_query: {seed_core}")
        return

    # Adicionar ao contract
    contract["fases"].append(candidate)
    logger.info(f"[APPEND] Fase adicionada: {candidate.get('name', 'N/A')}")


def _calc_entity_coverage(fases: List[dict], entities: List[str]) -> float:
    """Calcula a % de fases que cont√™m pelo menos uma entidade em must_terms"""
    if not entities or not fases:
        return 1.0

    covered = 0
    for f in fases:
        must_terms = f.get("must_terms", [])
        mt_str = " ".join(must_terms).lower()
        if any(e.lower() in mt_str for e in entities):
            covered += 1

    return covered / max(1, len(fases))


def _list_missing_entity_phases(fases: List[dict], entities: List[str]) -> List[str]:
    """Lista as fases que N√ÉO cont√™m nenhuma entidade em must_terms"""
    if not entities:
        return []

    missing = []
    for f in fases:
        must_terms = f.get("must_terms", [])
        mt_str = " ".join(must_terms).lower()
        if not any(e.lower() in mt_str for e in entities):
            phase_name = (
                f.get("name") or f.get("nome") or f.get("phase_type") or "sem-nome"
            )
            missing.append(phase_name)

    return missing


def _check_entity_coverage_soft(
    contract: dict, entities: List[str], min_coverage: float, logger
) -> None:
    """Valida entity coverage em modo SOFT (warning em vez de exception)"""
    fases = contract.get("fases") or contract.get("phases", [])
    coverage = _calc_entity_coverage(fases, entities)

    if entities and coverage < min_coverage:
        contract["_entity_coverage_warning"] = {
            "coverage": coverage,
            "min": min_coverage,
            "missing_phases": _list_missing_entity_phases(fases, entities),
        }
        logger.warning(
            f"Entity coverage {coverage:.0%} < {min_coverage:.0%} (modo soft). "
            f"Fases sem entidades: {contract['_entity_coverage_warning']['missing_phases']}"
        )


def _build_synthesis_sections(
    key_questions: List[str],
    research_objectives: List[str],
    entities: List[str],
    contract: dict,
) -> str:
    """Build synthesis sections for final report"""
    sections = []
    
    if key_questions:
        sections.append(f"**Key Questions:** {', '.join(key_questions[:5])}")
    
    if research_objectives:
        sections.append(f"**Research Objectives:** {', '.join(research_objectives[:3])}")
    
    if entities:
        sections.append(f"**Entities:** {', '.join(entities[:5])}")
    
    phases = contract.get("fases", [])
    if phases:
        sections.append(f"**Phases:** {len(phases)} planned")
    
    return "\n".join(sections)


def _render_contract(contract: Dict[str, Any]) -> str:
    """Render contract as markdown for display"""
    fases = contract.get("fases", [])
    num_fases = len(fases)
    lines = [f"## üìã Plano ‚Äì {num_fases} Fases\n"]

    intent = contract.get("intent", "")
    if intent:
        lines.append(f"**üéØ Objetivo:** {intent}\n")

    # Mostrar entidades
    entities = contract.get("entities", {})
    if entities.get("canonical"):
        lines.append(f"**üè∑Ô∏è Entidades:** {', '.join(entities['canonical'])}")
        if entities.get("aliases"):
            lines.append(f"**üîó Aliases:** {', '.join(entities['aliases'])}")
        lines.append("")

    lines.append(f"**üìç Fases:**\n")
    for i, fase in enumerate(fases, 1):
        lines.append(f"### Fase {i}/{num_fases} ‚Äì {fase.get('name', 'N/A')}")
        lines.append(f"**Objetivo:** {fase.get('objetivo', 'N/A')}")
        # Exibir seed_core (query rica para Discovery) em vez de seed_query
        seed_core = fase.get("seed_core", "N/A")
        lines.append(f"**Seed Query:** `{seed_core}`")

        # Mostrar must/avoid terms
        must_terms = fase.get("must_terms", [])
        avoid_terms = fase.get("avoid_terms", [])
        if must_terms:
            lines.append(f"**‚úÖ Must:** {', '.join(must_terms)}")
        if avoid_terms:
            lines.append(f"**‚ùå Avoid:** {', '.join(avoid_terms)}")

        # Mostrar time hint e source bias
        time_hint = fase.get("time_hint", {})
        source_bias = fase.get("source_bias", [])
        if time_hint:
            lines.append(f"**‚è∞ Tempo:** {time_hint.get('recency', 'N/A')}")
        if source_bias:
            lines.append(f"**üìä Fontes:** {' > '.join(source_bias)}")

        lines.append("")

    # Mostrar quality rails
    quality_rails = contract.get("quality_rails", {})
    if quality_rails:
        lines.append("**üõ°Ô∏è Quality Rails:**")
        lines.append(
            f"- M√≠nimo {quality_rails.get('min_unique_domains', 'N/A')} dom√≠nios √∫nicos"
        )
        if quality_rails.get("need_official_or_two_independent"):
            lines.append("- Fonte oficial OU ‚â•2 dom√≠nios independentes por fase")
        lines.append("")

    lines.append("---")
    lines.append(f"**üí° Responda:** **siga** | **continue**")
    return "\n".join(lines)


def _first_content_token(text: str) -> str:
    """Extract first meaningful token from text"""
    if not text:
        return ""
    
    tokens = text.strip().split()
    for token in tokens:
        if len(token) > 2 and token.isalpha():
            return token
    
    return tokens[0] if tokens else ""


def _build_entity_rules_compact() -> str:
    """Regras de entidades (v4.8 - Entity-Centric Policy)"""
    return """
**POL√çTICA ENTITY-CENTRIC (v4.8):**

| Quantidade | Mode | Seed_query | Must_terms (por fase) | Exemplo |
|------------|------|------------|----------------------|---------|
| 1-3 | üéØ FOCADO | Incluir TODOS | **TODAS as fases** devem ter | "RedeDr S√≥ Sa√∫de oncologia BR" |
| 4-6 | üìä DISTRIBU√çDO | Gen√©rica | industry:‚â§3, profiles/news:TODAS | seed:"sa√∫de digital BR", must:["RedeDr","DocTech","Hospital X"] |
| 7+ | üìä DISTRIBU√çDO | Gen√©rica | industry:‚â§3, profiles/news:TODAS | must:["Magalu","Via","Americanas",...] |

**Cobertura obrigat√≥ria (1-3 entidades): ‚â•70% das fases devem incluir as entidades em must_terms**
**Raz√£o:** Discovery Selector usa must_terms para prioriza√ß√£o + Analyst precisa de contexto focado
"""


def _build_planner_prompt(
    user_prompt: str,
    phases: int,
    current_date: Optional[str] = None,
    detected_context: Optional[dict] = None,
) -> str:
    """Build unified Planner prompt used by both Manual and SDK routes."""
    if not current_date:
        from datetime import datetime
        current_date = datetime.now().strftime("%Y-%m-%d")

    date_context = f"DATA ATUAL: {current_date}\n(Use esta data ao planejar fases de not√≠cias/eventos recentes. N√£o sugira anos passados como '2024' se estamos em 2025.)\n\n"

    # Orienta√ß√£o espec√≠fica por perfil detectado (compacta)
    profile_guidance = ""
    if detected_context:
        perfil = detected_context.get("perfil", "")
        setor = detected_context.get("setor", "")
        # Blocos curtos por perfil (2‚Äì3 bullets). Se j√° houver key_questions/entities, manter guidance minimalista
        short_guidance = {
            "company_profile": (
                f"Perfil mercado ({setor}): use 3y para panorama, 1y para tend√™ncias, 90d s√≥ para eventos pontuais. Priorize fontes oficiais/prim√°rias."
            ),
            "technical_spec": (
                f"Perfil t√©cnico ({setor}): panorama 3y, docs atuais 1y, releases 90d. Priorize docs oficiais/RFCs/repos."
            ),
            "regulation_review": (
                f"Perfil regulat√≥rio ({setor}): marco vigente 3y, compliance 1y, mudan√ßas 90d. Priorize gov/oficial."
            ),
            "literature_review": (
                f"Perfil acad√™mico ({setor}): fundamentos 3y+, estado da arte 1‚Äì3y, papers 1y. Priorize scholar/peri√≥dicos."
            ),
            "history_review": (
                f"Perfil hist√≥rico ({setor}): contexto 3y+, evolu√ß√£o 3y, an√°lise atual 1y. Priorize arquivos/oficial/academia."
            ),
        }
        pg = short_guidance.get(perfil, "")
        if pg:
            profile_guidance = pg + "\n\n"

    # Usar key_questions e entities do detected_context (se dispon√≠veis) - vers√£o compacta
    cot_preamble = ""
    if detected_context:
        # Usar as informa√ß√µes do Context Detection (CoT j√° foi feito l√°)
        key_q = detected_context.get("key_questions", [])
        entities = detected_context.get("entities_mentioned", [])
        objectives = detected_context.get("research_objectives", [])

        if key_q or entities or objectives:
            # SEMPRE mostrar key_questions e entities explicitamente (n√£o depender de reasoning_summary)
            cot_preamble = f"""
üìã **CONTEXTO J√Å ANALISADO (CONTEXT-LOCK):**
‚úÖ {len(key_q)} key questions identificadas
‚úÖ {len(entities)} entidades espec√≠ficas detectadas  
‚úÖ {len(objectives)} objetivos de pesquisa definidos
‚úÖ Perfil: {detected_context.get('perfil', 'N/A')}
üîí **PAYLOAD DO ESTRATEGISTA (USE EXCLUSIVAMENTE, N√ÉO RE-INFIRA):**
KEY_QUESTIONS={json.dumps(key_q[:10], ensure_ascii=False)}
ENTITIES_CANONICAL={json.dumps(entities[:15], ensure_ascii=False)}
RESEARCH_OBJECTIVES={json.dumps(objectives[:10], ensure_ascii=False)}
LANG_BIAS={detected_context.get('language_bias', ['pt-BR', 'en'])}
GEO_BIAS={detected_context.get('geo_bias', ['BR', 'global'])}
‚ö†Ô∏è **INSTRU√á√ïES CR√çTICAS:**
1. Crie fases que RESPONDAM √†s KEY_QUESTIONS listadas acima
2. Inclua ENTITIES_CANONICAL nos must_terms das fases apropriadas
3. Alinhe os objectives das fases aos RESEARCH_OBJECTIVES do estrategista
4. N√ÉO introduza novas entidades n√£o listadas acima
5. N√ÉO altere ou re-interprete os objetivos
6. Use SOMENTE os dados do payload acima
"""

    # Chain of Thought: SEMPRE usar informa√ß√µes do Context Detection (n√£o extrair novamente)
    if detected_context and (
        detected_context.get("key_questions")
        or detected_context.get("entities_mentioned")
    ):
        # Context Detection j√° fez o CoT - N√ÉO pedir re-extra√ß√£o
        chain_of_thought = f"""
‚öôÔ∏è **PROCESSO DE PLANEJAMENTO:**
Pense passo a passo INTERNAMENTE, mas N√ÉO exponha o racioc√≠nio. Retorne APENAS JSON.

1. **MAPEAR** cada KEY_QUESTION do payload acima ‚Üí uma fase espec√≠fica
2. **DIVIDIR** em at√© {phases} fases MECE (panorama ‚Üí detalhes ‚Üí atual/news)
3. **APLICAR** janelas temporais: 3y (panorama), 1y (tend√™ncias), 90d (not√≠cias)
4. **INCLUIR** ENTITIES_CANONICAL nos must_terms conforme phase_type

{cot_preamble}
"""
    else:
        # Fallback: se Context Detection falhou completamente
        chain_of_thought = f"""
‚ö†Ô∏è FALLBACK MODE (Context Detection falhou):
Extraia voc√™ mesmo as key questions e entidades da consulta abaixo e divida em fases.
{cot_preamble}
"""

    # Exemplo m√≠nimo (1 bloco) ‚Äî mant√©m orienta√ß√£o sem inflar prompt
    example_json = """    {
      "name": "Panorama geral",
      "objective": "Pergunta verific√°vel e espec√≠fica",
      "seed_query": "<3-6 palavras, sem operadores>",
      "seed_core": "<12-200 chars, 1 frase rica, sem operadores>",
      "must_terms": ["<todas as entidades mencionadas>"],
      "avoid_terms": ["<ru√≠do>"] ,
      "time_hint": {"recency": "1y", "strict": false},
      "source_bias": ["oficial", "primaria", "secundaria"],
      "evidence_goal": {"official_or_two_independent": true, "min_domains": 3},
      "lang_bias": ["pt-BR", "en"],
      "geo_bias": ["BR", "global"]
    }"""

    # P1: Exemplo ANTES/DEPOIS para seed_query (clareza de tema central)
    seed_before_after = """
‚ö†Ô∏è EXEMPLOS DE SEED QUERY - ANTES E DEPOIS:

‚ùå ERRADO (sem tema central):
- "volume fees Brasil" ‚Üí Falta contexto (fees de QU√ä?)
- "tend√™ncias servi√ßos Brasil" ‚Üí Gen√©rico (servi√ßos de QU√ä?)
- "reputa√ß√£o boutiques Brasil" ‚Üí Amb√≠guo (boutiques de QU√ä?)

‚úÖ CORRETO (tema presente):
- "volume fees executive search Brasil" ‚Üí Tema: executive search
- "tend√™ncias headhunting Brasil" ‚Üí Tema: headhunting
- "reputa√ß√£o boutiques executive search Brasil" ‚Üí Tema: executive search

REGRA: seed_query = TEMA_CENTRAL + ASPECTO + GEO
"""

    # P1: Instru√ß√µes para seed_core (OBRIGAT√ìRIO)
    seed_core_instructions = """
‚ö†Ô∏è **SEED_CORE (OBRIGAT√ìRIO para TODAS as fases):**
- Formato: 1 frase rica (12-200 chars), linguagem natural, SEM operadores
- Inclui: entidades + tema + aspecto + recorte geotemporal
- Contexto completo para Discovery Tool executar busca efetiva
- Rela√ß√£o com seed_query: seed_core √© expans√£o rica de seed_query

EXEMPLOS:
Fase "Volume setorial":
  seed_query: "volume executive search Brasil"
  seed_core: "volume anual mercado executive search Brasil √∫ltimos 3 anos fontes oficiais associa√ß√µes setor"

Fase "Tend√™ncias servi√ßos":
  seed_query: "tend√™ncias headhunting Brasil"
  seed_core: "tend√™ncias emergentes servi√ßos headhunting e recrutamento executivo Brasil √∫ltimos 12 meses inova√ß√µes tecnologia"

Fase "Perfis empresas":
  seed_query: "Korn Ferry portf√≥lio Brasil"
  seed_core: "Korn Ferry portf√≥lio servi√ßos posicionamento competitivo mercado brasileiro executive search √∫ltimos 2 anos"

‚ùå ERRADO (muito curta, sem contexto):
  seed_core: "Flow CNPJ Brasil"  // Apenas 3 palavras

‚úÖ CORRETO:
  seed_core: "Flow Executive Finders CNPJ registro Receita Federal Brasil raz√£o social data funda√ß√£o"
"""

    # Framework de auto-valida√ß√£o de realismo
    realism_framework = """
üîç AUTO-VALIDA√á√ÉO DE REALISMO (PENSE ANTES DE INCLUIR M√âTRICAS):

Para CADA m√©trica/dado que voc√™ incluir no objective, fa√ßa a pergunta:

'Empresas/organiza√ß√µes DESTE TIPO e PORTE divulgam isso publicamente?'

Use seu conhecimento sobre:
  ‚Ä¢ Pr√°ticas do setor (financeiro vs tech vs sa√∫de vs consultoria)
  ‚Ä¢ Tipo de empresa (listada vs privada vs startup vs p√∫blica)
  ‚Ä¢ Sensibilidade competitiva (pricing, margens, m√©tricas operacionais)
  ‚Ä¢ Obriga√ß√µes regulat√≥rias (empresas listadas divulgam mais)

HEUR√çSTICA SIMPLES:
  ‚úÖ Se encontraria em: site corporativo, press releases, relat√≥rios anuais
     ‚Üí INCLUIR no objective
  ‚ö†Ô∏è Se encontraria apenas em: relat√≥rios internos, pitches de vendas
     ‚Üí EVITAR ou marcar como 'se dispon√≠vel'
  ‚ùå Se √© vantagem competitiva: pricing real, custos, m√©tricas operacionais
     ‚Üí N√ÉO incluir, focar em proxies p√∫blicas

EXEMPLO DE RACIOC√çNIO:
Query: 'Boutique de executive search no Brasil'
M√©trica considerada: 'time-to-fill m√©dio, success rate %'

Pergunta: Consultoria de RH divulga m√©tricas operacionais?
Resposta: N√£o - s√£o vantagens competitivas confidenciais.
          Empresas listadas divulgam revenue agregado, privadas n√£o.

Objective ajustado: 'portf√≥lio de servi√ßos, setores atendidos,
                     ciclos/processos DECLARADOS (quando dispon√≠vel)'
                     [proxy p√∫blico para 'rapidez operacional']

‚ö†Ô∏è IMPORTANTE: Voc√™ conhece centenas de setores. Use esse conhecimento.
               N√£o force m√©tricas que voc√™ sabe serem privadas.
"""

    # Usar dicion√°rios globais de prompts
    system_prompt = PROMPTS["planner_system"].format(phases=phases)
    seed_rules = _build_seed_query_rules()
    time_windows = _build_time_windows_table()
    entity_rules = _build_entity_rules_compact()
    
    return (
        date_context
        + profile_guidance
        + cot_preamble
        + seed_before_after
        + seed_core_instructions
        + realism_framework
        + f"""

{system_prompt}

üéØ OBJETIVO DA PESQUISA:
{user_prompt}

{seed_rules}

{time_windows}

{entity_rules}

üéØ **ECONOMIA DE FASES (CRITICAL):**
AT√â """
        + str(phases)
        + """ fases permitidas. PREFIRA MENOS FASES BEM FOCADAS.

**QUANDO COMBINAR (1 fase):**
- Objetivo = comparar/rankear m√∫ltiplas entidades
- Overview geral ou an√°lise aggregada de mercado

**QUANDO ESPECIALIZAR (1 fase/entidade):**
- Usu√°rio pede "perfis detalhados" / "an√°lise profunda"
- Entidades muito distintas (B2B vs B2C, setores diferentes)
- Volume esperado >10 p√°ginas por entidade

**EXEMPLOS:**
- "Compare receita A, B, C" ‚Üí ‚úÖ 2 fases (receitas 3y + drivers 1y) | ‚ùå 6 fases (1/empresa + trends)
- "Perfis detalhados A, B, C" ‚Üí ‚úÖ 4 fases (3 perfis deep + comparativa) | Justificado: especializa√ß√£o necess√°ria

üìä **SCORING:** Precis√£o 40% + Economia 30% + MECE 30% ‚Üí Menos fases (mesma cobertura) = SUPERIOR

**CHECKLIST:** Antes de criar fase ‚Üí (1) Responde key_question √∫nica? (2) Aspecto/temporal diferente? (3) Combinar degrada qualidade? ‚Üí Se N√ÉO para qualquer ‚Üí N√ÉO CRIE

üî¥ **REGRA ESPECIAL - PEDIDOS EXPL√çCITOS DE NOT√çCIAS:**
Se o usu√°rio mencionar "not√≠cias", "noticias", "fase de not√≠cias", "eventos recentes":
‚Üí OBRIGAT√ìRIO criar fase type="news" com:
  - seed_query: "@noticias" + tema + entidades
  - time_hint: 1y (√∫ltimos 12 meses)
  - 90d SOMENTE se usu√°rio disser "breaking news", "√∫ltimos 90 dias" ou "muito recente"

‚öôÔ∏è **PROCESSO DE PLANEJAMENTO (Use o payload acima, N√ÉO re-extraia):**

Pense passo a passo INTERNAMENTE, mas N√ÉO exponha o racioc√≠nio. Retorne APENAS JSON.

**ETAPA 1 - MAPEAR (n√£o extrair):**
- Para cada KEY_QUESTION do payload ‚Üí crie 1 fase espec√≠fica
- Exemplo: KEY_QUESTION "Qual volume anual?" ‚Üí Fase "Volume setorial" (phase_type: industry)
- Exemplo: KEY_QUESTION "Qual reputa√ß√£o?" ‚Üí Fase "Perfis players" (phase_type: profiles)

**ETAPA 2 - DIVIDIR em fases MECE por phase_type:**

üéØ **ENTITY-CENTRIC MODE (1-3 entidades mencionadas):**
SE o payload tem 1-3 ENTITIES_CANONICAL ‚Üí MODO FOCADO:
  ‚úÖ **TODAS as fases** devem incluir essas entidades em must_terms
  ‚úÖ Seed_query de cada fase deve conter nomes das entidades
  ‚úÖ Objetivo: Manter foco laser nas entidades espec√≠ficas
  ‚ùå N√ÉO crie fases gen√©ricas sem as entidades
  
üìä **MULTI-ENTITY MODE (4+ entidades mencionadas):**
SE o payload tem 4+ ENTITIES_CANONICAL ‚Üí MODO DISTRIBU√çDO:
  - **industry**: pode ter subset (‚â§3 entidades representativas)
  - **profiles**: deve ter TODAS
  - **news**: deve ter TODAS

**Phase types (aplique a l√≥gica acima):**
- **industry**: panorama setorial (volume, tend√™ncias, players)
  - must_terms: [ENTITY-CENTRIC: todas entidades] [MULTI: subset ‚â§3] + termos setoriais + geo
  - time_hint: 1y ou 3y
- **profiles**: perfis detalhados de players
  - must_terms: **TODAS** as ENTITIES_CANONICAL do payload + geo
  - time_hint: 3y
- **news**: not√≠cias e eventos relevantes (√∫ltimos 12 meses)
  - must_terms: **TODAS** as ENTITIES_CANONICAL do payload + geo
  - seed_query: DEVE incluir "@noticias" + tema + entidades
  - time_hint: 1y (DEFAULT) | 90d SOMENTE para "breaking news" expl√≠cito

**ETAPA 3 - APLICAR janelas temporais corretas:
üîç **CONTEXTO IMPORTA:** A janela temporal depende do TIPO DE INFORMA√á√ÉO, n√£o apenas se √© "not√≠cia"!

"""
        + _build_time_windows_table()
        + """

‚ö†Ô∏è **CR√çTICO - ESTUDOS DE MERCADO (READ THIS!):**

üö® **SE** o objetivo geral cont√©m ["estudo", "an√°lise", "mercado", "setor", "panorama", "competitivo"]:

**ARQUITETURA OBRIGAT√ìRIA (N√ÉO NEGOCI√ÅVEL):**
1. ‚úÖ Fase "industry/profiles" com 3y (contexto/baseline)
2. ‚úÖ **Fase "not√≠cias/eventos" com 1y** (OBRIGAT√ìRIA se planejando 3+ fases)
   - seed_query DEVE ter "@noticias" + tema + entidades
   - Captura √∫ltimos 12 meses de eventos relevantes
   - 90d SOMENTE se usu√°rio pedir "breaking news" ou "√∫ltimos 90 dias" explicitamente
3. ‚úÖ Fase adicional de "perfis" ou "tend√™ncias" conforme necess√°rio

**‚ùå ERRO COMUM (N√ÉO FA√áA):**
- Criar apenas 1 fase "news" com 90d
- Esquecer de incluir "@noticias" na seed_query de fases de not√≠cias
- Criar fases gen√©ricas sem as entidades quando h√° 1-3 entidades (entity-centric)
- Resultado: perde tend√™ncias dos √∫ltimos 12 meses (70% das key questions n√£o respondidas) + falta foco nas entidades

**‚úÖ EXEMPLO CORRETO - Estudo entity-centric (3 entidades: Health+, Vida Melhor, OncoTech):**
```json
{
  "plan_intent": "Mapear oportunidades e riscos em sa√∫de digital com foco em 3 hospitais brasileiros",
  "phases": [
    {"name": "Panorama de mercado e players (3 anos)", "phase_type": "industry", 
    "seed_query": "RedeDr S√≥ Sa√∫de oncologia Brasil",  // ‚Üê ENTITY-CENTRIC: todas as 3 empresas
    "must_terms": ["RedeDr", "S√≥ Sa√∫de", "Onco Brasil", "sa√∫de digital", "Brasil"],  // ‚Üê TODAS
     "time_hint": {"recency": "3y"}},
    {"name": "Perfis, servi√ßos e reputa√ß√£o", "phase_type": "profiles", 
    "seed_query": "Health+ Vida Melhor OncoTech servi√ßos rankings reputa√ß√£o",  // ‚Üê ENTITY-CENTRIC: todas as 3
    "must_terms": ["Health+", "Vida Melhor", "OncoTech", "sa√∫de digital", "Brasil"],  // ‚Üê TODAS
     "time_hint": {"recency": "3y"}},
    {"name": "Not√≠cias e eventos (12 meses)", "phase_type": "news", 
    "seed_query": "@noticias Magalu Via Americanas varejo Brasil",  // ‚Üê ENTITY-CENTRIC + @noticias
    "must_terms": ["Magazine Luiza", "Via", "Americanas", "varejo", "Brasil"],  // ‚Üê TODAS
     "objective": "Aquisi√ß√µes, lan√ßamentos, mudan√ßas de mercado com foco nas 3 empresas-alvo (12 meses)",
     "time_hint": {"recency": "1y", "strict": false}}  // ‚Üê 1y (n√£o 90d!)
  ]
}
```

**‚ö†Ô∏è CONTRASTE - ERRADO (fases gen√©ricas, perdeu foco):**
```json
{
  "phases": [
    {"name": "Panorama geral", "seed_query": "mercado executive search Brasil",  // ‚ùå SEM entidades
     "must_terms": ["executive search", "Brasil"]},  // ‚ùå Faltam empresas
    {"name": "Tend√™ncias", "seed_query": "tend√™ncias servi√ßos headhunting Brasil",  // ‚ùå SEM entidades
     "must_terms": ["executive search", "assessment"]},  // ‚ùå Faltam empresas
    {"name": "Perfis", "seed_query": "Health+ Vida Melhor OncoTech",  // ‚úÖ Tem entidades MAS s√≥ em 1 fase
     "must_terms": ["Health+", "Vida Melhor", "OncoTech"]}  // ‚úÖ OK mas TARDE DEMAIS (70% das fases sem foco)
  ]
}
```
**Resultado errado: Judge detecta falta de foco nas empresas-alvo e cria novas fases redundantes (desperd√≠cio de budget)**

**üéØ VALIDA√á√ÉO OBRIGAT√ìRIA (checklist antes de retornar plano):**
- [ ] **Entity-centric?** Se 1-3 entidades: ‚â•70% das fases incluem essas entidades em must_terms?
- [ ] **Temporal coverage?** H√° fase com recency=1y para capturar tend√™ncias dos √∫ltimos 12 meses?
- [ ] **News phase?** Se pedido "not√≠cias" ou "estudo de mercado": fase news com "@noticias" e 1y?
- [ ] **Key questions cobertas?** Cada KEY_QUESTION do payload tem fase correspondente?
- [ ] **Seed_query v√°lido?** 3-8 palavras, sem operadores, cont√©m tema central + entidades?

üìê REGRAS OBRIGAT√ìRIAS:

‚úÖ CADA FASE TEM:
   - name: descritivo e √∫nico
   - objective: pergunta verific√°vel (n√£o gen√©rica!)
   - seed_query: 3-8 palavras, SEM operadores
   - seed_core: 12-200 chars, frase rica para discovery (OBRIGAT√ìRIO!)
     
     """
        + _build_seed_query_rules()
        + """
     
     """
        + _build_entity_rules_compact()
        + """
     
     **SLACK SEM√ÇNTICO PARA ASPECTOS/M√âTRICAS:**
     
     ‚ùå N√ÉO coloque m√©tricas/aspectos espec√≠ficos na seed:
     - "volume fees tempo-to-fill executive search Brasil" (6 palavras, MUITO espec√≠fica)
     
     ‚úÖ Seed gen√©rica + m√©tricas em must_terms:
     - seed: "mercado executive search Brasil" (4 palavras)
     - must_terms: ["volume", "fees", "tempo-to-fill", "coloca√ß√µes", "market size", ...]
     
   - must_terms: **CR√çTICO - TODOS OS NOMES V√ÉO AQUI (SEMPRE)**
     * Independente de quantos, TODOS os nomes v√£o em must_terms
     * Se usu√°rio mencionou 10 empresas, TODAS v√£o em must_terms
     * Se usu√°rio mencionou produtos/pessoas, TODOS v√£o em must_terms
     * Discovery vai usar must_terms para priorizar e expandir a busca
     * Seed_query + must_terms = m√°xima precis√£o
   - avoid_terms: ru√≠do a evitar
   - time_hint: {"recency": "90d|1y|3y", "strict": true/false}
   - source_bias: ["oficial", "primaria", "secundaria"]
   - evidence_goal: {"official_or_two_independent": true, "min_domains": 3}
   - lang_bias e geo_bias apropriados

üÜï **NOVO v4.7 - SEED_CORE E SEED_FAMILY_HINT:**

**seed_core** (OPCIONAL mas RECOMENDADO):
- Vers√£o RICA da seed_query (1 frase, ‚â§200 chars, sem operadores)
- Usado pelo Discovery para gerar 1-3 varia√ß√µes de busca
- Se ausente, Discovery usa seed_query (curta)
**EXEMPLOS:**
- seed_query: "aquisi√ß√µes headhunting Brasil" (curta, 3 palavras)
- seed_core: "aquisi√ß√µes e parcerias estrat√©gicas no mercado de headhunting no Brasil nos √∫ltimos 12 meses" (rica, contexto completo)

**seed_family_hint** (OPCIONAL, default: "entity-centric"):
- Orienta Discovery sobre TIPO de explora√ß√£o
- Valores: "entity-centric" | "problem-centric" | "outcome-centric" | "regulatory" | "counterfactual"
**TEMPLATES POR FAM√çLIA:**
- **entity-centric**: "<ENTIDADE/SETOR> <tema central> <recorte geotemporal>"
  - Ex: "Spencer Stuart executive search Brasil √∫ltimos 12 meses"
- **problem-centric**: "<problema/risco> <drivers/causas> <contexto/segmento>"
  - Ex: "escassez de talentos C-level causas mercado brasileiro"
- **outcome-centric**: "<efeito/resultado> <indicadores/impacto> <stakeholders>"
  - Ex: "impacto turnover executivo indicadores performance empresas"
- **regulatory**: "<norma/regulador> <exig√™ncias/procedimentos> <abrang√™ncia>"
  - Ex: "LGPD requisitos compliance headhunting Brasil"
- **counterfactual**: "<tese/controv√©rsia> <obje√ß√£o/ant√≠tese> <evid√™ncia-chave>"
  - Ex: "boutiques locais vs internacionais vantagens competitivas evid√™ncias"

**QUANDO USAR CADA FAM√çLIA:**
- **entity-centric** (default): Foco em empresas/produtos/pessoas espec√≠ficas
- **problem-centric**: Quando objetivo menciona "desafios", "riscos", "problemas"
- **outcome-centric**: Quando objetivo menciona "impacto", "resultados", "efeitos"
- **regulatory**: Quando objetivo menciona "compliance", "regula√ß√£o", "normas"
- **counterfactual**: Quando objetivo menciona "comparar", "contrastar", "alternativas"

‚ö†Ô∏è **IMPORTANTE:**
- Se Judge anterior sugeriu seed_family diferente (via NEW_PHASE), RESPEITE-A
- seed_core e seed_family_hint s√£o OPCIONAIS (backwards-compatible)
- Se ausentes, Discovery usa seed_query (comportamento atual)

‚ùå N√ÉO FA√áA:
   - Objetivos gen√©ricos ("explorar", "entender melhor")
   - Seed queries id√™nticas ou muito similares
   - Operadores em seed_query (apenas 3-6 palavras simples)
   - Esquecer @noticias para t√≥picos atuais
   - **IGNORAR ENTIDADES ESPEC√çFICAS** mencionadas no objetivo do usu√°rio
   - Ser gen√©rico quando o usu√°rio foi espec√≠fico (ex: usu√°rio menciona 10 empresas, voc√™ ignora)

üéØ SA√çDA OBRIGAT√ìRIA: APENAS JSON PURO (sem markdown, sem coment√°rios, sem texto extra)

SCHEMA JSON OBRIGAT√ìRIO (com phase_type + seed_core + seed_family_hint):
{{
  "plan_intent": "<objetivo do plano em 1 frase>",
  "total_phases_used": <int OPCIONAL: quantas fases criou, se omitir ser√° inferido>,
  "phases_justification": "<string OPCIONAL: por que esse n√∫mero de fases √© suficiente/econ√¥mico>",
  "assumptions_to_validate": ["<hip√≥tese1>", "<hip√≥tese2>"],
  "phases": [
    {{
      "name": "<nome descritivo da fase>",
      "phase_type": "industry|profiles|news|regulatory|financials|tech",
      "objective": "<pergunta verific√°vel e espec√≠fica>",
      "seed_query": "<3-6 palavras, SEM operadores (@, site:, OR, AND)>",
      "seed_core": "<OPCIONAL: 1 frase rica ‚â§200 chars, sem operadores>",
      "seed_family_hint": "<OPCIONAL: entity-centric|problem-centric|outcome-centric|regulatory|counterfactual>",
      "must_terms": ["<termo1>", "<termo2>"],
      "avoid_terms": ["<ru√≠do/SEO>"],
      "time_hint": {{"recency": "90d|1y|3y", "strict": false}},
      "source_bias": ["oficial","primaria","secundaria"],
      "evidence_goal": {{"official_or_two_independent": true, "min_domains": 3}},
      "lang_bias": ["pt-BR","en"],
      "geo_bias": ["BR","global"],
      "suggested_domains": ["<OPCIONAL: dom√≠nios priorit√°rios>"],
      "suggested_filetypes": ["<OPCIONAL: html, pdf, etc>"]
    }}
    // Repetir para 1 a """
        + str(phases)
        + """ fases (conforme necess√°rio, n√£o obrigat√≥rio usar todas)
  ],
  "quality_rails": {{"min_unique_domains": """
        + str(max(2, phases))
        + """,
    "need_official_or_two_independent": true
  }},
  "budget": {{"max_rounds": 2}}
}}

‚ö†Ô∏è IMPORTANTE: Voc√™ pode criar 1, 2, 3... at√© """
        + str(phases)
        + """ fases. Escolha o n√∫mero que FAZ SENTIDO para o objetivo!
- Objetivo simples/espec√≠fico? ‚Üí 2-3 fases podem bastar
- Objetivo complexo/amplo? ‚Üí Use mais fases (at√© o m√°ximo)

üí° EXEMPLO 1 - Gen√©rico (SEM empresas mencionadas):
"analisar ind√∫stria de executive search no Brasil":
- must_terms: ["executive search", "Brasil", "mercado"]  ‚Üê Gen√©rico OK

üí° EXEMPLO 2A - Poucas entidades (1-3):
"estudo sobre Health+, Vida Melhor e OncoTech no Brasil":
- seed_query: "Health+ Vida Melhor OncoTech sa√∫de digital Brasil"  ‚Üê 3 nomes na seed (OBRIGAT√ìRIO!)
- must_terms: ["Health+", "Vida Melhor", "OncoTech", "sa√∫de digital", "Brasil"]  ‚Üê TODOS aqui tamb√©m!

"estudo sobre Magalu e Via no Brasil":
- seed_query: "Magalu Via varejo digital Brasil"  ‚Üê 2 nomes na seed (OBRIGAT√ìRIO!)
- must_terms: ["Magalu", "Via", "varejo digital", "Brasil"]  ‚Üê TODOS aqui tamb√©m!

üí° EXEMPLO 2B - Muitas entidades (7+):
"estudo sobre Magalu, Via, Americanas, MercadoLivre, Shopee, Amazon, Submarino no Brasil":
- seed_query: "volume mercado varejo digital Brasil"  ‚Üê SEM nomes (tema + aspecto)
- must_terms: ["Magalu", "Via", "Americanas", "MercadoLivre", "Shopee", "Amazon", "Submarino"]  ‚Üê TODOS aqui!

üí° EXEMPLO CORRETO - Estudo de mercado varejo digital Brasil:
"mercado de varejo digital no Brasil (Magalu, Via, Americanas, MercadoLivre)":
```json
{{
  "plan_intent": "Mapear mercado de varejo digital no Brasil com foco em players nacionais e internacionais",
  "assumptions_to_validate": ["Crescimento do e-commerce regional supera o global", "Players locais t√™m vantagens log√≠sticas"],
  "phases": [
    {{"name": "Volume setorial", "phase_type": "industry", "objective": "Qual volume anual do varejo digital no Brasil?", "seed_query": "volume varejo digital Brasil", "seed_core": "volume anual vendas e-commerce Brasil", "must_terms": ["varejo digital", "e-commerce", "Brasil"], "avoid_terms": ["loja f√≠sica"], "time_hint": {{"recency": "1y", "strict": false}}, "source_bias": ["oficial", "primaria"], "evidence_goal": {{"official_or_two_independent": true, "min_domains": 3}}, "lang_bias": ["pt-BR"], "geo_bias": ["BR"]}},
    {{"name": "Tend√™ncias servi√ßos", "phase_type": "industry", "objective": "Quais tend√™ncias e servi√ßos adjacentes surgiram nos √∫ltimos 12 meses?", "seed_query": "tend√™ncias servi√ßos varejo digital Brasil", "seed_core": "tend√™ncias emergentes servi√ßos adjacentes varejo digital Brasil √∫ltimos 12 meses inova√ß√µes tecnologia", "must_terms": ["varejo digital", "omnicanal", "log√≠stica", "Brasil"], "avoid_terms": ["loja f√≠sica"], "time_hint": {{"recency": "1y", "strict": false}}, "source_bias": ["oficial", "primaria"], "evidence_goal": {{"official_or_two_independent": true, "min_domains": 3}}, "lang_bias": ["pt-BR"], "geo_bias": ["BR"]}},
    {{"name": "Perfis e reputa√ß√£o", "phase_type": "profiles", "objective": "Como se posicionam Magalu, Via, Americanas e MercadoLivre?", "seed_query": "reputa√ß√£o players varejo digital Brasil", "seed_core": "Magalu Via Americanas MercadoLivre posicionamento competitivo reputa√ß√£o mercado brasileiro varejo digital √∫ltimos 2 anos", "must_terms": ["Magalu", "Via", "Americanas", "MercadoLivre", "Brasil"], "avoid_terms": ["reclama√ß√µes"], "time_hint": {{"recency": "3y", "strict": false}}, "source_bias": ["oficial", "primaria"], "evidence_goal": {{"official_or_two_independent": true, "min_domains": 3}}, "lang_bias": ["pt-BR"], "geo_bias": ["BR"]}},
    {{"name": "Eventos recentes", "phase_type": "news", "objective": "Quais aquisi√ß√µes ou mudan√ßas ocorreram nos √∫ltimos 90 dias?", "seed_query": "@noticias aquisi√ß√µes varejo digital Brasil", "seed_core": "aquisi√ß√µes parcerias mudan√ßas estrat√©gicas Magalu Via Americanas MercadoLivre varejo digital Brasil √∫ltimos 90 dias", "must_terms": ["Magalu", "Via", "Americanas", "MercadoLivre", "Brasil"], "avoid_terms": ["promo√ß√µes"], "time_hint": {{"recency": "90d", "strict": true}}, "source_bias": ["oficial", "primaria"], "evidence_goal": {{"official_or_two_independent": true, "min_domains": 2}}, "lang_bias": ["pt-BR"], "geo_bias": ["BR"]}}
  ],
  "quality_rails": {{"min_unique_domains": 3, "need_official_or_two_independent": true}},
  "budget": {{"max_rounds": 2}}
}}
```

**OBSERVA√á√ÉO CR√çTICA sobre o exemplo acima:**
- ‚úÖ Fase 1 seed: "volume fees EXECUTIVE SEARCH Brasil" (tema presente!)
- ‚úÖ Fase 2 seed: "tend√™ncias servi√ßos HEADHUNTING Brasil" (tema presente!)
- ‚úÖ Fase 3 seed: "reputa√ß√£o players EXECUTIVE SEARCH Brasil" (tema presente!)
- ‚úÖ Fase 4 seed: "@noticias aquisi√ß√µes HEADHUNTING Brasil" (tema presente!)
- ‚ö†Ô∏è SEM tema: queries gen√©ricas que retornam noise (curr√≠culos, vagas, etc)

üéØ AGORA CRIE SEU PLANO:

‚ö†Ô∏è **CR√çTICO - POL√çTICAS DE phase_type:**
- **industry**: panorama setorial (volume, tend√™ncias). must_terms: termos setoriais + geo (‚â§5). time_hint: 1y ou 3y
- **profiles**: perfis espec√≠ficos de players. must_terms: **TODOS** os players + geo. time_hint: 3y
- **news**: eventos/not√≠cias do setor. must_terms: **TODOS** os players + geo. time_hint: 1y (padr√£o) OU 90d (apenas breaking news expl√≠cito)
  - ‚ö†Ô∏è NOVO (v4.4): News padr√£o = 1y (captura eventos relevantes 12 meses)
  - ‚ö†Ô∏è Exce√ß√£o: 90d apenas se objective menciona "√∫ltimos 90 dias" / "√∫ltimos 3 meses" / "breaking news"
- **regulatory**: marcos regulat√≥rios. must_terms: leis/normas + geo. time_hint: 1y ou 3y
- **financials**: m√©tricas financeiras. must_terms: players + m√©tricas. time_hint: 1y
- **tech**: especifica√ß√µes t√©cnicas. must_terms: tecnologias + componentes. time_hint: 1y

**INSTRU√á√ïES FINAIS:**

**üîí VALIDA√á√ÉO DE KEY_QUESTIONS COVERAGE (P0 - CR√çTICO):**
Antes de retornar o plano, verifique OBRIGATORIAMENTE:
1. **Para CADA key_question** listada no payload do Estrategista ‚Üí identifique qual fase a responde
2. **Se alguma key_question N√ÉO for coberta** ‚Üí crie fase adicional espec√≠fica
3. **Mapeamento mental obrigat√≥rio:**
   - Key_Q "Qual volume...?" ‚Üí Fase "Volume setorial" (industry, 1y ou 3y)
   - Key_Q "Quais tend√™ncias...?" ‚Üí Fase "Tend√™ncias/evolu√ß√£o" (industry, 1y) ‚Üê CR√çTICO!
   - Key_Q "Qual reputa√ß√£o...?" ‚Üí Fase "Perfis players" (profiles, 3y)
   - Key_Q "Quais eventos/not√≠cias...?" ‚Üí Fase "Eventos mercado" (news, 1y) ‚Üê v4.4: 1y padr√£o!
   - Key_Q "√öltimos dias/90d...?" ‚Üí Fase "Breaking news" (news, 90d) ‚Üê v4.4: apenas se expl√≠cito!
   - Key_Q "Quais riscos/oportunidades 3-5 anos?" ‚Üí Fase "Tend√™ncias/evolu√ß√£o" (industry, 1y)

**SE** alguma key_question n√£o tiver fase correspondente ‚Üí **ERRO CR√çTICO** ‚Üí crie fase adicional.

**EXEMPLO DE VALIDA√á√ÉO:**
```
10 key_questions fornecidas:
‚úì Q1-Q3 ‚Üí Fase 1 (industry 3y)
‚úì Q4-Q6 ‚Üí Fase 2 (profiles 3y) 
‚úì Q7-Q9 ‚Üí Fase 3 (tend√™ncias 1y)  ‚Üê Se esta fase n√£o existir, 30% das questions ficam sem resposta!
‚úì Q10 ‚Üí Fase 4 (news 90d)
```

**AP√ìS VALIDA√á√ÉO:**
1. Use as key_questions e entities j√° analisadas (acima) para criar as fases
2. Atribua phase_type correto para cada fase
3. Aplique as pol√≠ticas de must_terms por phase_type
4. Retorne APENAS JSON PURO (sem markdown, sem texto extra, sem racioc√≠nio)

üéØ **VALIDA√á√ÉO DE RESEARCH_OBJECTIVES COVERAGE (P0 - CR√çTICO):**

Antes de retornar o plano, verifique OBRIGATORIAMENTE:

1. **Para CADA research_objective** listado no payload ‚Üí identifique qual fase cobre
2. **Se algum objective N√ÉO for coberto** ‚Üí ajuste objectives de fase ou crie fase adicional

**EXEMPLO DE MAPEAMENTO:**
```
RESEARCH_OBJECTIVES do estrategista:
1. "Mapear principais players..." ‚Üí Fase "Perfis players" (phase_type: profiles)
2. "Comparar ofertas e pricing..." ‚Üí Fase "Modelos de pre√ßo" (phase_type: industry)
3. "Segmentar demanda por setor..." ‚Üí Fase "Panorama mercado" (phase_type: industry)
4. "Avaliar reputa√ß√£o e m√≠dia..." ‚Üí Fase "Perfis players" (phase_type: profiles)
5. "Identificar riscos regulat√≥rios..." ‚Üí Fase "Panorama mercado" (objective espec√≠fico)
6. "Recomendar estrat√©gias M&A..." ‚Üí Fase "Tend√™ncias e oportunidades" (phase_type: industry)
7. "Produzir matriz competitiva..." ‚Üí Fase "Perfis players" (phase_type: profiles)
```

**SE** algum research_objective n√£o tiver fase que o cubra ‚Üí ajuste fase existente ou crie nova.

**REGRA:** Objectives de fase devem ser MAIS ESPEC√çFICOS que research_objectives (subconjunto detalhado).
- ‚ùå ERRADO: Objective gen√©rico "Analisar mercado" (n√£o cobre research_objective espec√≠fico)
- ‚úÖ CERTO: Objective "Quantificar market share por tipo de player e identificar riscos regulat√≥rios" (cobre research_objectives 1, 3, 5)

**DICA:** Se um research_objective menciona "matriz competitiva", uma fase DEVE ter isso explicitamente no objective.
Se menciona "M&A/expans√£o", uma fase DEVE cobrir fus√µes/aquisi√ß√µes/parcerias.

---

üéØ **ACCEPTANCE CRITERIA (VALIDA√á√ÉO FINAL OBRIGAT√ìRIA):**

Antes de retornar o JSON, verifique:

‚úÖ **ESTRUTURA:**
- [ ] 1-3 fases criadas (pode ser menos que o m√°ximo se suficiente)
- [ ] Cada fase tem TODOS os campos obrigat√≥rios
- [ ] JSON v√°lido (sem markdown, sem coment√°rios, sem texto extra)

‚úÖ **SEED_QUERY (curta, para UI/telemetria):**
- [ ] 3-6 palavras (excluindo @noticias)
- [ ] SEM operadores (site:, filetype:, OR, AND, aspas)
- [ ] Cont√©m tema central + aspecto + geo
- [ ] Se 1-3 entidades: TODOS os nomes na seed_query

‚úÖ **SEED_CORE (rica, para Discovery):**
- [ ] 12-200 caracteres (1 frase completa)
- [ ] SEM operadores
- [ ] Inclui: entidades + tema + aspecto + recorte geotemporal
- [ ] N√ÉO repete seed_query sem adicionar pelo menos 1 aspecto + 1 entidade

‚úÖ **MUST_TERMS:**
- [ ] 2-8 termos (n√£o vazio, n√£o excessivo)
- [ ] TODAS as entidades can√¥nicas inclu√≠das (quando aplic√°vel)
- [ ] SEM overlap com avoid_terms

‚úÖ **OBJECTIVE:**
- [ ] Pergunta verific√°vel (verbo: mapear/identificar/comparar/quantificar)
- [ ] Condi√ß√£o de sucesso clara (ex: "Quantificar market share por player")
- [ ] N√ÉO gen√©rico (ex: "entender melhor", "explorar")

‚úÖ **MECE (N√ÉO OVERLAP COM DISCOVERY):**
- [ ] N√ÉO gerar varia√ß√µes da seed (Discovery far√° isso)
- [ ] N√ÉO criar m√∫ltiplas queries por fase (apenas 1 seed_query + 1 seed_core)

‚úÖ **NEWS PHASES:**
- [ ] time_hint.recency = "1y" (padr√£o) OU "90d" (apenas se expl√≠cito)
- [ ] time_hint.strict = true
- [ ] @noticias na seed_query

üîé **SELF-CHECK (execute ANTES de retornar JSON):**

Antes de retornar o plano, verifique OBRIGATORIAMENTE cada item abaixo:

1Ô∏è‚É£ **Todas as key_questions cobertas?**
   - Cada KEY_QUESTION do payload tem ‚â•1 fase correspondente?
   - Se alguma ficou √≥rf√£ ‚Üí crie fase adicional

2Ô∏è‚É£ **Not√≠cias (se pedidas)?**
   - Se usu√°rio mencionou "not√≠cias" OU √© "estudo de mercado" ‚Üí existe fase type="news"?
   - Fase news tem time_hint.recency="1y" (n√£o 90d) e strict=true?
   - Seed_query da fase news tem "@noticias" + tema + entidades?

3Ô∏è‚É£ **Seeds v√°lidas?**
   - Cada seed_query tem 3-8 palavras (excluindo @noticias)?
   - seed_query N√ÉO usa operadores (site:, filetype:, OR, AND, aspas)?
   - seed_core tem ‚â•12 caracteres e √© DIFERENTE de seed_query?
   - seed_core inclui pelo menos 1 entidade + 1 aspecto adicional?
   - seed_core √© 1 frase rica (n√£o apenas palavras soltas)?

4Ô∏è‚É£ **ENTIDADES (cobertura m√≠nima):**
   - Se ‚â§3 entidades ‚Üí elas aparecem em must_terms de pelo menos 70% das fases?
   - Seed_query das fases de profiles/news incluem TODAS as entidades?
   - Se >3 entidades ‚Üí pelo menos as 3 principais em must_terms de cada fase?

5Ô∏è‚É£ **MECE (sem overlap):**
   - Objectives das fases s√£o mutualmente exclusivos (n√£o overlap √≥bvio)?
   - Se detectou overlap ‚Üí reescreva objectives antes de retornar
   - Fases cobrem TODO o escopo (nenhuma key_question √≥rf√£)?

‚úÖ **SEED_CORE VALIDATION:**
- [ ] TODAS as fases t√™m seed_core (12-200 chars)
- [ ] seed_core ‚â† seed_query (seed_core √© EXPANS√ÉO)
- [ ] seed_core inclui: entidades + tema + aspecto + geo/temporal
- [ ] seed_core SEM operadores (@, site:, OR, AND)

‚úÖ **Se todos os checks passarem ‚Üí retorne JSON**
‚ùå **Se algum falhar ‚Üí corrija ANTES de retornar**

FORMATO DE SA√çDA:
[JSON do plano completo]
"""
    )
def _build_analyst_prompt(query: str, phase_context: Dict = None) -> str:
    """Build unified Analyst prompt used by both Manual and SDK routes."""
    # Extrair informa√ß√µes da fase atual
    phase_info = ""
    if phase_context:
        phase_name = phase_context.get("name", "Fase atual")
        # Contract usa "objetivo" (PT), n√£o "objective" (EN)
        phase_objective = phase_context.get("objetivo") or phase_context.get(
            "objective", ""
        )

        # Adicionar must_terms e avoid_terms para guiar o Analyst
        must_terms = phase_context.get("must_terms", [])
        avoid_terms = phase_context.get("avoid_terms", [])

        phase_info = (
            f"\n**FASE ATUAL:** {phase_name}\n**Objetivo da Fase:** {phase_objective}"
        )

        if must_terms:
            # Mostrar at√© 8 termos priorit√°rios (evitar prompt muito longo)
            terms_display = ", ".join(must_terms[:8])
            if len(must_terms) > 8:
                terms_display += f" (e mais {len(must_terms) - 8})"
            phase_info += f"\n**Termos Priorit√°rios:** {terms_display}"

            # P0: Enfatizar obrigatoriedade dos must_terms
            phase_info += f"""
‚ö†Ô∏è **MUST_TERMS S√ÉO OBRIGAT√ìRIOS:**
- TODOS os fatos extra√≠dos DEVEM mencionar pelo menos 1 termo priorit√°rio
- Se contexto n√£o menciona must_terms, reportar em lacunas (ex: "Falta dados sobre [termo]")
- Coverage_score deve penalizar aus√™ncia de must_terms
- Exemplo: Query "market share Flow Executive Brasil" + must_terms ["Flow Executive", "market share", "Brasil"]
  ‚Üí ‚úÖ Fato CORRETO: "Flow Executive tem 15% de market share no Brasil" (3/3 must_terms)
  ‚Üí ‚ùå Fato INCORRETO: "Mercado de consultoria no Brasil cresceu 10%" (1/3 must_terms - gen√©rico demais)"""

        if avoid_terms:
            # Mostrar at√© 5 termos a evitar
            avoid_display = ", ".join(avoid_terms[:5])
            if len(avoid_terms) > 5:
                avoid_display += f" (e mais {len(avoid_terms) - 5})"
            phase_info += f"\n**Evitar:** {avoid_display}"

    # Extrair objetivo espec√≠fico para enfatizar
    objective_emphasis = ""
    if phase_context:
        # Contract usa "objetivo" (PT), n√£o "objective" (EN)
        obj = phase_context.get("objetivo") or phase_context.get("objective", "")
        if obj:
            objective_emphasis = f"\n\nüéØ **SUA MISS√ÉO ESPEC√çFICA NESTA FASE:**\n{obj}\n\n**FOQUE APENAS** em extrair fatos que RESPONDEM DIRETAMENTE esta pergunta/objetivo!"

    # Usar dicion√°rios globais de prompts
    system_prompt = PROMPTS["analyst_system"]
    calibration_rules = PROMPTS["analyst_calibration"]
    
    prompt_template = (
        system_prompt
        + "\n\nOBJETIVO: {query_text}{phase_block}{objective_block}\n\n"
        + calibration_rules
        + "\n\nJSON:\n"
        + "{\n"
        + '  "summary": "Resumo FOCADO NO OBJETIVO da fase (n√£o gen√©rico!)",\n'
        + '  "facts": [\n'
        + '    {\n'
        + '      "texto": "Fato que RESPONDE ao objetivo da fase",\n'
        + '      "confian√ßa": "alta|m√©dia|baixa", \n'
        + '      "evidencias": [{"url": "...", "trecho": "..."}]\n'
        + '    }\n'
        + '  ],\n'
        + '  "lacunas": ["O que AINDA FALTA para responder completamente o objetivo"],\n'
        + '  "self_assessment": {\n'
        + '    "coverage_score": 0.7,\n'
        + '    "confidence": "alta|m√©dia|baixa",\n'
        + '    "gaps_critical": true,\n'
        + '    "suggest_refine": false,\n'
        + '    "suggest_pivot": true,\n'
        + '    "reasoning": "Por que pivot: lacuna precisa de dados 90d (fase atual: 3y)"\n'
        + '  }\n'
        + '}\n\n'
        + "Retorne APENAS JSON."
    )

    # üî¥ FIX P0: Usar str.replace em vez de .format para evitar KeyError com literais JSON no template
    # O template cont√©m exemplos JSON com {"summary": ...} que .format() interpreta como placeholder
    final_prompt = prompt_template.replace("{query_text}", query)
    final_prompt = final_prompt.replace("{phase_block}", phase_info)
    final_prompt = final_prompt.replace("{objective_block}", objective_emphasis)

    return final_prompt
class JudgeLLM:
    def __init__(self, valves):
        self.valves = valves
        # Usar modelo espec√≠fico se configurado, sen√£o usa modelo padr√£o
        model = valves.LLM_MODEL_JUDGE or valves.LLM_MODEL
        self.model_name = model
        self.llm = _get_llm(valves, model_name=model)
        # Base kwargs: ser√£o filtrados por get_safe_llm_params (GPT-5 n√£o aceita temperature)
        self.generation_kwargs = {"temperature": 0.3}

    def _calculate_phase_score(self, metrics: Dict[str, float]) -> float:
        """Calcula phase_score audit√°vel usando pesos configur√°veis (v4.7)

        F√≥rmula:
        phase_score = w_cov * coverage
                    + w_nf * novel_fact_ratio
                    + w_nd * novel_domain_ratio
                    + w_div * domain_diversity
                    - w_contra * contradiction_score

        Args:
            metrics: Dict com coverage, novel_fact_ratio, novel_domain_ratio,
                    domain_diversity, contradiction_score

        Returns:
            Score normalizado 0.0-1.0 (pode ser negativo se contradi√ß√µes altas)
        """
        weights = getattr(
            self.valves,
            "PHASE_SCORE_WEIGHTS",
            {
                "w_cov": 0.35,
                "w_nf": 0.25,
                "w_nd": 0.15,
                "w_div": 0.15,
                "w_contra": 0.40,
            },
        )

        coverage = metrics.get("coverage", 0.0)
        novel_fact_ratio = metrics.get("novel_fact_ratio", 0.0)
        novel_domain_ratio = metrics.get("novel_domain_ratio", 0.0)
        domain_diversity = metrics.get("domain_diversity", 0.0)
        contradiction_score = metrics.get("contradiction_score", 0.0)

        score = (
            weights["w_cov"] * coverage
            + weights["w_nf"] * novel_fact_ratio
            + weights["w_nd"] * novel_domain_ratio
            + weights["w_div"] * domain_diversity
            - weights["w_contra"] * contradiction_score
        )

        return round(score, 3)

    def _switch_seed_family(self, current_family: str) -> str:
        """Troca fam√≠lia de seed para explora√ß√£o sistem√°tica (v4.7)

        Ciclo: entity-centric ‚Üí problem-centric ‚Üí outcome-centric ‚Üí regulatory ‚Üí counterfactual ‚Üí entity-centric

        Args:
            current_family: Fam√≠lia atual

        Returns:
            Pr√≥xima fam√≠lia no ciclo
        """
        family_order = [
            "entity-centric",
            "problem-centric",
            "outcome-centric",
            "regulatory",
            "counterfactual",
        ]

        try:
            idx = family_order.index(current_family)
            next_idx = (idx + 1) % len(family_order)
            return family_order[next_idx]
        except ValueError:
            # Fam√≠lia desconhecida, retornar default
            return "problem-centric"  # Primeira alternativa ao entity-centric

    def _calculate_weighted_fact_delta(self, analysis: dict) -> float:
        """
        Calculate weighted fact delta based on confidence scores.
        
        WIN #2: Instead of simple count, weight facts by confidence:
        - alta confian√ßa = 1.0
        - m√©dia confian√ßa = 0.7  
        - baixa confian√ßa = 0.4
        
        Args:
            analysis: Analysis dict containing facts list
            
        Returns:
            Weighted delta (float) - sum of confidence scores for new facts
        """
        facts = analysis.get("facts", [])
        if not facts:
            return 0.0
            
        # Map confidence levels to weights
        confidence_weights = {
            "alta": 1.0,
            "m√©dia": 0.7,
            "baixa": 0.4
        }
        
        # Calculate weighted sum
        weighted_sum = 0.0
        for fact in facts:
            confidence = fact.get("confian√ßa", "m√©dia")  # Default to m√©dia
            weight = confidence_weights.get(confidence, 0.7)  # Default weight
            weighted_sum += weight
            
        return weighted_sum
    
    def _calculate_multi_dimensional_similarity(
        self, new_obj: str, new_seed: str, new_type: str, existing_phases: list
    ) -> tuple[float, int]:
        """
        Calculate multi-dimensional similarity for duplicate detection.
        
        CORE FIX: Compare 3 dimensions with weights:
        - 0.5 * objective similarity (TF-IDF cosine similarity)
        - 0.3 * seed_query similarity (Jaccard similarity)
        - 0.2 * phase_type overlap (exact match)
        
        Args:
            new_obj: New phase objective
            new_seed: New phase seed query
            new_type: New phase type
            existing_phases: List of existing phases
            
        Returns:
            Tuple of (max_weighted_score, index_of_most_similar_phase)
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        
        max_weighted_score = 0.0
        max_sim_idx = 0
        
        if not existing_phases:
            return max_weighted_score, max_sim_idx
            
        # Extract existing objectives
        existing_objs = [
            p.get("objetivo") or p.get("objective", "")
            for p in existing_phases
        ]
        existing_seeds = [
            p.get("seed_query", "") for p in existing_phases
        ]
        existing_types = [
            p.get("phase_type", "") for p in existing_phases
        ]
        
        # 1. Objective similarity (TF-IDF cosine similarity)
        objective_sim = 0.0
        if new_obj and existing_objs and any(existing_objs):
            all_objs = [new_obj] + existing_objs
            vectorizer = TfidfVectorizer()
            vectors = vectorizer.fit_transform(all_objs)
            similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()
            objective_sim = similarities.max() if len(similarities) > 0 else 0.0
        
        # 2. Seed query similarity (Jaccard similarity)
        seed_sim = 0.0
        if new_seed and existing_seeds and any(existing_seeds):
            new_tokens = set(new_seed.lower().split())
            max_jaccard = 0.0
            for existing_seed in existing_seeds:
                if existing_seed:
                    existing_tokens = set(existing_seed.lower().split())
                    intersection = len(new_tokens & existing_tokens)
                    union = len(new_tokens | existing_tokens)
                    jaccard = intersection / union if union > 0 else 0.0
                    max_jaccard = max(max_jaccard, jaccard)
            seed_sim = max_jaccard
        
        # 3. Phase type overlap (exact match)
        type_overlap = 0.0
        if new_type and existing_types:
            type_overlap = 1.0 if new_type in existing_types else 0.0
        
        # Calculate weighted score
        weighted_score = (
            0.5 * objective_sim +
            0.3 * seed_sim +
            0.2 * type_overlap
        )
        
        # Find the most similar phase by iterating and tracking max
        if existing_phases:
            for idx, phase in enumerate(existing_phases):
                phase_obj = phase.get('objetivo') or phase.get('objective', '')
                phase_seed = phase.get('seed_query', '')
                phase_type = phase.get('phase_type', '')
                
                p_obj_sim = 0.0
                if new_obj and phase_obj:
                    try:
                        all_o = [new_obj, phase_obj]
                        vect = TfidfVectorizer()
                        vecs = vect.fit_transform(all_o)
                        sims = cosine_similarity(vecs[0:1], vecs[1:]).flatten()
                        p_obj_sim = sims[0] if len(sims) > 0 else 0.0
                    except: pass
                
                p_seed_sim = 0.0
                if new_seed and phase_seed:
                    nt = set(new_seed.lower().split())
                    pt = set(phase_seed.lower().split())
                    p_seed_sim = len(nt & pt) / len(nt | pt) if len(nt | pt) > 0 else 0.0
                
                p_type_over = 1.0 if new_type == phase_type and new_type else 0.0
                phase_score = 0.5 * p_obj_sim + 0.3 * p_seed_sim + 0.2 * p_type_over
                
                if phase_score > max_weighted_score:
                    max_weighted_score = phase_score
                    max_sim_idx = idx
            
        return max_weighted_score, max_sim_idx

    def _validate_quality_rails(
        self, analysis, phase_context, intent_profile: Optional[str] = None
    ):
        """Gates M√çNIMOS - apenas safety net para casos extremos

        REBALANCED (v4.5.1): Reduzido de 6 gates r√≠gidos para 2 gates m√≠nimos
        Filosofia: Prompts guiam, gates alertam casos extremos
        """
        facts = analysis.get("facts", [])

        # Gate 1: ZERO fatos (caso extremo √≥bvio)
        if not facts:
            return {
                "passed": False,
                "reason": "Sem fatos encontrados - imposs√≠vel responder ao objetivo",
                "suggested_query": "buscar fontes espec√≠ficas e verific√°veis",
            }

        # Gate 2: Combina√ß√£o de problemas (evid√™ncia fraca + coverage baixo)
        # Apenas bloqueia quando AMBOS s√£o muito baixos
        metrics = _extract_quality_metrics(facts)
        evidence_coverage = metrics["facts_with_evidence"] / len(facts) if facts else 0
        coverage_score = analysis.get("self_assessment", {}).get("coverage_score", 0)

        # NOVO THRESHOLD: 50% evid√™ncia + 50% coverage (muito mais permissivo)
        if evidence_coverage < 0.5 and coverage_score < 0.5:
            return {
                "passed": False,
                "reason": f"Qualidade muito baixa: {evidence_coverage*100:.0f}% evid√™ncia + {coverage_score*100:.0f}% coverage (ambos <50%)",
                "suggested_query": "buscar fontes com dados espec√≠ficos e verific√°veis",
            }

        return {"passed": True}

    def _check_evidence_staleness(
        self, facts, phase_context, intent_profile: Optional[str] = None
    ):
        """Verifica staleness (recency) das evid√™ncias (P1.2) com gates por perfil"""
        if not facts:
            return {"passed": True}

        # Extrair time_hint do phase_context
        time_hint = phase_context.get("time_hint", {}) if phase_context else {}
        recency = time_hint.get("recency", "1y")
        strict = time_hint.get("strict", False)
        # Aplicar gate por perfil (se perfil exigir strict, for√ßa strict)
        try:
            profile = (
                intent_profile
                or getattr(self.valves, "INTENT_PROFILE", None)
                or "company_profile"
            )
            gates = self.valves.GATES_BY_PROFILE.get(profile, {})
            if gates.get("staleness_strict"):
                strict = True
        except Exception:
            pass

        if not strict:
            return {"passed": True}  # Se n√£o √© strict, n√£o verifica staleness

        # Converter recency para dias
        recency_days = self._parse_recency_to_days(recency)
        if recency_days is None:
            return {"passed": True}  # Recency inv√°lido, n√£o verifica

        # Verificar idade das evid√™ncias
        from datetime import datetime, timedelta

        cutoff_date = datetime.now() - timedelta(days=recency_days)

        old_evidence_count = 0
        total_evidence_count = 0

        for fact in facts:
            evidencias = fact.get("evidencias", [])
            for ev in evidencias:
                total_evidence_count += 1
                ev_date_str = ev.get("data", "")

                if ev_date_str:
                    try:
                        # Tentar parsear data (YYYY-MM-DD)
                        ev_date = datetime.fromisoformat(ev_date_str)
                        if ev_date < cutoff_date:
                            old_evidence_count += 1
                    except:
                        # Se n√£o conseguir parsear, assumir que √© antiga
                        old_evidence_count += 1

        if total_evidence_count == 0:
            return {"passed": True}

        old_ratio = old_evidence_count / total_evidence_count

        # KPI: se >50% das evid√™ncias s√£o antigas, bloquear DONE
        if old_ratio > 0.5:
            return {
                "passed": False,
                "reason": f"{old_ratio*100:.0f}% evid√™ncias fora da janela de {recency} (strict=true)",
                "suggested_query": f"Buscar evid√™ncias mais recentes (√∫ltimos {recency})",
            }

        return {"passed": True}

    def _parse_recency_to_days(self, recency):
        """Converte recency string para dias"""
        if recency == "90d":
            return 90
        elif recency == "1y":
            return 365
        elif recency == "3y":
            return 1095
        else:
            return None

    async def run(
        self,
        user_prompt: str,
        analysis: Dict[str, Any],
        phase_context: Dict[str, Any] = None,
        telemetry_loops: Optional[List[Dict[str, Any]]] = None,
        intent_profile: Optional[str] = None,
        full_contract: Optional[Dict] = None,
        valves=None,
        refine_queries: Optional[List[Dict]] = None,
        phase_candidates: Optional[List[Dict]] = None,
        previous_queries: Optional[List[str]] = None,
        failed_queries: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        if not self.llm:
            raise ValueError("LLM n√£o configurado")

        phase_info = ""
        if phase_context:
            phase_info = f"\n**Crit√©rios:** {', '.join(phase_context.get('accept_if_any_of', []))}"

        # Build judge prompt inline (temporary implementation)
        prompt = f"""
        Analise os resultados da pesquisa e decida o pr√≥ximo passo.
        
        Usu√°rio: {user_prompt}
        An√°lise: {analysis}
        Contexto da Fase: {phase_context}
        Loops de Telemetria: {len(telemetry_loops)}
        
        Retorne JSON com:
        - verdict: "done" | "refine" | "new_phase"
        - reasoning: explica√ß√£o da decis√£o
        - next_query: pr√≥xima query se refine
        - phase_score: 0.0-1.0
        """

        # Filtrar par√¢metros incompat√≠veis com GPT-5/O1
        safe_params = get_safe_llm_params(self.model_name, self.generation_kwargs)

        # Use retry function if enabled, otherwise single attempt
        if getattr(self.valves, "ENABLE_LLM_RETRY", True):
            max_retries = int(getattr(self.valves, "LLM_MAX_RETRIES", 3) or 3)
            out = await _safe_llm_run_with_retry(
                self.llm,
                prompt,
                safe_params,
                timeout=self.valves.LLM_TIMEOUT_DEFAULT,
                max_retries=max_retries,
            )
        else:
            out = await _safe_llm_run_with_retry(
                self.llm,
                prompt,
                safe_params,
                timeout=self.valves.LLM_TIMEOUT_DEFAULT,
                max_retries=1,
            )
        if not out:
            raise RuntimeError("Judge failed")

        parsed = parse_json_resilient(out.get("replies", [""])[0], mode="balanced", allow_arrays=False)
        if not parsed:
            raise ValueError("Judge output inv√°lido")

        # ===== JUDGE ENXUTO: 3 SINAIS AUTOM√ÅTICOS, 3 REGRAS MECE =====
        # Filosofia: Gen√©rico, sem thresholds manuais, sem whitelists

        # SINAL 1: Lacunas expl√≠citas (do Analyst)
        has_lacunas = bool(analysis.get("lacunas"))

        # SINAL 2: Tra√ß√£o (crescimento absoluto em dom√≠nios OU fatos)
        traction = True  # Default para primeiro loop
        if telemetry_loops and len(telemetry_loops) >= 2:
            last = telemetry_loops[-1]
            prev = telemetry_loops[-2]
            delta_domains = last.get("unique_domains", 0) - prev.get(
                "unique_domains", 0
            )
            
            # ‚úÖ WIN #2: Weight traction by fact confidence instead of simple count
            delta_facts = self._calculate_weighted_fact_delta(analysis)
            traction = (delta_domains > 0) or (delta_facts > 0)

        # SINAL 3: Dois loops flat consecutivos (histerese)
        two_flat_loops = False
        if telemetry_loops and len(telemetry_loops) >= 3:
            last = telemetry_loops[-1]
            prev = telemetry_loops[-2]
            prev_prev = telemetry_loops[-3]

            # √öltimo loop flat?
            delta_d_last = last.get("unique_domains", 0) - prev.get("unique_domains", 0)
            delta_f_last = last.get("n_facts", 0) - prev.get("n_facts", 0)
            last_flat = (delta_d_last == 0) and (delta_f_last == 0)

            # Pen√∫ltimo loop flat?
            delta_d_prev = prev.get("unique_domains", 0) - prev_prev.get(
                "unique_domains", 0
            )
            delta_f_prev = prev.get("n_facts", 0) - prev_prev.get("n_facts", 0)
            prev_flat = (delta_d_prev == 0) and (delta_f_prev == 0)

            two_flat_loops = last_flat and prev_flat

        # SINAL 4: Key Questions Status (do LLM Judge, n√£o heur√≠stica)
        # Judge LLM avalia: coverage, blind_spots, se descobertas invalidam hip√≥teses
        key_questions_coverage = 1.0  # Default: 100%
        blind_spots = []

        # Extrair key_questions_status do JSON do Judge (se dispon√≠vel)
        kq_status = parsed.get("key_questions_status", {})
        if kq_status:
            key_questions_coverage = float(kq_status.get("coverage", 1.0))
            blind_spots = kq_status.get("blind_spots", [])

        # SINAL 5: Blind Spots Cr√≠ticos (descobertas que invalidam hip√≥teses)
        loops = len(telemetry_loops) if telemetry_loops else 0
        blind_spots_signal = bool(blind_spots) and (
            loops >= 1 or len(blind_spots) >= 3
        )

        # ===== v4.7: CALCULAR PHASE_SCORE AUDIT√ÅVEL =====
        # Coletar m√©tricas necess√°rias para o score
        facts = analysis.get("facts", [])
        lacunas = analysis.get("lacunas", [])

        # M√©tricas de telemetria (√∫ltima itera√ß√£o)
        last_loop = telemetry_loops[-1] if telemetry_loops else {}
        novel_fact_ratio = last_loop.get("new_facts_ratio", 0.0)
        novel_domain_ratio = last_loop.get("new_domains_ratio", 0.0)
        unique_domains = last_loop.get("unique_domains", 0)

        # Calcular domain_diversity (Herfindahl invertido ou simples ratio)
        # Simplifica√ß√£o: usar unique_domains / facts como proxy
        domain_diversity = (
            min(1.0, unique_domains / max(len(facts), 1)) if facts else 0.0
        )

        # Calcular contradiction_score (do Analyst ou telemetria)
        sa = analysis.get("self_assessment", {})
        contradiction_score = 0.0
        try:
            # Se Analyst reportou contradi√ß√µes, usar como score
            contradictions_count = last_loop.get("contradictions", 0)
            if contradictions_count > 0:
                contradiction_score = min(
                    1.0, contradictions_count / max(len(facts), 1)
                )
        except Exception:
            pass

        # Montar dict de m√©tricas para phase_score
        phase_metrics = {
            "coverage": key_questions_coverage,  # Do LLM Judge
            "novel_fact_ratio": novel_fact_ratio,
            "novel_domain_ratio": novel_domain_ratio,
            "domain_diversity": domain_diversity,
            "contradiction_score": contradiction_score,
            "loops_without_gain": 2 if two_flat_loops else (0 if traction else 1),
        }

        # Calcular phase_score
        phase_score = self._calculate_phase_score(phase_metrics)

        # Obter gates do perfil/phase_type
        phase_type = (
            phase_context.get("phase_type", "industry") if phase_context else "industry"
        )
        gates = getattr(self.valves, "GATES_BY_PROFILE", {}).get(
            phase_type, {"threshold": 0.60, "two_flat_loops": 2}
        )
        threshold = gates.get("threshold", 0.60)
        required_flat_loops = gates.get("two_flat_loops", 2)

        # Calcular flat_streak (quantos loops consecutivos sem ganho)
        flat_streak = 0
        if telemetry_loops and len(telemetry_loops) >= 2:
            for i in range(len(telemetry_loops) - 1, 0, -1):
                curr = telemetry_loops[i]
                prev = telemetry_loops[i - 1]
                delta_d = curr.get("unique_domains", 0) - prev.get("unique_domains", 0)
                delta_f = curr.get("n_facts", 0) - prev.get("n_facts", 0)
                if delta_d == 0 and delta_f == 0:
                    flat_streak += 1
                else:
                    break

        # Calcular overlap_similarity (similaridade entre fatos desta fase vs anteriores)
        # Simplifica√ß√£o: usar novel_fact_ratio invertido como proxy
        overlap_similarity = 1.0 - novel_fact_ratio if novel_fact_ratio > 0 else 0.0

        # ===== DECIS√ÉO PROGRAM√ÅTICA BASEADA EM PHASE_SCORE (v4.7) =====
        programmatic_decision = {}
        seed_family_switch = None

        # ‚úÖ Reaproveitar new_phase do Judge LLM (se dispon√≠vel)
        judge_new_phase = parsed.get("new_phase", {})

        # ===== SAFETY RAILS (prioridade m√°xima, sobrescrevem score) =====

        # Rail 1: Contradi√ß√µes cr√≠ticas ‚Üí NEW_PHASE imediato com seed_family switch
        contradiction_hard_gate = getattr(self.valves, "CONTRADICTION_HARD_GATE", 0.75)
        if contradiction_score >= contradiction_hard_gate:
            current_family = (
                phase_context.get("seed_family_hint", "entity-centric")
                if phase_context
                else "entity-centric"
            )
            seed_family_switch = self._switch_seed_family(current_family)
            programmatic_decision = {
                "verdict": "new_phase",
                "reasoning": f"Contradi√ß√µes cr√≠ticas ({contradiction_score:.2f} ‚â• {contradiction_hard_gate}). Trocar √¢ngulo",
                "seed_family": seed_family_switch,
            }

        # Rail 2: Duplica√ß√£o alta (overlap ‚â• 0.90) ‚Üí REFINE
        elif overlap_similarity >= 0.90:
            programmatic_decision = {
                "verdict": "refine",
                "reasoning": f"Overlap muito alto ({overlap_similarity:.2f}). Refinar busca",
            }

        # ===== REGRAS MECE BASEADAS EM PHASE_SCORE (v4.7) =====

        # Regra 1: Score BOM + coverage OK ‚Üí DONE
        elif (
            phase_score >= threshold
            and key_questions_coverage >= getattr(self.valves, "COVERAGE_TARGET", 0.70)
        ):
            programmatic_decision = {
                "verdict": "done",
                "reasoning": f"Phase score {phase_score:.2f} ‚â• {threshold:.2f}, coverage {key_questions_coverage*100:.0f}% OK",
            }

        # Regra 2: Score BAIXO + flat_streak atingido ‚Üí NEW_PHASE com seed_family switch
        elif phase_score < threshold and flat_streak >= required_flat_loops:
            current_family = (
                phase_context.get("seed_family_hint", "entity-centric")
                if phase_context
                else "entity-centric"
            )
            seed_family_switch = self._switch_seed_family(current_family)
            programmatic_decision = {
                "verdict": "new_phase",
                "reasoning": f"Phase score {phase_score:.2f} < {threshold:.2f} ap√≥s {flat_streak} loops flat. Trocar fam√≠lia de explora√ß√£o",
                "seed_family": seed_family_switch,
            }

        # Regra 3: Score BAIXO mas ainda h√° tra√ß√£o ‚Üí REFINE
        elif phase_score < threshold and traction:
            programmatic_decision = {
                "verdict": "refine",
                "reasoning": f"Phase score {phase_score:.2f} < {threshold:.2f} mas h√° tra√ß√£o. Refinar",
            }

        # Fallback: REFINE (caso n√£o se encaixe em nenhuma regra acima)
        else:
            programmatic_decision = {
                "verdict": "refine",
                "reasoning": f"Score {phase_score:.2f}, coverage {key_questions_coverage*100:.0f}%. Continuar refinando",
            }

        # Aplicar rails de qualidade program√°ticos
        verdict = parsed.get("verdict", "done").strip()
        reasoning = parsed.get("reasoning", "").strip()
        next_query = parsed.get("next_query", "").strip()

        # Salvar decis√£o original do Judge para compara√ß√£o
        original_verdict = verdict
        original_reasoning = reasoning
        modifications = []

        # Se decis√£o program√°tica for diferente de "done", usar ela (tem prioridade)
        if (
            programmatic_decision.get("verdict")
            and programmatic_decision["verdict"] != "done"
        ):
            modifications.append(
                f"Programmatic override: {original_verdict} ‚Üí {programmatic_decision['verdict']}"
            )
            verdict = programmatic_decision["verdict"]
            reasoning = programmatic_decision["reasoning"]
            next_query = programmatic_decision.get("next_query", next_query)

        # üîí CONSISTENCY CHECK: Reasoning vs Verdict (3 camadas)

        # Camada 1: Lacunas expl√≠citas do Analyst
        if verdict == "done" and has_lacunas:
            logger.warning(
                f"[JUDGE] Inconsist√™ncia: verdict=done mas {len(analysis.get('lacunas', []))} lacunas no Analyst"
            )
            modifications.append(
                f"Consistency check: done ‚Üí refine ({len(analysis.get('lacunas', []))} lacunas encontradas)"
            )
            verdict = "refine"
            reasoning = f"[AUTO-CORRE√á√ÉO] Lacunas detectadas pelo Analyst. {reasoning}"

        # Camada 2: Key_questions coverage baixa (hip√≥teses n√£o respondidas)
        if verdict == "done" and key_questions_coverage < 0.70:
            logger.warning(
                f"[JUDGE] Inconsist√™ncia: verdict=done mas key_questions coverage={key_questions_coverage:.2f} < 0.70"
            )
            modifications.append(
                f"Consistency check: done ‚Üí refine (key_questions coverage {key_questions_coverage*100:.0f}% < 70%)"
            )
            verdict = "refine"
            reasoning = f"[AUTO-CORRE√á√ÉO] {key_questions_coverage*100:.0f}% das key_questions relevantes respondidas (< 70%). {reasoning}"

        # Camada 3: Blind Spots cr√≠ticos (descobertas que mudam contexto)
        # APENAS corrige DONE ‚Üí NEW_PHASE se Judge errou ao ignorar blind spots cr√≠ticos
        if verdict == "done" and blind_spots_signal:
            logger.warning(
                f"[JUDGE] Inconsist√™ncia: verdict=done mas {len(blind_spots)} blind_spots cr√≠ticos detectados"
            )
            modifications.append(
                f"Consistency check: done ‚Üí new_phase ({len(blind_spots)} blind_spots cr√≠ticos)"
            )
            verdict = "new_phase"
            reasoning = f"[AUTO-CORRE√á√ÉO] Blind spots cr√≠ticos invalidam hip√≥teses iniciais: {'; '.join(blind_spots[:2])}. {reasoning}"

        # Incluir nova fase se foi criada programaticamente
        new_phase = parsed.get("new_phase", {})
        if programmatic_decision.get("new_phase"):
            new_phase = programmatic_decision["new_phase"]

        # üìä FASE 1: Log JSON do Judge (observabilidade/auditoria)
        decision = {
            "decision": verdict,
            "reason": reasoning,
            "coverage": phase_metrics.get("coverage", 0),
            "domains": len(phase_metrics.get("domains", set())),
            "evidence": phase_metrics.get("evidence_coverage", 0),
            "staleness_ok": True,  # TODO: implementar staleness check se necess√°rio
            "loops": f"{len(telemetry_loops) if telemetry_loops else 0}/{getattr(self.valves, 'MAX_AGENT_LOOPS', 2)}",
        }
        logger.info(f"[JUDGE]{json.dumps(decision, ensure_ascii=False)}")

        return {
            "reasoning": reasoning,
            "verdict": verdict,
            "next_query": next_query,
            "refine": parsed.get("refine", {}),
            "new_phase": new_phase,
            "unexpected_findings": parsed.get("unexpected_findings", []),
            "proposed_phase": new_phase,  # Para compatibilidade
            # ‚úÖ v4.7: M√©tricas audit√°veis
            "phase_score": phase_score,
            "phase_metrics": phase_metrics,
            "seed_family": seed_family_switch,  # Presente apenas se NEW_PHASE por explora√ß√£o
            "modifications": modifications,  # Lista de modifica√ß√µes aplicadas
        }


# ==================== PROMPTS DICTIONARY ====================

PROMPTS = {
    # ===== PLANNER PROMPTS =====
    "planner_system": """Voc√™ √© o PLANNER. Crie um plano de pesquisa estruturado em AT√â {phases} fases (pode ser menos se suficiente).

üéØ FILOSOFIA DE PLANEJAMENTO:
- Crie APENAS as fases NECESS√ÅRIAS para cobrir o objetivo
- Melhor ter 2-3 fases bem focadas do que 4-5 gen√©ricas
- O Judge pode criar novas fases dinamicamente se descobrir lacunas
- M√°ximo permitido: {phases} fases (mas pode ser menos!)""",

    "planner_seed_rules": """
**SEED_QUERY (3-8 palavras, SEM operadores):**
- Estrutura: TEMA_CENTRAL + SETOR/CONTEXTO + ASPECTO + GEO
- Se 1-3 entidades: incluir TODOS os nomes + contexto setorial
- Se 4+ entidades: seed gen√©rica + contexto setorial + TODOS em must_terms
- @noticias: adicionar 3-6 palavras espec√≠ficas (eventos, tipos, a√ß√µes)
- **CR√çTICO**: Sempre incluir contexto setorial para evitar resultados irrelevantes

Exemplos:
‚úÖ "Vila Nova Partners executive search Brasil" (entidade + setor)
‚úÖ "Flow Executive search Brasil not√≠cias" (entidade + setor + contexto)
‚úÖ "RedeDr S√≥ Sa√∫de oncologia Brasil" (entidade + setor m√©dico)
‚úÖ "volume autos el√©tricos Brasil" (4+ entidades + setor)
‚úÖ "@noticias recalls ve√≠culos el√©tricos Brasil" (breaking news + setor)
‚ùå "Flow Brasil not√≠cia" (falta contexto setorial!)
‚ùå "volume fees Brasil" (falta tema!)
‚ùå "buscar dados verific√°veis" (gen√©rico demais)""",

    "planner_time_windows": """
**JANELAS TEMPORAIS:**

| Recency | Uso | Exemplo |
|---------|-----|---------|
| **90d** | Breaking news expl√≠cito | "√∫ltimos 90 dias", "breaking news" |
| **1y** | Tend√™ncias/estado atual (DEFAULT news) | "eventos recentes", "aquisi√ß√µes ano" |
| **3y** | Panorama/contexto hist√≥rico | "evolu√ß√£o setorial", "baseline" |

**Regra Pr√°tica:**
- News SEM prazo expl√≠cito ‚Üí 1y (captura 12 meses)
- News COM "90 dias" ‚Üí 90d (breaking only)
- Estudos de mercado ‚Üí 3y (contexto) + 1y (tend√™ncias) [OBRIGAT√ìRIO]""",

    "planner_entity_rules": """
**POL√çTICA ENTITY-CENTRIC (v4.8):**

| Quantidade | Mode | Seed_query | Must_terms (por fase) |
|------------|------|------------|----------------------|
| 1-3 | üéØ FOCADO | Incluir TODOS | **TODAS as fases** devem ter |
| 4-6 | üìä DISTRIBU√çDO | Gen√©rica | industry:‚â§3, profiles/news:TODAS |
| 7+ | üìä DISTRIBU√çDO | Gen√©rica | industry:‚â§3, profiles/news:TODAS |

**Cobertura obrigat√≥ria (1-3 entidades): ‚â•70% das fases devem incluir as entidades em must_terms**""",

    # ===== ANALYST PROMPTS =====
    "analyst_system": """‚ö†Ô∏è **FORMATO JSON OBRIGAT√ìRIO - REGRAS CR√çTICAS:**

Retorne APENAS um objeto JSON v√°lido. Proibi√ß√µes absolutas:
‚ùå Markdown fences (```json ou ```)
‚ùå Coment√°rios inline (// ou /* */)
‚ùå Texto explicativo antes/depois do JSON
‚ùå Aspas simples (use APENAS ")
‚ùå Quebras de linha dentro de strings

**ANTES DE RETORNAR, VALIDE MENTALMENTE:**
1. ‚úÖ Come√ßa com { e termina com } ?
2. ‚úÖ Todas as strings t√™m aspas DUPLAS " ?
3. ‚úÖ V√≠rgulas corretas (sem trailing commas) ?
4. ‚úÖ Nenhum coment√°rio inline ?
5. ‚úÖ Nenhum markdown fence ?

SE algum item falhar ‚Üí CORRIJA antes de retornar!

**SCHEMA EXATO (copie a estrutura channel):**
{
  "summary": "string resumo",
  "facts": [{"texto": "...", "confian√ßa": "alta|m√©dia|baixa", "evidencias": [{"url": "...", "trecho": "..."}]}],
  "lacunas": ["..."],
  "self_assessment": {"coverage_score": 0.7, "confidence": "m√©dia", "gaps_critical": true, "suggest_refine": false, "reasoning": "..."}
}

---

Voc√™ √© um ANALYST. Extraia 3-5 fatos importantes do contexto.

**PRIORIDADE #1**: Responda DIRETAMENTE ao objetivo da fase
- Priorize fatos sobre os Termos Priorit√°rios mencionados
- Ignore conte√∫do relacionado aos termos em "Evitar"  
- **CR√çTICO**: Ignore conte√∫do que n√£o tem contexto setorial relevante
- **CR√çTICO**: Se encontrar entidades com nomes similares mas em contextos diferentes (ex: "Flow" em outro setor), IGNORE se n√£o for relevante ao objetivo
- Busque evid√™ncias concretas (URLs + trechos)
- Valide se o contexto setorial das informa√ß√µes encontradas corresponde ao objetivo da pesquisa""",

    "analyst_calibration": """
üéØ CALIBRA√á√ÉO DE coverage_score (PRAGM√ÅTICA):

**0.0-0.3 (BAIXO - RESPOSTA INADEQUADA):**
‚Üí coverage_score = 0.2
‚Üí gaps_critical = True
‚Üí suggest_refine = True

**0.4-0.6 (M√âDIO - RESPOSTA PARCIAL mas √öTIL):**
‚Üí coverage_score = 0.6
‚Üí gaps_critical = False
‚Üí suggest_refine = False

**0.7-0.9 (ALTO - RESPOSTA S√ìLIDA):**
‚Üí coverage_score = 0.8
‚Üí gaps_critical = False
‚Üí suggest_refine = False""",

    # ===== JUDGE PROMPTS =====
    "judge_system": """Voc√™ √© o JUDGE. Sua fun√ß√£o: ANALISAR e DECIDIR se a pesquisa est√° COMPLETA ou precisa de mais informa√ß√µes.

üß† **ABORDAGEM LLM-FIRST:**
- Analise QUALITATIVAMENTE a qualidade dos fatos extra√≠dos
- Avalie se os fatos respondem adequadamente ao objetivo da pesquisa
- Identifique lacunas cr√≠ticas que impedem uma resposta satisfat√≥ria
- Considere a diversidade de fontes e dom√≠nios encontrados
- Proponha decis√£o baseada em JULGAMENTO, n√£o apenas m√©tricas num√©ricas""",

    "judge_philosophy": """
üéØ **FILOSOFIA DE DECIS√ÉO INTELIGENTE:**

**DONE = Resposta Satisfat√≥ria ao Objetivo**
- Os fatos extra√≠dos respondem adequadamente √† pergunta original?
- H√° evid√™ncias concretas (nomes, n√∫meros, datas, fontes espec√≠ficas)?
- A diversidade de fontes √© adequada para o escopo?
- As lacunas restantes s√£o secund√°rias ou cr√≠ticas?

**REFINE = Busca Mais Espec√≠fica Necess√°ria**
- Fatos gen√©ricos demais, falta especificidade?
- Lacunas cr√≠ticas impedem resposta ao objetivo?
- Fontes insuficientes ou repetitivas?
- Necessidade de foco em entidades espec√≠ficas mencionadas?

**NEW_PHASE = Abordagem Completamente Diferente**
- Mudan√ßa significativa de escopo, temporalidade ou fonte?
- √Çngulo de pesquisa diferente que pode revelar informa√ß√µes complementares?
- Necessidade de abordar aspectos n√£o cobertos pela pesquisa atual?

**PRINC√çPIO FUNDAMENTAL:** Priorize QUALIDADE sobre QUANTIDADE. √â melhor ter poucos fatos de alta qualidade que respondem ao objetivo do que muitos fatos gen√©ricos.""",
}


# ============================================================================
# 1. STATE DEFINITION (Completo - espelha Orchestrator)
# ============================================================================

class PlannerLLM:
    def __init__(self, valves):
        self.valves = valves
        # Usar modelo espec√≠fico se configurado, sen√£o usa modelo padr√£o
        model = valves.LLM_MODEL_PLANNER or valves.LLM_MODEL
        self.model_name = model
        self.llm = _get_llm(valves, model_name=model)
        # Base kwargs: ser√£o filtrados por get_safe_llm_params
        self.generation_kwargs = {"temperature": 0}

    def _build_prompt(
        self,
        user_prompt: str,
        phases: int,
        current_date: Optional[str] = None,
        detected_context: Optional[dict] = None,
    ) -> str:
        """Use unified Planner prompt."""
        return _build_planner_prompt(
            user_prompt,
            phases,
            current_date=current_date,
            detected_context=detected_context,
        )

    async def _generate_seed_core_with_llm(
        self, phase: dict, entities: List[str], geo_bias: List[str]
    ) -> Optional[str]:
        """Gera seed_core usando LLM (1 frase rica, sem operadores)."""
        try:
            objective = phase.get("objective", "")[:150]
            seed_family = phase.get("seed_family_hint", "entity-centric")

            # Templates por fam√≠lia (orientar LLM)
            family_templates = {
                "entity-centric": "entidade + tema + recorte geotemporal",
                "problem-centric": "problema/risco + drivers/causas + contexto",
                "outcome-centric": "efeito/resultado + indicadores + stakeholders",
                "regulatory": "norma/regulador + exig√™ncias + abrang√™ncia",
                "counterfactual": "tese/controv√©rsia + obje√ß√£o + evid√™ncia-chave",
            }

            template = family_templates.get(seed_family, "entidade + tema + contexto")
            entities_str = ", ".join(entities[:3]) if entities else "N/A"
            geo_str = ", ".join(geo_bias[:2]) if geo_bias else "Brasil"

            prompt = f"""Gere seed_core (1 frase, 12-200 chars, sem operadores).
OBJETIVO: {objective}
ENTIDADES: {entities_str}
GEO: {geo_str}
FAM√çLIA: {seed_family} ‚Üí {template}
REGRAS:
- SEM operadores (site:, filetype:, after:, before:)
- Linguagem natural, clara
- Incluir 1-2 entidades can√¥nicas
- Incluir geo quando relevante
SA√çDA (JSON puro):
{{"seed_core": "sua frase aqui"}}"""

            # Chamar LLM com timeout curto (20s)
            safe_kwargs = get_safe_llm_params(self.model_name, self.generation_kwargs)
            safe_kwargs["request_timeout"] = 20

            result = await asyncio.wait_for(
                self.llm.run(prompt, generation_kwargs=safe_kwargs),
                timeout=25,  # 25s total (20s HTTP + 5s margem)
            )

            # Handle both response formats: {"content": ...} or {"replies": [...]}
            content = None
            if result and isinstance(result, dict):
                if "content" in result:
                    content = result["content"]
                elif "replies" in result and result["replies"]:
                    content = result["replies"][0]
            
            if not content:
                logger.warning("[Planner] LLM seed_core: resposta vazia ou formato inv√°lido")
                return None

            content = content.strip()

            # Parse JSON
            parsed = parse_json_resilient(content)
            if not parsed or not isinstance(parsed, dict):
                logger.warning(
                    f"[Planner] LLM seed_core: JSON inv√°lido - {content[:100]}"
                )
                return None

            seed_core = parsed.get("seed_core", "").strip()

            # Validar
            if not seed_core or len(seed_core) < 12 or len(seed_core) > 200:
                logger.warning(
                    f"[Planner] LLM seed_core: tamanho inv√°lido ({len(seed_core)} chars)"
                )
                return None

            # Verificar operadores proibidos
            forbidden_ops = ["site:", "filetype:", "after:", "before:"]
            if any(op in seed_core for op in forbidden_ops):
                logger.warning(
                    f"[Planner] LLM seed_core cont√©m operadores proibidos: {seed_core}"
                )
                return None

            logger.info(
                f"[Planner] LLM seed_core gerado com sucesso: '{seed_core[:80]}...'"
            )
            return seed_core

        except asyncio.TimeoutError:
            logger.warning("[Planner] LLM seed_core: timeout ap√≥s 25s")
            return None
        except Exception as e:
            logger.warning(f"[Planner] LLM seed_core falhou: {e}")
            return None

    def _validate_contract(self, obj: dict, phases: int, user_prompt: str) -> dict:
        """Valida e normaliza o contrato do novo formato JSON"""
        phases_list = obj.get("phases", [])
        intent_profile = obj.get("intent_profile", "company_profile")

        if not phases_list:
            raise ValueError("Nenhuma fase encontrada")

        # Validar n√∫mero de fases (pode ser MENOS que o m√°ximo, mas n√£o MAIS)
        if len(phases_list) > phases:
            raise ValueError(
                f"Excesso de fases: {len(phases_list)} > {phases} (m√°ximo permitido)"
            )

        # Defaults do perfil
        profile_defaults = getattr(self.valves, "PROFILE_DEFAULTS", {})
        profile = (
            intent_profile if intent_profile in profile_defaults else "company_profile"
        )
        defaults = profile_defaults.get(profile, {})

        # Validar cada fase
        validated_phases = []
        for i, phase in enumerate(phases_list, 1):
            # Validar campos obrigat√≥rios
            required_fields = [
                "name",
                "objective",
                "seed_query",
                "seed_core",
                "must_terms",
                "avoid_terms",
                "time_hint",
                "source_bias",
                "evidence_goal",
                "lang_bias",
                "geo_bias",
            ]

            for field in required_fields:
                if field not in phase:
                    raise ValueError(f"Fase {i} falta campo obrigat√≥rio: {field}")

            # Validar seed_query (3-6 palavras, sem operadores)
            seed_query = phase["seed_query"].strip()
            
            # Validar seed_query (3-8 palavras, SEM contar @noticias)
            clean_words = [
                w for w in seed_query.split() if w.lower() != "@noticias" and w.strip()
            ]
            if len(clean_words) < 3 or len(clean_words) > 8:
                raise ValueError(
                    f"Fase {i}: seed_query (sem @noticias) deve ter 3-8 palavras, tem {len(clean_words)} palavras: {clean_words}"
                )

            # Validar proibi√ß√£o de operadores
            forbidden_ops = [
                "site:",
                "filetype:",
                "after:",
                "before:",
                "AND",
                "OR",
                '"',
                "'",
            ]
            for op in forbidden_ops:
                if op in seed_query:
                    raise ValueError(
                        f"Fase {i}: seed_query n√£o pode conter operador '{op}'"
                    )

            # Validar avoid_terms n√£o sobrep√µe must_terms
            must_terms = phase["must_terms"]
            avoid_terms = phase["avoid_terms"]
            overlap = set(must_terms) & set(avoid_terms)
            if overlap:
                raise ValueError(
                    f"Fase {i}: must_terms e avoid_terms sobrep√µem: {overlap}"
                )

            # Validar time_hint
            time_hint = phase.get("time_hint") or defaults.get(
                "time_hint", {"recency": "1y", "strict": False}
            )
            
            if "recency" not in time_hint or time_hint["recency"] not in [
                "90d",
                "1y",
                "3y",
            ]:
                raise ValueError(f"Fase {i}: time_hint.recency deve ser 90d, 1y ou 3y")

            # Validar seed_core
            seed_core = phase.get("seed_core", "").strip()
            
            if not seed_core or len(seed_core) < 12:
                logger.warning(f"[PLANNER] Phase '{phase.get('name', 'unnamed')}' missing valid seed_core (len={len(seed_core) if seed_core else 0}), using fallback")
                # Keep minimal fallback for edge cases only
                canonical = obj.get("entities", {}).get("canonical", [])
                objective_words = phase["objective"].split()[:10]
                seed_core = f"{' '.join(canonical[:2])} {seed_query} {objective_words}".strip()[:200]
                seed_core_source = "orchestrator_fallback"
            else:
                seed_core_source = "planner_llm"  # From LLM

            # Store seed_core and source for telemetry
            phase["seed_core"] = seed_core
            phase["seed_core_source"] = seed_core_source

            # Validar source_bias
            valid_sources = ["oficial", "primaria", "secundaria", "terciaria"]
            source_bias = phase.get("source_bias") or defaults.get(
                "source_bias", ["oficial", "primaria", "secundaria"]
            )
            if not all(s in valid_sources for s in source_bias):
                raise ValueError(
                    f"Fase {i}: source_bias deve conter apenas: {valid_sources}"
                )

            # Validar evidence_goal
            evidence_goal = phase.get("evidence_goal") or {
                **{"official_or_two_independent": True},
                **defaults.get("evidence_goal", {"min_domains": 3}),
            }
            if (
                "official_or_two_independent" not in evidence_goal
                or "min_domains" not in evidence_goal
            ):
                raise ValueError(
                    f"Fase {i}: evidence_goal deve conter official_or_two_independent e min_domains"
                )

            validated_phases.append(
                {
                    "id": i,
                    "objetivo": phase["objective"],
                    "query_sugerida": seed_query,  # Para compatibilidade
                    "name": phase["name"],
                    "seed_query": seed_query,
                    "seed_core": phase.get("seed_core", ""),
                    "seed_family_hint": phase.get("seed_family_hint", "entity-centric"),
                    "must_terms": must_terms,
                    "avoid_terms": avoid_terms,
                    "time_hint": time_hint,
                    "source_bias": source_bias,
                    "evidence_goal": evidence_goal,
                    "lang_bias": phase.get("lang_bias")
                    or defaults.get("lang_bias", ["pt-BR", "en"]),
                    "geo_bias": phase.get("geo_bias")
                    or defaults.get("geo_bias", ["BR", "global"]),
                    "suggested_domains": phase.get("suggested_domains", []),
                    "suggested_filetypes": phase.get("suggested_filetypes", []),
                    "accept_if_any_of": [f"Info sobre {phase['objective']}"],  # Para compatibilidade
                }
            )

        if len(validated_phases) < 2:
            raise ValueError(f"Apenas {len(validated_phases)} fases")

        # Validar entity coverage
        entities_canonical = obj.get("entities", {}).get("canonical", [])
        contract_dict = {
            "versao": "3.0",
            "intent": obj.get("intent", f"Pesquisar: {user_prompt}"),
            "intent_profile": intent_profile,
            "entities": obj.get("entities", {"canonical": [], "aliases": []}),
            "fases": validated_phases,
            "quality_rails": obj.get(
                "quality_rails",
                {
                    "min_unique_domains": max(self.valves.MIN_UNIQUE_DOMAINS, phases),
                    "need_official_or_two_independent": self.valves.REQUIRE_OFFICIAL_OR_TWO_INDEPENDENT,
                },
            ),
            "budget": obj.get("budget", {"max_rounds": 2}),
        }

        return contract_dict

    async def run(
        self,
        user_prompt: str,
        phases: int = 2,
        current_date: Optional[str] = None,
        previous_plan: Optional[str] = None,
        detected_context: Optional[dict] = None,
    ) -> Dict[str, Any]:
        if not self.llm:
            raise ValueError("LLM n√£o configurado")

        phases = max(2, min(10, int(phases or 2)))

        # Se houver plano anterior, contextualize para permitir refinamento
        if previous_plan:
            contextual_prompt = f"""PLANO ANTERIOR:
{previous_plan}

PEDIDO DE REFINAMENTO/AJUSTE:
{user_prompt}

INSTRU√á√ïES:
- Se o pedido for um refinamento/ajuste do plano anterior (ex: "pesquise not√≠cias de 2025", "adicione mais fases"), ATUALIZE o plano anterior
- Mantenha as fases existentes e ajuste apenas o que foi solicitado
- Se for um pedido COMPLETAMENTE NOVO (sem rela√ß√£o com o plano anterior), crie um novo plano
- Responda com o plano completo (anterior ajustado OU novo)"""
            prompt = self._build_prompt(
                contextual_prompt,
                phases,
                current_date=current_date,
                detected_context=detected_context,
            )
        else:
            prompt = self._build_prompt(
                user_prompt,
                phases,
                current_date=current_date,
                detected_context=detected_context,
            )

        for attempt in range(1, 3):
            try:
                # Base params (ser√£o filtrados para GPT-5/O1)
                base_params = dict(self.generation_kwargs)

                # JSON mode to reduce latency/noise
                if getattr(self.valves, "FORCE_JSON_MODE", True):
                    base_params["response_format"] = {"type": "json_object"}

                # Planner fast-read timeout (fail fast)
                prt = int(
                    getattr(
                        self.valves,
                        "PLANNER_REQUEST_TIMEOUT",
                        getattr(self.valves, "LLM_TIMEOUT_PLANNER", 180),
                    )
                    or 180
                )
                cap = int(getattr(self.valves, "HTTPX_READ_TIMEOUT", 180) or 180)
                base_params["request_timeout"] = max(20, min(prt, cap))

                # Filtrar par√¢metros incompat√≠veis com GPT-5/O1
                gen_kwargs = get_safe_llm_params(self.model_name, base_params)

                # Use retry function if enabled, otherwise single attempt
                if getattr(self.valves, "ENABLE_LLM_RETRY", True):
                    max_retries = int(getattr(self.valves, "LLM_MAX_RETRIES", 3) or 3)
                    out = await _safe_llm_run_with_retry(
                        self.llm,
                        prompt,
                        gen_kwargs,
                        timeout=int(
                            getattr(
                                self.valves,
                                "LLM_TIMEOUT_PLANNER",
                                self.valves.LLM_TIMEOUT_DEFAULT,
                            )
                            or self.valves.LLM_TIMEOUT_DEFAULT
                        ),
                        max_retries=max_retries,
                    )
                else:
                    out = await _safe_llm_run_with_retry(
                        self.llm,
                        prompt,
                        gen_kwargs,
                        timeout=int(
                            getattr(
                                self.valves,
                                "LLM_TIMEOUT_PLANNER",
                                self.valves.LLM_TIMEOUT_DEFAULT,
                            )
                            or self.valves.LLM_TIMEOUT_DEFAULT
                        ),
                        max_retries=1,
                    )
                if not out or not out.get("replies"):
                    raise ValueError("LLM vazio")

                obj = _extract_json_from_text(out["replies"][0])
                if not obj:
                    raise ValueError("JSON inv√°lido")

                contract = self._validate_contract(obj, phases, user_prompt)
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[Planner] {len(contract['fases'])} fases")

                return {"contract": contract, "contract_hash": _hash_contract(contract)}

            except (json.JSONDecodeError, ValueError, KeyError) as e:
                # Erros de parse/valida√ß√£o - tentar novamente
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[Planner] Tentativa {attempt} - Parse error: {e}")
                if attempt < 2:
                    # Retry with compact prompt (avoid appending error text)
                    prompt = _build_planner_prompt(
                        user_prompt,
                        phases,
                        current_date=current_date,
                        detected_context=detected_context,
                    )
                else:
                    raise ContractGenerationError(
                        f"Failed to generate valid contract after 3 attempts: {e}"
                    ) from e
            except ContractValidationError as e:
                # Erro de valida√ß√£o espec√≠fico - propagar
                raise
            except Exception as e:
                # Erro inesperado - tentar uma vez, depois propagar
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[Planner] Tentativa {attempt} - Unexpected error: {e}")
                if attempt < 2:
                    # Retry with compact prompt as above
                    prompt = _build_planner_prompt(
                        user_prompt,
                        phases,
                        current_date=current_date,
                        detected_context=detected_context,
                    )
                else:
                    raise ContractGenerationError(
                        f"Unexpected error generating contract: {e}"
                    ) from e
        
        # Fallback return (should never reach here)
        return {"contract": {}, "contract_hash": ""}


# ============================================================================
# 1. STATE DEFINITION (Completo - espelha Orchestrator)
# ============================================================================

class ResearchState(TypedDict, total=False):
    """Estado compartilhado entre n√≥s LangGraph - TODOS os campos do Orchestrator"""
    
    # ===== CONTEXTO & CONTROLE =====
    correlation_id: str
    query: str                          # Query atual (pode mudar em refine)
    original_query: str                 # Query inicial da fase
    phase_index: int                    # √çndice da fase atual (1-based)
    phase_context: Dict                 # Contexto completo da fase
    loop_count: int                     # Contador de loops (0-based)
    max_loops: int                      # Limite de loops
    
    # ===== CONTRACT & CONFIGURATION =====
    contract: Dict                      # Contract completo
    all_phase_queries: List[str]        # TODAS as queries (para Context Reducer)
    intent_profile: str                 # Perfil detectado
    
    # ===== DISCOVERY RESULTS =====
    discovered_urls: List[str]          # URLs descobertas
    new_urls: List[str]                 # URLs novas (n√£o em cache)
    cached_urls: List[str]              # URLs j√° scraped
    
    # ===== SCRAPING & CACHE =====
    scraped_cache: Dict[str, str]       # {url: content}
    raw_content: str                    # Conte√∫do bruto scraped
    filtered_content: str               # Ap√≥s Context Reducer
    accumulated_context: str            # Acumulado de TODAS itera√ß√µes
    
    # ===== ANALYSIS RESULTS =====
    analysis: Dict                      # Output do Analyst
    summary: str
    facts: List[Dict]
    lacunas: List
    self_assessment: Dict
    
    # ===== EVIDENCE METRICS =====
    evidence_metrics: Dict
    unique_domains: int
    facts_with_evidence: int
    facts_with_multiple_sources: int
    high_confidence_facts: int
    contradictions: int
    evidence_coverage: float
    
    # ===== NOVELTY TRACKING =====
    used_claim_hashes: List[str]        # Hashes de fatos j√° vistos
    used_domains: List[str]             # Dom√≠nios j√° consultados
    new_facts_ratio: float
    new_domains_ratio: float
    
    # ===== JUDGE RESULTS =====
    judgement: Dict                     # Output do Judge
    verdict: Literal["done", "refine", "new_phase"]
    reasoning: str
    next_query: Optional[str]
    new_phase: Optional[Dict]
    phase_score: float
    phase_metrics: Dict
    modifications: List[str]
    
    # ===== COVERAGE & QUALITY =====
    coverage_score: float
    entities_covered: int
    analyst_confidence: str
    gaps_critical: bool
    suggest_refine: bool
    suggest_pivot: bool
    
    # ===== TELEMETRY =====
    telemetry_loops: List[Dict]
    seed_core_source: Optional[str]
    analyst_proposals: Dict
    # ===== EVENT EMITTER (UX) =====
    __event_emitter__: Optional[Callable]
    
    # ===== FLAGS & CONTROL =====
    diminishing_returns: bool
    failed_query: bool
    previous_queries: List[str]
    failed_queries: List[str]
    
    # ===== PHASE RESULTS (Global) =====
    phase_results: List[Dict]
    
    # ===== SYNTHESIS (Final) =====
    final_synthesis: Optional[str]
# ============================================================================
# 2. HELPER CLASSES (COMPLETAS - j√° migradas acima)
# ============================================================================
# Todas as classes helper j√° foram migradas completamente:
# - Deduplicator (linhas 909+)
# - AsyncOpenAIClient (linhas 1526+)  
# - AnalystLLM (linhas 1675+)
# - JudgeLLM (linhas 2106+)
# - PlannerLLM (linhas 3129+)
# - PROMPTS dictionary (linhas 2800+)
# ============================================================================
# 3. GRAPH NODES (Implementa√ß√µes completas)
# ============================================================================
# ============================================================================
# 3. NODE WRAPPERS (Chamam c√≥digo existente)
# ============================================================================
class GraphNodes:
    """Wrappers FINOS - delegam para c√≥digo existente (ex-Orchestrator)"""
    
    def __init__(self, valves, discovery_tool, scraper_tool, context_reducer_tool=None):
        self.valves = valves
        self.discovery_tool = discovery_tool
        self.scraper_tool = scraper_tool
        self.context_reducer_tool = context_reducer_tool
        
        # Instanciar LLM components
        self.analyst = None
        self.judge = None
        self.deduplicator = None
    
    async def discovery_node(self, state: ResearchState) -> Dict:
        """Discovery node - complete implementation from Orchestrator._run_discovery"""
        correlation_id = state.get('correlation_id', 'unknown')
        em = state.get('__event_emitter__')
        await _safe_emit(em, f"[DISCOVERY][{correlation_id}] start")
        query = state.get('query', '')
        phase_context = state.get('phase_context', {})
        
        # ===== TELEMETRY =====
        tel = StepTelemetry(
            step="discovery",
            correlation_id=correlation_id,
            start_ms=time.time() * 1000,
            inputs_brief=f"query={query[:50]}...",
            counters={"urls_found": 0, "new_urls": 0},
        )
        
        try:
            # Call discovery tool
            discovery_params = {
                "query": query,
                "phase_context": phase_context,
                "correlation_id": correlation_id,
            }
            
            if asyncio.iscoroutinefunction(self.discovery_tool):
                discovery_result = await self.discovery_tool(**discovery_params)
            else:
                # Use asyncio.to_thread for sync functions
                discovery_result = await asyncio.to_thread(
                    lambda: self.discovery_tool(**discovery_params)
                )
            
            if isinstance(discovery_result, str):
                discovery_result = json.loads(discovery_result)
            
            discovered_urls = discovery_result.get("urls", [])
            new_urls = [url for url in discovered_urls if url not in state.get('scraped_cache', {})]
            
            # ===== TELEMETRY FINAL =====
            if tel is not None:
                tel.end_ms = time.time() * 1000
                if tel.counters is not None:
                    tel.counters["urls_found"] = len(discovered_urls)
                    tel.counters["new_urls"] = len(new_urls)
                tel.outputs_brief = f"found={len(discovered_urls)}, new={len(new_urls)}"
                
                # Validate telemetry completeness
                if tel.end_ms <= tel.start_ms:
                    logger.warning(f"[{tel.step.upper()}] Invalid telemetry timing: end_ms={tel.end_ms} <= start_ms={tel.start_ms}")
                if tel.counters is not None and tel.counters.get("urls_found", 0) < 0:
                    logger.warning(f"[{tel.step.upper()}] Invalid counter: urls_found={tel.counters['urls_found']}")
                
                logger.info(f"[{tel.step.upper()}] {json.dumps(tel.to_dict())}")
            
            # Validate state transition
            if not isinstance(discovered_urls, list):
                logger.warning(f"[DISCOVERY] State validation: discovered_urls is not a list: {type(discovered_urls)}")
                discovered_urls = []
            if not isinstance(new_urls, list):
                logger.warning(f"[DISCOVERY] State validation: new_urls is not a list: {type(new_urls)}")
                new_urls = []
            
            out = {
                "discovered_urls": discovered_urls,
                "new_urls": new_urls,
                "cached_urls": list(state.get('scraped_cache', {}).keys()),
            }
            await _safe_emit(em, f"[DISCOVERY][{correlation_id}] end urls={len(discovered_urls)} new={len(new_urls)}")
            return out
            
        except Exception as e:
            logger.error(f"Discovery failed: {e}")
            
            # Recovery: Try to return partial results if available
            partial_urls = []
            try:
                # If we have any partial results from the tool call, use them
                if 'discovery_result' in locals() and discovery_result and isinstance(discovery_result, dict):
                    partial_urls = discovery_result.get("urls", [])
            except:
                pass
            
            # Log recovery attempt
            if partial_urls:
                logger.info(f"[DISCOVERY] Recovery: Using {len(partial_urls)} partial URLs")
            else:
                logger.warning(f"[DISCOVERY] Recovery: No partial results available")
            
            out = {
                "discovered_urls": partial_urls,
                "new_urls": [url for url in partial_urls if url not in state.get('scraped_cache', {})],
                "cached_urls": list(state.get('scraped_cache', {}).keys()),
                "error": str(e),
                "failed_query": True,
            }
            await _safe_emit(em, f"[DISCOVERY][{correlation_id}] recovery urls={len(partial_urls)}")
            return out
    
    async def scrape_node(self, state: ResearchState) -> Dict:
        """Scrape node - complete implementation from Orchestrator._run_scraping"""
        correlation_id = state.get('correlation_id', 'unknown')
        em = state.get('__event_emitter__')
        await _safe_emit(em, f"[SCRAPE][{correlation_id}] start")
        new_urls = state.get('new_urls', [])
        scraped_cache = state.get('scraped_cache', {})
        
        # ===== TELEMETRY =====
        tel = StepTelemetry(
            step="scraping",
            correlation_id=correlation_id,
            start_ms=time.time() * 1000,
            inputs_brief=f"urls={len(new_urls)}",
            counters={"scraped": 0, "failed": 0},
        )
        
        try:
            # Call scraper tool for new URLs
            if new_urls:
                scrape_params = {
                    "urls": new_urls,
                    "correlation_id": correlation_id,
                }
                
                try:
                    if asyncio.iscoroutinefunction(self.scraper_tool):
                        scrape_result = await self.scraper_tool(**scrape_params)
                    else:
                        scrape_result = await asyncio.to_thread(
                            lambda: self.scraper_tool(**scrape_params)
                        )
                    
                    if isinstance(scrape_result, str):
                        scrape_result = json.loads(scrape_result)
                    
                    # Update scraped cache
                    scraped_content = scrape_result.get("content", {})
                    scraped_cache.update(scraped_content)
                    
                except TypeError as e:
                    # Tool doesn't support correlation_id - try with basic params
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(f"[S_WRAPPER] Advanced params failed: {e}")
                        print(f"[S_WRAPPER] Falling back to basic urls only")
                    
                    try:
                        basic_params = {"urls": new_urls}
                        if asyncio.iscoroutinefunction(self.scraper_tool):
                            scrape_result = await self.scraper_tool(**basic_params)
                        else:
                            scrape_result = await asyncio.to_thread(
                                lambda: self.scraper_tool(**basic_params)
                            )
                        
                        if isinstance(scrape_result, str):
                            scrape_result = json.loads(scrape_result)
                        
                        scraped_content = scrape_result.get("content", {})
                        scraped_cache.update(scraped_content)
                        
                        if getattr(self.valves, "VERBOSE_DEBUG", False):
                            print(f"[S_WRAPPER] Fallback successful: {len(scraped_content)} URLs scraped")
                            
                    except Exception as fallback_e:
                        logger.error(f"[S_WRAPPER] Even basic scraping failed: {fallback_e}")
                        # Continue without scraping this batch
                except Exception as e:
                    logger.error(f"[S_WRAPPER] Unexpected scraping error: {e}")
                    # Continue without scraping this batch
            
            # ===== TELEMETRY FINAL =====
            if tel is not None:
                tel.end_ms = time.time() * 1000
                if tel.counters is not None:
                    tel.counters["scraped"] = len(scraped_cache)
                tel.outputs_brief = f"total_scraped={len(scraped_cache)}"
                
                # Validate telemetry completeness
                if tel.end_ms <= tel.start_ms:
                    logger.warning(f"[{tel.step.upper()}] Invalid telemetry timing: end_ms={tel.end_ms} <= start_ms={tel.start_ms}")
                if tel.counters is not None and tel.counters.get("scraped", 0) < 0:
                    logger.warning(f"[{tel.step.upper()}] Invalid counter: scraped={tel.counters['scraped']}")
                
                logger.info(f"[{tel.step.upper()}] {json.dumps(tel.to_dict())}")
            
            # Validate state transition
            if not isinstance(scraped_cache, dict):
                logger.warning(f"[SCRAPING] State validation: scraped_cache is not a dict: {type(scraped_cache)}")
                scraped_cache = {}
            
            out = {
                "scraped_cache": scraped_cache,
                "raw_content": "\n\n".join(scraped_cache.values()),
            }
            await _safe_emit(em, f"[SCRAPE][{correlation_id}] end cache={len(scraped_cache)} bytes={len(out['raw_content'])}")
            return out
            
        except Exception as e:
            logger.error(f"Scraping failed: {e}")
            
            # Recovery: Return existing cache even if new scraping failed
            logger.info(f"[SCRAPING] Recovery: Returning existing cache with {len(scraped_cache)} URLs")
            
            out = {
                "scraped_cache": scraped_cache,
                "raw_content": "\n\n".join(scraped_cache.values()),
                "error": str(e),
                "failed_query": True,
            }
            await _safe_emit(em, f"[SCRAPE][{correlation_id}] recovery cache={len(scraped_cache)}")
            return out
    
    async def reduce_node(self, state: ResearchState) -> Dict:
        """Reduce node - complete implementation from Orchestrator._run_context_reduction"""
        correlation_id = state.get('correlation_id', 'unknown')
        em = state.get('__event_emitter__')
        await _safe_emit(em, f"[REDUCE][{correlation_id}] start")
        raw_content = state.get('raw_content', '')
        accumulated_context = state.get('accumulated_context', '')
        all_phase_queries = state.get('all_phase_queries', [])
        job_id = state.get('job_id', '')
        
        # Early return if no content or tool not available
        if not raw_content or not getattr(self.valves, 'ENABLE_CONTEXT_REDUCER', False) or not self.context_reducer_tool:
            accumulated_context += f"\n{raw_content}\n"
            out = {
                "filtered_content": raw_content,
                "accumulated_context": accumulated_context,
            }
            await _safe_emit(em, f"[REDUCE][{correlation_id}] bypass chars={len(raw_content)} acc={len(accumulated_context)}")
            return out
        
        # ===== TELEMETRY =====
        tel = StepTelemetry(
            step="context_reducer",
            correlation_id=correlation_id,
            start_ms=time.time() * 1000,
            inputs_brief=f"content={len(raw_content)} chars",
            counters={"input_chars": len(raw_content), "output_chars": 0},
        )
        
        try:
            context_params = {
                "corpo": {"ultrasearcher_result": {"scraped_content": raw_content}},
                "mode": "coarse",
                "queries": all_phase_queries,
                "tipo": "fase",
            }
            
            if job_id:
                context_params["job_id"] = job_id
            
            if self.context_reducer_tool is not None:
                try:
                    tool = self.context_reducer_tool  # Store reference to avoid linter issues
                    if asyncio.iscoroutinefunction(tool):
                        context_result = await tool(**context_params)
                    else:
                        context_result = await asyncio.to_thread(
                            lambda: tool(**context_params)
                        )
                except Exception as e:
                    logger.warning(f"Context Reducer failed: {e}")
                    context_result = {"final_markdown": raw_content}
            else:
                context_result = {"final_markdown": raw_content}
            
            if isinstance(context_result, str):
                context_result = json.loads(context_result)
            
            filtered_content = context_result.get("final_markdown", raw_content)
            reduction = (
                (1 - len(filtered_content) / len(raw_content)) * 100
                if len(raw_content) > 0
                else 0
            )
            
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[Context Reducer] coarse: {len(raw_content)} ‚Üí {len(filtered_content)} chars (-{reduction:.1f}%)")
            
            # ===== TELEMETRY FINAL =====
            if tel is not None:
                tel.end_ms = time.time() * 1000
                if tel.counters is not None:
                    tel.counters["output_chars"] = len(filtered_content)
                tel.outputs_brief = f"reduced={len(filtered_content)} chars (-{reduction:.1f}%)"
                logger.info(f"[{tel.step.upper()}] {json.dumps(tel.to_dict())}")
            
            # Accumulate context
            accumulated_context += f"\n{filtered_content}\n"
            
            out = {
                "filtered_content": filtered_content,
                "accumulated_context": accumulated_context,
            }
            await _safe_emit(em, f"[REDUCE][{correlation_id}] end chars={len(filtered_content)} acc={len(accumulated_context)}")
            return out
            
        except (KeyError, ValueError, TypeError) as e:
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[Context Reducer] Parse error: {e}, usando raw")
            accumulated_context += f"\n{raw_content}\n"
            out = {
                "filtered_content": raw_content,
                "accumulated_context": accumulated_context,
            }
            await _safe_emit(em, f"[REDUCE][{correlation_id}] parse_error acc={len(accumulated_context)}")
            return out
        except Exception as e:
            logger.error(f"Context reducer failed unexpectedly: {e}")
            accumulated_context += f"\n{raw_content}\n"
            out = {
                "filtered_content": raw_content,
                "accumulated_context": accumulated_context,
            }
            await _safe_emit(em, f"[REDUCE][{correlation_id}] fail acc={len(accumulated_context)}")
            return out
    
    async def analyze_node(self, state: ResearchState) -> Dict:
        """Analyze node - complete implementation from Orchestrator._run_analysis"""
        correlation_id = state.get('correlation_id', 'unknown')
        em = state.get('__event_emitter__')
        await _safe_emit(em, f"[ANALYZE][{correlation_id}] start")
        filtered_content = state.get('filtered_content', '')
        accumulated_context = state.get('accumulated_context', '')
        phase_context = state.get('phase_context', {})
        
        # ===== TELEMETRY =====
        tel = StepTelemetry(
            step="analyst",
            correlation_id=correlation_id,
            start_ms=time.time() * 1000,
            inputs_brief=f"context={len(accumulated_context)} chars",
            counters={"facts": 0, "lacunas": 0},
        )
        
        # Accumulate filtered content
        if filtered_content:
            accumulated_context += f"\n\n{filtered_content}"
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(f"[Accumulator] Total acumulado: {len(accumulated_context)} chars")
        
        # Deduplica√ß√£o opcional para Analyst
        analyst_context = accumulated_context
        
        if getattr(self.valves, 'ENABLE_ANALYST_DEDUPLICATION', False) and accumulated_context:
            # Divis√£o mais agressiva para garantir ativa√ß√£o da deduplica√ß√£o
            paragraphs = [
                p.strip() for p in accumulated_context.split("\n\n") if p.strip()
            ]
            
            # Se ainda n√£o tem par√°grafos suficientes, dividir por senten√ßas
            if len(paragraphs) < getattr(self.valves, 'MAX_ANALYST_PARAGRAPHS', 50):
                # Dividir por senten√ßas (pontos seguidos de espa√ßo)
                sentences = [
                    s.strip() for s in accumulated_context.replace('\n', ' ').split('. ') if s.strip()
                ]
                # Agrupar senten√ßas em par√°grafos de ~3 senten√ßas
                paragraphs = []
                for i in range(0, len(sentences), 3):
                    paragraph = '. '.join(sentences[i:i+3])
                    if paragraph and not paragraph.endswith('.'):
                        paragraph += '.'
                    paragraphs.append(paragraph)
            
            max_paragraphs = getattr(self.valves, 'MAX_ANALYST_PARAGRAPHS', 50)
            if len(paragraphs) > max_paragraphs:
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[DEDUP ANALYST] ‚úÖ ATIVADO: {len(paragraphs)} par√°grafos > {max_paragraphs} ‚Üí deduplicando para Analyst...")
                
                # Calcular preserva√ß√£o de contexto recente
                if filtered_content:
                    new_paragraphs = [
                        p.strip() for p in filtered_content.split("\n\n") if p.strip()
                    ]
                    new_count = len(new_paragraphs)
                    
                    # Cap dynamic preservation to avoid disabling deduplication
                    preserve_recent_pct = min(
                        0.3,
                        new_count / len(paragraphs)
                    )
                    
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(f"[DEDUP ANALYST] üîí Preservation: {preserve_recent_pct:.1%} ({new_count} new / {len(paragraphs)} total)")
                else:
                    # No new content - use lower preservation for old accumulated data
                    preserve_recent_pct = 0.2
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(f"[DEDUP ANALYST] ‚ö†Ô∏è No new content - preserving {preserve_recent_pct:.1%} of accumulated")
                
                # Usar estrat√©gia espec√≠fica do Analyst
                algorithm = getattr(self.valves, "ANALYST_DEDUP_ALGORITHM", "semantic")
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[DEDUP ANALYST] üß† Algoritmo: {algorithm.upper()}")
                    print(f"[DEDUP ANALYST] üìä Input: {len(paragraphs)} par√°grafos ‚Üí Target: {max_paragraphs}")
                
                # Deduplicar com context-aware
                dedupe_result = self.deduplicator.dedupe(
                    chunks=paragraphs,
                    max_chunks=max_paragraphs,
                    algorithm=algorithm,
                    threshold=getattr(self.valves, "DEDUP_SIMILARITY_THRESHOLD", 0.85),
                    preserve_order=True,
                    preserve_recent_pct=preserve_recent_pct,
                    shuffle_older=True,
                    reference_first=True,
                    # Context-aware parameters
                    must_terms=phase_context.get("must_terms", []),
                    key_questions=phase_context.get("key_questions", []),
                    enable_context_aware=getattr(self.valves, "ENABLE_CONTEXT_AWARE_DEDUP", False),
                )
                
                analyst_context = "\n\n".join(dedupe_result["chunks"])
                
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[DEDUP ANALYST] {dedupe_result['original_count']} ‚Üí {dedupe_result['deduped_count']} par√°grafos ({dedupe_result['reduction_pct']:.1f}% redu√ß√£o)")
                    
                    # Additional telemetry
                    preserved_count = int(len(paragraphs) * preserve_recent_pct)
                    print(f"[DEDUP ANALYST] üìå Recent preservation: {preserved_count} paragraphs ({preserve_recent_pct:.1%}) protected from deduplication")
                    print(f"[DEDUP ANALYST] üéØ Valve setting: ANALYST_PRESERVE_RECENT_PCT = {getattr(self.valves, 'ANALYST_PRESERVE_RECENT_PCT', 1.0)}")
        
        # Chamar Analyst
        analysis = await self.analyst.run(
            query=state.get('query', ''),
            accumulated_context=analyst_context,
            phase_context=phase_context,
        )
        
        # Validar resultado
        if not isinstance(analysis, dict):
            logger.error(f"[CRITICAL] Analyst returned non-dict: {type(analysis)}")
            analysis = {
                "summary": "",
                "facts": [],
                "lacunas": ["Erro: Analyst retornou tipo inv√°lido"],
            }
        
        # Garantir campos m√≠nimos
        if not analysis.get("facts"):
            analysis["facts"] = []
        if not analysis.get("lacunas"):
            analysis["lacunas"] = []
        if not analysis.get("summary"):
            analysis["summary"] = ""
        
        if getattr(self.valves, "VERBOSE_DEBUG", False):
            print(f"[DEBUG] [ITERATION] Analyst completed, got {len(analysis.get('summary', ''))} chars summary")
            print(f"[DEBUG] [ITERATION] Analyst facts count: {len(analysis.get('facts', []))}")
            print(f"[DEBUG] [ITERATION] Analyst lacunas count: {len(analysis.get('lacunas', []))}")
        
        # Calculate evidence metrics
        evidence_metrics = _extract_quality_metrics(analysis.get("facts", []))

        # Calculate novelty metrics
        new_facts_ratio, new_domains_ratio = self._calculate_novelty_metrics(analysis, state)

        # Update state with used hashes/domains for next iteration
        import hashlib
        current_fact_hashes = []
        current_domains = set()
        
        for f in analysis.get("facts", []):
            if isinstance(f, dict):
                fact_text = f.get("texto", "")
                if fact_text:
                    current_fact_hashes.append(hashlib.sha256(fact_text.strip().lower().encode("utf-8")).hexdigest())
                
                # Extract domains from evidences
                for ev in f.get("evidencias", []) or []:
                    url = (ev or {}).get("url") or ""
                    try:
                        dom = url.split("/")[2] if "/" in url else ""
                        if dom:
                            current_domains.add(dom)
                    except Exception:
                        pass
        
        # Update state with new hashes/domains (with caps to avoid unbounded growth)
        max_hashes = 5000
        max_domains = 2000
        hashes = state.get("used_claim_hashes", []) + current_fact_hashes
        domains = state.get("used_domains", []) + list(current_domains)
        state["used_claim_hashes"] = hashes[-max_hashes:]
        state["used_domains"] = domains[-max_domains:]
        
        # ===== TELEMETRY FINAL =====
        if tel is not None:
            tel.end_ms = time.time() * 1000
            if tel.counters is not None:
                tel.counters["facts"] = len(analysis.get("facts", []))
                tel.counters["lacunas"] = len(analysis.get("lacunas", []))
            tel.outputs_brief = f"facts={len(analysis.get('facts', []))}, lacunas={len(analysis.get('lacunas', []))}"
            logger.info(f"[{tel.step.upper()}] {json.dumps(tel.to_dict())}")
            
        out = {
            "analysis": analysis,
            "summary": analysis.get('summary', ''),
            "facts": analysis.get('facts', []),
            "lacunas": analysis.get('lacunas', []),
            "self_assessment": analysis.get('self_assessment', {}),
            "evidence_metrics": evidence_metrics,
            "new_facts_ratio": new_facts_ratio,
            "new_domains_ratio": new_domains_ratio,
        }
        await _safe_emit(em, f"[ANALYZE][{correlation_id}] end facts={len(out['facts'])} gaps={len(out['lacunas'])}")
        return out
    
    async def judge_node(self, state: ResearchState) -> Dict:
        """Judge node - complete implementation with loop count increment and telemetry"""
        correlation_id = state.get('correlation_id', 'unknown')
        em = state.get('__event_emitter__')
        await _safe_emit(em, f"[JUDGE][{correlation_id}] start")
        analysis = state.get('analysis', {})
        phase_context = state.get('phase_context', {})
        telemetry_loops = state.get('telemetry_loops', [])
        current_loop = state.get('loop_count', 0)
        
        logger.info(f"[JUDGE][{correlation_id}] Avaliando fase (loop {current_loop})")
        
        try:
            # Prepare telemetry data for Judge
            telemetry_entry = {
                "loop": current_loop,
                "n_facts": len(analysis.get("facts", [])),
                "unique_domains": 0,  # Would be calculated from evidence_metrics
                "new_facts_ratio": analysis.get("new_facts_ratio", 0.0),
                "new_domains_ratio": analysis.get("new_domains_ratio", 0.0),
                "contradictions": 0,  # Would be calculated from analysis
            }
            
            # Add to telemetry loops
            updated_telemetry_loops = telemetry_loops + [telemetry_entry]
            
            # Call Judge LLM with full context
            _emit_decision_snapshot(
                step="judge",
                vector={
                    "loop": current_loop,
                    "facts": len(analysis.get("facts", [])),
                },
                reason="judge_start",
            )
            judgement = await self.judge.run(
                user_prompt=state.get('query', ''),
                analysis=analysis,
                phase_context=phase_context,
                telemetry_loops=updated_telemetry_loops,
                intent_profile=state.get('intent_profile'),
                full_contract=state.get('contract'),
                valves=self.valves,
                refine_queries=state.get('refine_queries'),
                phase_candidates=state.get('phase_candidates'),
                previous_queries=state.get('previous_queries'),
                failed_queries=state.get('failed_queries'),
            )
            
            # Increment loop count
            new_loop_count = current_loop + 1
            
            out = {
                "judgement": judgement,
                "verdict": judgement.get("verdict", "done"),
                "reasoning": judgement.get("reasoning", ""),
                "next_query": judgement.get("next_query", ""),
                "new_phase": judgement.get("new_phase", {}),
                "phase_score": judgement.get("phase_score", 0.0),
                "phase_metrics": judgement.get("phase_metrics", {}),
                "seed_family": judgement.get("seed_family"),
                "modifications": judgement.get("modifications", []),
                # State management
                "loop_count": new_loop_count,
                "telemetry_loops": updated_telemetry_loops,
            }
            _emit_decision_snapshot(
                step="judge",
                vector={
                    "verdict": out.get("verdict"),
                    "phase_score": out.get("phase_score"),
                    "metrics": out.get("phase_metrics"),
                },
                reason="judge_result",
            )
            await _safe_emit(em, f"[JUDGE][{correlation_id}] end verdict={out['verdict']} score={out['phase_score']}")
            return out
        except Exception as e:
            logger.error(f"[JUDGE] Erro: {e}")
            return {
                "judgement": {},
                "verdict": "done",
                "reasoning": f"Erro: {e}",
                "next_query": "",
                "new_phase": {},
                "phase_score": 0.0,
                "phase_metrics": {},
                "seed_family": None,
                "modifications": [],
                # State management
                "loop_count": current_loop + 1,
                "telemetry_loops": telemetry_loops,
            }
    def _calculate_novelty_metrics(self, analysis: Dict, state: ResearchState) -> tuple[float, float]:
        """Calculate novelty metrics for facts and domains"""
        # Get current facts
        current_facts = analysis.get("facts", [])
        if not current_facts:
            return 0.0, 0.0
        
        # Calculate new facts ratio
        used_hashes = set(state.get("used_claim_hashes", []))
        new_facts = 0
        for fact in current_facts:
            if isinstance(fact, dict):
                fact_text = fact.get("texto", "")
                if fact_text:
                    import hashlib
                    fact_hash = hashlib.sha256(fact_text.strip().lower().encode("utf-8")).hexdigest()
                    if fact_hash not in used_hashes:
                        new_facts += 1
        
        new_facts_ratio = new_facts / len(current_facts) if current_facts else 0.0
        
        # Calculate new domains ratio
        used_domains = set(state.get("used_domains", []))
        current_domains = set()
        for fact in current_facts:
            if isinstance(fact, dict):
                for ev in fact.get("evidencias", []) or []:
                    url = (ev or {}).get("url") or ""
                    try:
                        dom = url.split("/")[2] if "/" in url else ""
                        if dom:
                            current_domains.add(dom)
                    except Exception:
                        pass
        
        new_domains = len(current_domains - used_domains)
        new_domains_ratio = new_domains / len(current_domains) if current_domains else 0.0
        
        return new_facts_ratio, new_domains_ratio
def should_continue(state: ResearchState) -> str:
    """
    Router PURO - decide pr√≥ximo n√≥ baseado APENAS em state
    
    Enhanced with programmatic gates from JudgeLLM decision logic:
    - Contradiction hard gate
    - Overlap similarity gate  
    - Phase score thresholds
    - Flat streak logic
    - Diminishing returns detection
    
    Returns:
        "discovery" -> Loop de refinamento
        "done" -> Fim da fase
        "new_phase" -> Criar nova fase (fora do grafo)
    """
    verdict = state.get("verdict")
    judgement = state.get("judgement")
    if not verdict or judgement is None:
        logger.error("[ROUTER] Missing verdict/judgement - forcing done")
        return "done"
    verdict = verdict or "done"
    loop_count = state.get("loop_count", 0)
    max_loops = state.get("max_loops", 3)
    diminishing_returns = state.get("diminishing_returns", False)
    phase_score = state.get("phase_score", 0.0)
    telemetry_loops = state.get("telemetry_loops", [])
    phase_context = state.get("phase_context", {})
    analysis = state.get("analysis", {})
    
    # DEBUG: Log router decision inputs
    if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
        print(f"[ROUTER] === Decision Inputs ===")
        print(f"[ROUTER] loop_count={loop_count}/{max_loops}")
        print(f"[ROUTER] verdict={verdict}")
        print(f"[ROUTER] phase_score={phase_score:.2f}")
        print(f"[ROUTER] diminishing_returns={diminishing_returns}")
        print(f"[ROUTER] telemetry_loops_count={len(telemetry_loops)}")
        print(f"[ROUTER] analysis_keys={list(analysis.keys()) if analysis else 'None'}")
    
    # ===== PROGRAMMATIC GATES (Safety Rails) =====
    
    # Gate 1: Contradiction Hard Gate
    contradiction_score = analysis.get("self_assessment", {}).get("contradiction_score", 0.0)
    contradiction_hard_gate = 0.75  # Default valve value
    if contradiction_score >= contradiction_hard_gate:
        if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
            print(f"[ROUTER][RAIL] Contradictions critical: {contradiction_score:.2f} ‚â• {contradiction_hard_gate}")
        return "new_phase"
    
    # Gate 2: Overlap Similarity Gate
    overlap_similarity = state.get("phase_metrics", {}).get("overlap_similarity", 0.0)
    if overlap_similarity >= 0.90:
        if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
            print(f"[ROUTER][RAIL] Overlap too high: {overlap_similarity:.2f}")
        return "discovery"  # Force refine
    
    # Gate 3: Diminishing Returns Gate
    if diminishing_returns:
        # Check if there are essential gaps mentioned in reasoning
        reasoning = state.get("reasoning", "")
        essential_gap_keywords = ["lacuna essencial", "informa√ß√£o cr√≠tica", "dados fundamentais"]
        has_essential_gaps = any(keyword in reasoning.lower() for keyword in essential_gap_keywords)
        
        if not has_essential_gaps:
            if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
                print(f"[ROUTER][RAIL] Diminishing returns detected without essential gaps")
            return "done"
    
    # ===== PHASE SCORE BASED DECISIONS =====
    
    # Calculate flat streak (consecutive loops without improvement)
    flat_streak = 0
    if telemetry_loops and len(telemetry_loops) >= 2:
        for i in range(len(telemetry_loops) - 1, 0, -1):
            curr = telemetry_loops[i]
            prev = telemetry_loops[i - 1]
            delta_d = curr.get("unique_domains", 0) - prev.get("unique_domains", 0)
            delta_f = curr.get("n_facts", 0) - prev.get("n_facts", 0)
            if delta_d == 0 and delta_f == 0:
                flat_streak += 1
            else:
                break
    
    # Get gates configuration
    gates = getattr(phase_context, "gates", {})
    threshold = gates.get("threshold", 0.60)
    required_flat_loops = gates.get("two_flat_loops", 2)
    coverage_target = 0.70  # Default valve value
    
    # Calculate coverage
    key_questions_coverage = analysis.get("self_assessment", {}).get("coverage_score", 0.0)
    
    # Decision Tree (MECE)
    
    # Rule 1: DONE - Score good + coverage OK
    if verdict == "done" or (phase_score >= threshold and key_questions_coverage >= coverage_target):
        if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
            print(f"[ROUTER] Decision: DONE (verdict={verdict}, score={phase_score:.2f}‚â•{threshold}, coverage={key_questions_coverage:.2f}‚â•{coverage_target})")
        _emit_decision_snapshot(
            step="router",
            vector={
                "verdict": verdict,
                "phase_score": phase_score,
                "threshold": threshold,
                "coverage": key_questions_coverage,
                "coverage_target": coverage_target,
                "flat_streak": flat_streak,
                "loop_count": loop_count,
            },
            reason="done",
        )
        return "done"
    
    # Rule 2: NEW_PHASE - Score low + flat streak reached
    if verdict == "new_phase" or (phase_score < threshold and flat_streak >= required_flat_loops):
        if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
            print(f"[ROUTER] Decision: NEW_PHASE (verdict={verdict}, score={phase_score:.2f}<{threshold}, flat_streak={flat_streak}‚â•{required_flat_loops})")
        _emit_decision_snapshot(
            step="router",
            vector={
                "verdict": verdict,
                "phase_score": phase_score,
                "threshold": threshold,
                "flat_streak": flat_streak,
                "required_flat_loops": required_flat_loops,
                "loop_count": loop_count,
            },
            reason="new_phase",
        )
        return "new_phase"
    
    # Rule 3: REFINE - Budget e next_query v√°lidos
    if verdict == "refine":
        next_query = (state.get("next_query") or "").strip()
        if loop_count >= max_loops:
            logger.warning(f"[ROUTER] Refine requested but budget exhausted (loop {loop_count}/{max_loops})")
            _emit_decision_snapshot(
                step="router",
                vector={"verdict": verdict, "loop_count": loop_count, "max_loops": max_loops, "budget_exhausted": True},
                reason="refine_skipped_budget",
            )
            return "done"
        if not next_query:
            logger.warning(f"[ROUTER] Refine requested but next_query is empty")
            _emit_decision_snapshot(
                step="router",
                vector={"verdict": verdict, "loop_count": loop_count, "max_loops": max_loops, "next_query_present": False},
                reason="refine_skipped_empty_next_query",
            )
            return "done"
        if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
            print(f"[ROUTER] Decision: DISCOVERY (refine; next_query set; loop_count={loop_count}<{max_loops})")
        _emit_decision_snapshot(
            step="router",
            vector={"verdict": verdict, "loop_count": loop_count, "max_loops": max_loops, "next_query_present": True},
            reason="refine",
        )
        return "discovery"
    
    # Rule 4: Fallback -> DONE
    if getattr(state.get("valves"), "VERBOSE_DEBUG", False):
        print(f"[ROUTER] Decision: DONE (fallback - no conditions met)")
    _emit_decision_snapshot(
        step="router",
        vector={"verdict": verdict},
        reason="fallback_done",
    )
    return "done"


# ============================================================================
# 5. BUILD GRAPH
# ============================================================================

def build_research_graph(valves, discovery_tool, scraper_tool, context_reducer_tool=None):
    """
    Constr√≥i o grafo de pesquisa
    
    Fluxo:
    discovery ‚Üí scrape ‚Üí reduce ‚Üí analyze ‚Üí judge
         ‚Üë                              ‚Üì
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ refine ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì done
                      END
    """
    
    # Criar inst√¢ncia dos n√≥s
    nodes = GraphNodes(valves, discovery_tool, scraper_tool, context_reducer_tool)
    
    # Criar grafo
    workflow = StateGraph(ResearchState) if LANGGRAPH_AVAILABLE else StateGraph()
    
    # Adicionar n√≥s
    workflow.add_node("discovery", nodes.discovery_node)
    workflow.add_node("scrape", nodes.scrape_node)
    workflow.add_node("reduce", nodes.reduce_node)
    workflow.add_node("analyze", nodes.analyze_node)
    workflow.add_node("judge", nodes.judge_node)
    
    # Fluxo linear at√© Judge
    workflow.set_entry_point("discovery")
    workflow.add_edge("discovery", "scrape")
    workflow.add_edge("scrape", "reduce")
    workflow.add_edge("reduce", "analyze")
    workflow.add_edge("analyze", "judge")
    
    # Decis√£o condicional ap√≥s Judge
    workflow.add_conditional_edges(
        "judge",
        should_continue,
        {
            "discovery": "discovery",  # Loop refinamento
            "new_phase": END,          # Sair (Pipe cria fase)
            "done": END,               # Concluir fase
        }
    )
    
    # Compilar com checkpointer
    memory = MemorySaver()

    try:
        compiled_graph = workflow.compile(checkpointer=memory)
        # Verify compiled graph has required methods
        if not hasattr(compiled_graph, 'ainvoke'):
            logger.warning("Compiled graph missing ainvoke method")
        return compiled_graph
    except Exception as e:
        logger.error(f"Failed to compile LangGraph: {e}")
        # Return a mock object that will fail gracefully
        class MockGraph:
            async def ainvoke(self, *args, **kwargs):
                raise RuntimeError(f"LangGraph compilation failed: {e}")
        return MockGraph()


class Pipe:
    """
    Pipe compat√≠vel com OpenWebUI - delega ao LangGraph
    
    Responsabilidades:
    1. Gerenciar ciclo de FASES (n√£o loops internos)
    2. Criar novas fases (quando Judge retorna NEW_PHASE)
    3. Chamar s√≠ntese final
    4. Manter fallback para modo manual
    
    TODO: Copiar Valves e m√©todos auxiliares do PipeManual (linhas ~5000-6000)
    """
    
    class Valves(BaseModel):
        """Complete Valves configuration class - copied from PipeHaystack"""
        class Config:
            validate_assignment = True

        def __init__(self, **data):
            super().__init__(**data)

        # UX
        DEBUG_LOGGING: bool = Field(default=False, description="Logs detalhados")
        ENABLE_LINE_BUDGET_GUARD: bool = Field(
            default=False,
            description="Ativar Line-Budget Guard na inicializa√ß√£o (alerta sobre fun√ß√µes muito grandes)",
        )

        # Orquestra√ß√£o
        USE_PLANNER: bool = Field(default=True, description="Usar planner")
        MAX_AGENT_LOOPS: int = Field(
            default=3,
            ge=1,
            le=10,
            description="Max loops/fase (aumentado de 2‚Üí3 para evitar new_phases for√ßadas prematuramente)",
        )
        DEFAULT_PHASE_COUNT: int = Field(
            default=3,
            ge=2,
            le=10,
            description="M√°ximo de fases iniciais (Planner cria AT√â este n√∫mero)",
        )
        MAX_PHASES: int = Field(
            default=6,
            ge=3,
            le=15,
            description="M√°ximo TOTAL de fases (iniciais + criadas pelo Judge)",
        )
        VERBOSE_DEBUG: bool = Field(
            default=False, description="Habilitar logs detalhados de debug"
        )

        # Timeouts (segundos)
        LLM_TIMEOUT_DEFAULT: int = Field(
            default=60,
            ge=30,
            le=300,
            description="Timeout padr√£o para chamadas LLM (Planner, Judge)",
        )
        LLM_TIMEOUT_ANALYST: int = Field(
            default=90,
            ge=30,
            le=300,
            description="Timeout para Analyst (processa mais contexto)",
        )

        # ‚úÖ NOVO: Timeout dedicado para s√≠ntese sem cap
        LLM_TIMEOUT_SYNTHESIS: int = Field(
            default=600,  # ‚¨ÜÔ∏è Aumentado: 300‚Üí600s (10 minutos)
            ge=60,
            le=1800,  # ‚¨ÜÔ∏è M√°ximo: 900‚Üí1800s (30 minutos para casos extremos)
            description="Timeout para S√≠ntese Final (processa muito contexto) - Default 600s (10min), Max 1800s (30min). IMPORTANTE: Se aumentar, garanta que HTTPX_READ_TIMEOUT tamb√©m suba ou ser√° ignorado.",
        )

        # Planner/API behavior
        FORCE_JSON_MODE: bool = Field(
            default=True,
            description="For√ßar response_format=json (quando suportado) para reduzir lat√™ncia e ru√≠do",
        )
        PLANNER_REQUEST_TIMEOUT: int = Field(
            default=180,
            ge=20,
            le=600,
            description="Timeout de leitura HTTP do Planner em segundos (falha r√°pida) ‚Äì default elevado para 180s",
        )
        ENABLE_LLM_RETRY: bool = Field(
            default=True,
            description="Habilitar retry com backoff exponencial para chamadas LLM",
        )
        LLM_MAX_RETRIES: int = Field(
            default=3,
            ge=1,
            le=5,
            description="M√°ximo de tentativas com backoff exponencial",
        )
        HTTPX_READ_TIMEOUT: int = Field(
            default=180,
            ge=60,
            le=600,  # ‚¨ÜÔ∏è Aumentado: 300‚Üí600s
            description="Timeout de leitura HTTP base (httpx client). Para s√≠ntese final, usar LLM_TIMEOUT_SYNTHESIS.",
        )
        LLM_TIMEOUT_PLANNER: int = Field(
            default=180,
            ge=60,
            le=600,
            description="Timeout externo espec√≠fico do Planner (prompts maiores)",
        )

        # Synthesis Control
        ENABLE_DEDUPLICATION: bool = Field(
            default=True, description="Habilitar deduplica√ß√£o na s√≠ntese final"
        )
        PRESERVE_PARAGRAPH_ORDER: bool = Field(
            default=True,
            description="Shuffle para sele√ß√£o justa + reordenar para preservar narrativa (True=recomendado); False=ordenar por tamanho",
        )
        MAX_CONTEXT_CHARS: int = Field(
            default=150000,
            description="M√°ximo de caracteres no contexto para LLM (reduzido para melhor qualidade)",
        )

        # Deduplication Parameters - CALIBRADO PARA QUALIDADE (v4.4)
        MAX_DEDUP_PARAGRAPHS: int = Field(
            default=200,  # ‚¨áÔ∏è Reduzido: 300‚Üí200 par√°grafos (~24k chars, ~6k tokens)
            ge=50,
            le=1000,
            description="M√°ximo de par√°grafos ap√≥s deduplica√ß√£o - Default 200 (~24k chars). ATEN√á√ÉO: >300 pode causar prompt >12k tokens levando a timeout (300s+) ou s√≠ntese gen√©rica!",
        )
        DEDUP_SIMILARITY_THRESHOLD: float = Field(
            default=0.80,  # ‚¨áÔ∏è Reduzido: 0.85‚Üí0.80 (mais agressivo, -20% duplicatas)
            ge=0.0,
            le=1.0,
            description="Threshold de similaridade (0.0-1.0, mais baixo = mais agressivo)",
        )
        DEDUP_RELEVANCE_WEIGHT: float = Field(
            default=0.7,
            ge=0.0,
            le=1.0,
            description="Peso da relev√¢ncia vs diversidade (0.0-1.0, mais alto = mais conservador)",
        )
        DEDUP_ALGORITHM: str = Field(
            default="mmr",
            description="Algoritmo de deduplica√ß√£o: 'mmr' (padr√£o) | 'minhash' (r√°pido) | 'tfidf' (sem√¢ntico) | 'semantic' (Haystack embeddings)",
        )

        CONTEXT_AWARE_PRIORITY_THRESHOLD: float = Field(
            default=0.75,
            ge=0.0,
            le=1.0,
            description="Threshold de prioridade para SEMPRE preservar chunk (0.75 = preserva top 25%)"
        )

        SEMANTIC_MODEL: str = Field(
            default="sentence-transformers/all-MiniLM-L6-v2",
            description="Modelo de embeddings para deduplica√ß√£o sem√¢ntica (lightweight por padr√£o)"
        )

        # Analyst Dedup Strategy
        ANALYST_DEDUP_ALGORITHM: str = Field(
            default="semantic",
            description="Algoritmo para Analyst: 'mmr' | 'minhash' | 'tfidf' | 'semantic'"
        )
        ANALYST_DEDUP_MODEL: str = Field(
            default="sentence-transformers/all-MiniLM-L6-v2",
            description="Modelo embeddings para Analyst (se semantic)"
        )

        # Synthesis Dedup Strategy
        SYNTHESIS_DEDUP_ALGORITHM: str = Field(
            default="mmr",
            description="Algoritmo para Synthesis: 'mmr' | 'minhash' | 'tfidf' | 'semantic'"
        )
        SYNTHESIS_DEDUP_MODEL: str = Field(
            default="sentence-transformers/paraphrase-MiniLM-L3-v2",
            description="Modelo embeddings para Synthesis (se semantic, mais r√°pido)"
        )

        # Context-Aware
        ENABLE_CONTEXT_AWARE_DEDUP: bool = Field(
            default=True,
            description="Ativar dedup context-aware (preserva must_terms/key_questions)"
        )
        CONTEXT_AWARE_PRESERVE_PCT: float = Field(
            default=0.12,  # P0: Reduzido temporariamente para evitar preserva√ß√£o excessiva
            description="% de chunks high-priority a preservar (0.0-1.0)"
        )

        # Deduplication for Analyst (per-iteration)
        ENABLE_ANALYST_DEDUPLICATION: bool = Field(
            default=False,
            description="Dedupe contexto ANTES de enviar ao Analyst (reduz tokens, mant√©m contexto completo para pr√≥ximas itera√ß√µes)",
        )
        MAX_ANALYST_PARAGRAPHS: int = Field(
            default=200,
            ge=50,
            le=500,
            description="M√°ximo de par√°grafos para Analyst (~24k chars, ~6k tokens) - Analyst processa menos que S√≠ntese",
        )
        ANALYST_PRESERVE_RECENT_PCT: float = Field(
            default=1.0,  # 100% by default - preserve ALL new content
            ge=0.0,
            le=1.0,
            description="% of recent content to preserve intact in Analyst deduplication (1.0 = 100% preserved, 0.95 = old behavior, 0.0 = dedupe everything)"
        )

        # Official domains mapping (used by discovery/scoring and diversity caps)
        OFFICIAL_DOMAINS: Dict[str, List[str]] = Field(
            default_factory=lambda: {
                "default": ["gov.br", ".gov", "bcb.gov.br", "cvm.gov.br"],
                "regulation_review": ["planalto.gov.br", "camara.leg.br", "senado.leg.br"],
            },
            description="Lista de dom√≠nios oficiais por perfil",
        )

        # Diversity caps by profile (for context selection)
        DIVERSITY_CAPS_BY_PROFILE: Dict[str, Dict[str, int]] = Field(
            default_factory=lambda: {
                "default": {"min_new_domains": 2, "min_official": 1, "min_independent": 2},
                "conservative": {"min_new_domains": 1, "min_official": 2, "min_independent": 1},
            },
            description="M√≠nimos por bucket para sele√ß√£o de contexto, por perfil",
        )

        # Continue detection configuration
        CONTINUE_TERMS_OVERRIDE: Optional[List[str]] = Field(
            default=None,
            description="Substitui termos padr√£o de detec√ß√£o de 'siga' (se None, usa defaults)",
        )
        STRICT_CONTINUE_ACTIVATION: bool = Field(
            default=True,
            description="Ativar gate estrito para execu√ß√£o mesmo quando detec√ß√£o ampla for positiva",
        )

        # Judge Duplicate Detection
        DUPLICATE_DETECTION_THRESHOLD: float = Field(
            default=0.70,
            ge=0.5,
            le=0.9,
            description="Threshold for NEW_PHASE duplicate detection (0.70 = 70% similarity blocks duplicate, lower = more lenient, higher = more strict)"
        )

        # Quality Rails Parameters
        MIN_UNIQUE_DOMAINS: int = Field(
            default=2, description="M√≠nimo de dom√≠nios √∫nicos por fase"
        )
        REQUIRE_OFFICIAL_OR_TWO_INDEPENDENT: bool = Field(
            default=True, description="Exigir fonte oficial ou duas independentes"
        )

        # --- FASE 1: Entity Coverage ---
        MIN_ENTITY_COVERAGE: float = Field(
            default=0.70,
            ge=0.0,
            le=1.0,
            description="Cobertura m√≠nima de entidades nas fases (0.70 = 70% das fases devem conter entidades)",
        )
        MIN_ENTITY_COVERAGE_STRICT: bool = Field(
            default=True,
            description="True = hard-fail se coverage < MIN_ENTITY_COVERAGE | False = warning + Judge decide",
        )

        # --- FASE 1: Seeds ---
        SEED_VALIDATION_STRICT: bool = Field(
            default=False,
            description="False = tenta patch leve em seeds magras; True = s√≥ valida (sem patch)",
        )

        # --- FASE 1: News slot ---
        ENFORCE_NEWS_SLOT: bool = Field(
            default=True,
            description="Manter a pol√≠tica existente de news slot; adicionamos telemetria",
        )

        # --- FASE 2: Planner economy controls ---
        PLANNER_ECONOMY_THRESHOLD: float = Field(
            default=0.75,
            ge=0.0,
            le=1.0,
            description="Threshold de uso de budget de fases para warning (0.75 = warn se >75% usado)",
        )

        PLANNER_OVERLAP_THRESHOLD: float = Field(
            default=0.70,
            ge=0.0,
            le=1.0,
            description="Threshold de similaridade entre objectives para detectar overlap (0.70 = 70% similaridade via TF-IDF)",
        )

        # Preferred tools mapping (deterministic resolution)
        PREFERRED_TOOLS: Dict[str, str] = Field(
            default={}, description="Preferred tool names for deterministic resolution"
        )

        # (removido: OFFICIAL_DOMAINS duplicado por vertical; usar OFFICIAL_DOMAINS por perfil definido acima)

        # LLM Configuration
        OPENAI_BASE_URL: str = Field(
            default="https://api.openai.com/v1", description="API Base"
        )
        OPENAI_API_KEY: str = Field(default="", description="API Key")
        LLM_MODEL: str = Field(
            default="gpt-4o", description="Modelo padr√£o para todos os componentes"
        )
        LLM_TEMPERATURE: float = Field(
            default=0.2, ge=0.0, le=1.0, description="Temperature"
        )
        LLM_MAX_TOKENS: int = Field(
            default=2048,
            ge=100,
            le=4000,
            description="‚ö†Ô∏è DEPRECATED: N√£o usado pelo Pipe (incompat√≠vel GPT-5/O1). Mantido para compatibilidade.",
        )

        # Modelos espec√≠ficos por componente (opcional - deixe vazio para usar LLM_MODEL)
        LLM_MODEL_PLANNER: str = Field(
            default="", description="Modelo para Planner (vazio = usa LLM_MODEL)"
        )
        LLM_MODEL_ANALYST: str = Field(
            default="", description="Modelo para Analyst (vazio = usa LLM_MODEL)"
        )
        LLM_MODEL_JUDGE: str = Field(
            default="", description="Modelo para Judge (vazio = usa LLM_MODEL)"
        )
        LLM_MODEL_SYNTHESIS: str = Field(
            default="",
            description="Modelo para S√≠ntese Final (vazio = usa LLM_MODEL) - Use modelo mais capaz aqui!",
        )

        # Context Reducer
        ENABLE_CONTEXT_REDUCER: bool = Field(
            default=True, description="Habilitar Context Reducer"
        )
        CONTEXT_MODE: str = Field(
            default="light", description="Modo: coarse|light|ultra"
        )

        # ‚úÖ NOVO: Controle de exporta√ß√£o PDF
        AUTO_EXPORT_PDF: bool = Field(
            default=False,
            description="Exportar automaticamente relat√≥rio para PDF ao final da s√≠ntese",
        )

        EXPORT_FULL_CONTEXT: bool = Field(
            default=True,
            description="Se True, exporta contexto bruto completo; se False, apenas relat√≥rio final",
        )
        # Gates por perfil (P1) - EXPANDIDO v4.7: phase_score thresholds + two_flat_loops
        GATES_BY_PROFILE: Dict[str, Dict[str, Any]] = Field(
            default={
                "company_profile": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": True,
                    "threshold": 0.60,
                    "two_flat_loops": 2,
                },
                "regulation_review": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": True,
                    "threshold": 0.62,
                    "two_flat_loops": 1,
                },
                "technical_spec": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": True,
                    "threshold": 0.60,
                    "two_flat_loops": 2,
                },
                "literature_review": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": False,
                    "threshold": 0.58,
                    "two_flat_loops": 2,
                },
                "history_review": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": False,
                    "threshold": 0.58,
                    "two_flat_loops": 2,
                },
                # Phase-type specific gates (usado quando phase_context tem phase_type)
                "news": {
                    "min_new_facts": 0.7,
                    "min_new_domains": 0.5,
                    "staleness_strict": True,
                    "threshold": 0.65,
                    "two_flat_loops": 2,
                },
                "industry": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": False,
                    "threshold": 0.58,
                    "two_flat_loops": 2,
                },
                "profiles": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": True,
                    "threshold": 0.60,
                    "two_flat_loops": 2,
                },
                "tech": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": False,
                    "threshold": 0.60,
                    "two_flat_loops": 2,
                },
                "regulatory": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": True,
                    "threshold": 0.62,
                    "two_flat_loops": 1,
                },
                "financials": {
                    "min_new_facts": 0.6,
                    "min_new_domains": 0.4,
                    "staleness_strict": True,
                    "threshold": 0.64,
                    "two_flat_loops": 1,
                },
            },
            description="Gates de novidade/staleness por perfil + phase_score thresholds (v4.7)",
        )
        # ‚úÖ NOVO v4.7: Phase Score Weights (audit√°vel, configur√°vel)
        PHASE_SCORE_WEIGHTS: Dict[str, float] = Field(
            default={
                "w_cov": 0.35,  # Peso: coverage (key_questions respondidas)
                "w_nf": 0.25,  # Peso: novel_fact_ratio (fatos novos)
                "w_nd": 0.15,  # Peso: novel_domain_ratio (dom√≠nios novos)
                "w_div": 0.15,  # Peso: domain_diversity (distribui√ß√£o uniforme)
                "w_contra": 0.40,  # Penalidade: contradiction_score (contradi√ß√µes)
            },
            description="Pesos para c√°lculo de phase_score (v4.7) - Score = w_cov*coverage + w_nf*novel_facts + w_nd*novel_domains + w_div*diversity - w_contra*contradictions",
        )

        # ‚úÖ NOVO v4.7: Coverage target global (usado em decis√£o DONE)
        COVERAGE_TARGET: float = Field(
            default=0.70,
            ge=0.0,
            le=1.0,
            description="Target m√≠nimo de coverage para considerar DONE (0.0-1.0, default 0.70 = 70%)",
        )

        # ‚úÖ NOVO v4.7: Contradiction hard gate (for√ßa NEW_PHASE imediato)
        CONTRADICTION_HARD_GATE: float = Field(
            default=0.75,
            ge=0.0,
            le=1.0,
            description="Threshold de contradiction_score para for√ßar NEW_PHASE imediato (0.0-1.0, default 0.75)",
        )
    
    def __init__(self):
        self.valves = self.Valves()
        self._last_contract = None
        self._detected_context = None
        self._phase_results = []
        
        # Initialize LLM components lazily; will be created after valves merge
        self.analyst = None
        self.judge = None
        self.planner = None
        self.deduplicator = None
        
        logger.info("[PIPE] Pipe inicializado")
    def _generate_pdf_base64(self, html_content: str, title: str) -> Optional[str]:
        """Generate PDF from HTML content and return as base64 string"""
        try:
            from weasyprint import HTML, CSS
            import base64
            
            # Generate PDF in memory
            pdf_bytes = HTML(string=html_content).write_pdf()
            
            # Encode to base64
            pdf_b64 = base64.b64encode(pdf_bytes).decode('utf-8')
            
            return pdf_b64
        except ImportError:
            logger.warning("weasyprint not installed, PDF export disabled")
            return None
        except Exception as e:
            logger.error(f"PDF generation failed: {e}")
            return None
    
    async def pipe(
        self,
        body: dict,
        __user__: dict = None,
        __tools__: dict = None,
        __event_emitter__: Optional[Callable] = None,
        **kwargs,
    ) -> AsyncGenerator[str, None]:
        """Complete pipe method implementation from PipeHaystack"""
        # Generate correlation ID for request tracing
        correlation_id = str(uuid.uuid4())[:8]
        logger.info(
            "Pipeline execution started", extra={"correlation_id": correlation_id}
        )

        # Validate internal state before processing
        self._validate_pipeline_state()

        # Extract current date from OpenWebUI metadata or system
        current_date = None
        try:
            metadata = (body or {}).get("metadata", {})
            variables = metadata.get("variables", {}) if metadata else {}
            current_date = variables.get("{{CURRENT_DATE}}") or variables.get(
                "CURRENT_DATE"
            )
        except Exception:
            pass
        if not current_date:
            from datetime import datetime
            current_date = datetime.now().strftime("%Y-%m-%d")

        # Store for use in prompts
        self._current_date = current_date

        # read valves override from body
        val_in = (body or {}).get("valves")
        if val_in:
            try:
                # pydantic copy/update
                self.valves = (
                    self.valves.model_copy(update=val_in)
                    if hasattr(self.valves, "model_copy")
                    else self.Valves(**{**self.valves.__dict__, **val_in})
                )
            except Exception:
                # best-effort set attributes
                for k, v in (val_in or {}).items():
                    if hasattr(self.valves, k):
                        setattr(self.valves, k, v)
            # Re-init components lazily after valves change
            if self.analyst is None:
                self.analyst = AnalystLLM(self.valves)
            if self.judge is None:
                self.judge = JudgeLLM(self.valves)
            if self.planner is None:
                self.planner = PlannerLLM(self.valves)
            if self.deduplicator is None:
                self.deduplicator = Deduplicator(self.valves)

        user_msg = (
            body.get("messages", [{"content": ""}])[-1].get("content", "") or ""
        ).strip()
        low = user_msg.lower()

        # üéØ DETEC√á√ÉO DE INTEN√á√ÉO (antes de qualquer processamento)
        # Comandos de continua√ß√£o
        # Detec√ß√£o ampla vs ativa√ß√£o estrita do comando "siga":
        # - Detec√ß√£o ampla (detectar inten√ß√£o): lista flex√≠vel de termos (override por valves)
        # - Ativa√ß√£o estrita (executar plano): subconjunto opcional/estrito, controlado por valve
        continue_terms_default = (
            "siga","continue","prosseguir","continua","prossegue",
            "go on","keep going","next"
        )
        terms_override = getattr(self.valves, "CONTINUE_TERMS_OVERRIDE", None)
        continue_terms = tuple(terms_override) if terms_override else continue_terms_default
        is_continue_command = any(t in low for t in continue_terms)

        # Gate estrito opcional para ativa√ß√£o (evita auto-execu√ß√£o por termos amb√≠guos)
        strict_activation = getattr(self.valves, "STRICT_CONTINUE_ACTIVATION", True)
        if strict_activation:
            strict_terms = {"siga", "continue", "prosseguir"}
            is_strict_activation = any(t in low for t in strict_terms)
        else:
            is_strict_activation = is_continue_command

        # ‚úÖ NOVO: Detectar inten√ß√£o de refinamento de plano
        refinement_keywords = [
            "adicione", "inclua", "acrescente", "mude", "altere", "ajuste",
            "remova", "tire", "delete", "corrija", "refine", "modifique",
            "aumente", "reduza", "expanda", "foque mais", "menos em", "troque",
            "substitua", "adapte", "personalize", "atualize",
        ]
        is_refinement = any(kw in low for kw in refinement_keywords)
        has_previous_plan = bool(self._last_contract)

        # üîí DECIS√ÉO: Preservar contexto ou re-detectar?
        should_preserve_context = (is_strict_activation and is_continue_command) or (
            is_refinement and has_previous_plan
        )

        if should_preserve_context and self._detected_context:
            # Manter contexto anterior (refinamento ou siga)
            logger.info(
                f"[PIPE] Contexto preservado: is_continue={is_continue_command}, strict_activation={is_strict_activation}, is_refinement={is_refinement}, has_plan={has_previous_plan}"
            )
            _emit_decision_snapshot(
                step="router_pre",
                vector={
                    "is_continue": is_continue_command,
                    "strict_activation": is_strict_activation,
                    "is_refinement": is_refinement,
                    "has_previous_plan": has_previous_plan,
                },
                reason="preserve_context",
            )
            yield f"**[CONTEXT]** üîí Mantendo contexto: {self._detected_context.get('perfil', 'N/A')}\n"
            if is_refinement:
                yield f"**[INFO]** üí° Modo refinamento detectado - ajustando plano existente\n"
        else:
            # Re-detectar contexto (nova query)
            if hasattr(self, '_context_locked') and self._context_locked:
                logger.info("[PIPE] Nova query detectada - desbloqueando contexto")
                self._context_locked = False
                self._detected_context = None

            self._detected_context = await self._detect_unified_context(user_msg, body)
            logger.info(f"[PIPE] Contexto detectado: {self._detected_context}")
            _emit_decision_snapshot(
                step="router_pre",
                vector={"detected_context": self._detected_context or {}},
                reason="detect_context",
            )

            # üîó SINCRONIZAR PERFIL DETECTADO com intent_profile (fonte √∫nica de verdade)
            if self._detected_context:
                self._intent_profile = self._detected_context.get(
                    "perfil", "company_profile"
                )
                logger.info(f"[PIPE] Perfil sincronizado: {self._intent_profile}")
                yield f"**[CONTEXT]** üîç Perfil: {self._detected_context.get('perfil', 'N/A')} | Setor: {self._detected_context.get('setor', 'N/A')} | Tipo: {self._detected_context.get('tipo', 'N/A')}\n"

        d_callable, s_callable, cr_callable = self._resolve_tools(__tools__ or {})

        # Manual route - single execution path
        # If the user asks to continue (siga), execute stored contract
        if low in {"siga", "continue", "prosseguir"}:
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(
                    f"[DEBUG] SIGA mode: _last_contract exists: {bool(self._last_contract)}"
                )

            # Try to get contract from stored state first
            if not self._last_contract:
                # Try to extract contract from message history
                yield "**[INFO]** Procurando plano no hist√≥rico...\n"
                contract = await self._extract_contract_from_history(body)
                if contract:
                    self._last_contract = contract
                    yield "**[INFO]** ‚úÖ Plano recuperado do hist√≥rico\n"
                else:
                    yield "**[AVISO]** Nenhum contrato pendente ou encontrado no hist√≥rico\n"
                    yield "**[DICA]** Tente criar um novo plano primeiro\n"
                    return

            job_id = f"cached_{int(time.time() * 1000)}"

            # Check if LangGraph is available and working
            if not LANGGRAPH_AVAILABLE:
                yield "**[ERRO]** LangGraph n√£o est√° dispon√≠vel. Instale: pip install langgraph\n"
                return

            try:
                # Test if StateGraph can be instantiated
                test_workflow = StateGraph(ResearchState) if LANGGRAPH_AVAILABLE else StateGraph()
                test_workflow.add_node("test", lambda x: x)
            except Exception as e:
                yield f"**[ERRO]** LangGraph n√£o est√° funcionando corretamente: {e}\n"
                yield "**[SUGEST√ÉO]** Reinstale o LangGraph: pip uninstall langgraph && pip install langgraph\n"
                return

            # Build graph and execute phases
            graph = build_research_graph(
                self.valves, d_callable, s_callable, cr_callable
            )

            # Verify graph has required methods
            if not hasattr(graph, 'ainvoke'):
                yield "**[ERRO]** Grafo compilado n√£o tem m√©todo ainvoke. Verifique instala√ß√£o do LangGraph\n"
                return

            yield f"\n### üöÄ Execu√ß√£o iniciada com LangGraph\n"
            
            # ===== GLOBAL STATE PERSISTENCE =====
            # Initialize global state that persists across all phases
            global_state = {
                "scraped_cache": {},  # Shared URL cache across phases
                "used_claim_hashes": [],  # Novelty tracking across phases
                "used_domains": [],  # Domain diversity tracking
                "phase_results": [],  # Accumulated results
                "accumulated_context": "",  # Global context accumulation
                "telemetry_loops": [],  # Global telemetry
            }
            
            phase_results = []
            telemetry_data = {
                "execution_id": job_id,
                "start_time": time.time(),
                "phases": [],
            }

            # ===== PHASE EXECUTION WITH STATE PERSISTENCE =====
            phases_to_execute = self._last_contract.get("fases", []).copy()
            phase_index = 0
            
            while phase_index < len(phases_to_execute):
                phase_index += 1
                ph = phases_to_execute[phase_index - 1]
                objetivo, q = ph["objetivo"], ph["query_sugerida"]
                
                yield f"\n**Fase {phase_index}** ‚Äì {objetivo}\n"
                
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    yield f"**[DEBUG]** Cache global: {len(global_state['scraped_cache'])} URLs, {len(global_state['used_claim_hashes'])} hashes\n"

                # Initialize state for this phase with GLOBAL STATE
                initial_state = ResearchState(
                    query=q,
                    phase_context=ph,
                    correlation_id=correlation_id,
                    loop_count=0,
                    max_loops=self.valves.MAX_AGENT_LOOPS,
                    # ===== PERSISTENT STATE =====
                    scraped_cache=global_state["scraped_cache"].copy(),
                    accumulated_context=global_state["accumulated_context"],
                    telemetry_loops=global_state["telemetry_loops"].copy(),
                    used_claim_hashes=global_state["used_claim_hashes"].copy(),
                    used_domains=global_state["used_domains"].copy(),
                    phase_results=global_state["phase_results"].copy(),
                    # ===== PHASE-SPECIFIC STATE =====
                    job_id=job_id,
                    valves=self.valves,
                    original_query=q,
                    phase_index=phase_index,
                    contract=self._last_contract,
                    all_phase_queries=[phase.get("query_sugerida", "") for phase in phases_to_execute],
                    intent_profile=self._detected_context.get("perfil", "general") if self._detected_context else "general",
                    discovered_urls=[],
                    new_urls=[],
                    cached_urls=list(global_state["scraped_cache"].keys()),
                    raw_content="",
                    filtered_content="",
                    analysis={},
                    evidence_metrics={},
                    judgement={},
                    verdict="",
                    phase_score=0.0,
                    phase_metrics={},
                    seed_family=None,
                    modifications=[],
                    coverage_score=0.0,
                    entities_covered=[],
                    analyst_confidence="baixa",
                    gaps_critical=False,
                    suggest_refine=False,
                    suggest_pivot=False,
                    diminishing_returns=False,
                    failed_query=False,
                    new_phase_proposed=False,
                    phase_candidates=[],
                    refine_queries=[],
                    previous_queries=[],
                    failed_queries=[],
                    new_facts_ratio=1.0,
                    new_domains_ratio=1.0,
                    unique_domains=0,
                    n_facts=0,
                    contradictions=0,
                    high_confidence_facts=0,
                    facts_with_multiple_sources=0,
                    lacunas_count=0,
                    seed_core_source=None,
                    analyst_proposals=[],
                    final_synthesis=None,
                )

                # ===== EXECUTE GRAPH (LangGraph manages internal loops) =====
                try:
                    phase_start_time = time.time()
                    
                    # ‚úÖ CRITICAL: Let LangGraph handle ALL internal loops
                    # The graph will automatically loop discovery‚Üíscrape‚Üíreduce‚Üíanalyze‚Üíjudge
                    # based on the should_continue router decisions
                    try:
                        final_state = await graph.ainvoke(initial_state)
                    except AttributeError as e:
                        if "ainvoke" in str(e):
                            yield f"**[ERRO]** M√©todo ainvoke n√£o dispon√≠vel no grafo. Verifique instala√ß√£o do LangGraph\n"
                            continue
                        else:
                            raise
                    except Exception as e:
                        yield f"**[ERRO]** Falha na execu√ß√£o do LangGraph: {e}\n"
                        logger.error(f"LangGraph execution failed: {e}")
                        continue
                    
                    phase_duration = time.time() - phase_start_time
                    
                    # Process result
                    verdict = final_state.get("verdict", "done")
                    loop_count = final_state.get("loop_count", 0)
                    
                    yield f"**[FASE {phase_index}]** Verdict: {verdict} (loops: {loop_count})\n"
                    yield f"**[FASE {phase_index}]** Dura√ß√£o: {phase_duration:.1f}s\n"
                    
                    # ===== UPDATE GLOBAL STATE =====
                    # Preserve state for next phase
                    global_state.update({
                        "scraped_cache": final_state.get("scraped_cache", global_state["scraped_cache"]),
                        "used_claim_hashes": final_state.get("used_claim_hashes", global_state["used_claim_hashes"]),
                        "used_domains": final_state.get("used_domains", global_state["used_domains"]),
                        "accumulated_context": final_state.get("accumulated_context", global_state["accumulated_context"]),
                        "telemetry_loops": final_state.get("telemetry_loops", global_state["telemetry_loops"]),
                    })
                    
                    # Add to phase results
                    phase_result = {
                        "phase": phase_index,
                        "objective": objetivo,
                        "result": final_state,
                        "verdict": verdict,
                        "loops": loop_count,
                        "duration": phase_duration,
                    }
                    phase_results.append(phase_result)
                    global_state["phase_results"].append(phase_result)
                    
                    # ===== HANDLE NEW_PHASE VERDICT =====
                    if verdict == "new_phase":
                        new_phase = final_state.get("new_phase", {})
                        if new_phase:
                            # Add new phase to the execution queue
                            phases_to_execute.append(new_phase)
                            yield f"**[NOVA FASE]** Adicionada: {new_phase.get('objetivo', 'N/A')}\n"
                            yield f"**[INFO]** Total de fases: {len(phases_to_execute)}\n"
                    
                    # ===== DEBUG STATE PERSISTENCE =====
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        yield f"**[DEBUG]** Estado global atualizado:\n"
                        yield f"  - Cache: {len(global_state['scraped_cache'])} URLs\n"
                        yield f"  - Hashes: {len(global_state['used_claim_hashes'])} claims\n"
                        yield f"  - Domains: {len(global_state['used_domains'])} domains\n"
                        yield f"  - Context: {len(global_state['accumulated_context'])} chars\n"
                    
                except Exception as e:
                    logger.error(f"Phase {phase_index} execution failed: {e}")
                    yield f"**[ERRO]** Falha na fase {phase_index}: {e}\n"
                    # Continue with next phase even if one fails
                    continue

            # Emitir telemetria estruturada
            if self.valves.DEBUG_LOGGING:
                yield f"\n**[TELEMETRIA]** üìä Dados estruturados da execu√ß√£o:\n"
                yield f"```json\n{json.dumps(telemetry_data, indent=2, ensure_ascii=False)}\n```\n"

            yield f"\n**[S√çNTESE FINAL]**\n"
            # Call _synthesize_final with phase results
            async for synthesis_chunk in self._synthesize_final(phase_results, global_state, cr_callable, user_msg, body, __event_emitter__):
                yield synthesis_chunk

            # üîì UNLOCK contexto ap√≥s conclus√£o para permitir nova detec√ß√£o
            self._last_contract = None
            if hasattr(self, '_context_locked'):
                self._context_locked = False
            logger.info("[PIPE] Context unlocked after completion")
            return

        # Otherwise, build a contract using PlannerLLM
        if not self.valves.USE_PLANNER:
            raise ValueError("USE_PLANNER desabilitado")

        requested_phases = _extract_phase_count(user_msg)
        phases = (
            requested_phases if requested_phases else self.valves.DEFAULT_PHASE_COUNT
        )

        yield f"**[PLANNER]** At√© {phases} fases (conforme necess√°rio)...\n\n"

        planner = PlannerLLM(self.valves)
        out = await planner.run(
            user_prompt=user_msg,
            phases=phases,
            current_date=getattr(self, "_current_date", None),
            detected_context=self._detected_context,
        )

        # Validate contract
        if not isinstance(out, dict) or "phases" not in out:
            yield "**[ERRO]** Contrato inv√°lido gerado pelo Planner\n"
            return

        self._last_contract = out
        yield "**[INFO]** ‚úÖ Contrato gerado com sucesso\n"

        # Render contract for user
        yield f"\nüìã **Plano de Pesquisa**\n\n"
        for i, phase in enumerate(out.get("phases", []), 1):
            yield f"**Fase {i}:** {phase.get('objective', 'N/A')}\n"
            yield f"- Query: {phase.get('query_sugerida', 'N/A')}\n"
            yield f"- Seed: {phase.get('seed_core', 'N/A')}\n\n"

        yield "**[INFO]** Digite 'siga' para executar o plano\n"

    async def _synthesize_final(
        self,
        phase_results: List[Dict],
        global_state: Optional[Dict] = None,
        cr_callable: Optional[Callable] = None,
        user_msg: str = "",
        body: dict = None,
        __event_emitter__=None,
    ):
        """S√≠ntese final adaptativa com Context Reducer ou deduplica√ß√£o + LLM

        FLUXO:
        1. **Context Reducer** (se habilitado): Redu√ß√£o global com todas as queries
        2. **Fallback** (se CR desabilitado/falhar):
           - Deduplica√ß√£o MMR-lite (preserva ordem ou ordena por tamanho)
           - Truncamento ao MAX_CONTEXT_CHARS
           - Detec√ß√£o de contexto unificado (usa _detected_context ou detecta)
           - S√≠ntese adaptativa com UMA chamada LLM (prompt din√¢mico)

        ADAPTIVE SYNTHESIS:
        - Usa _detected_context para determinar setor, tipo, perfil
        - Gera prompt com instru√ß√µes espec√≠ficas para o contexto
        - Estrutura de relat√≥rio adaptada (se√ß√µes, estilo, foco)
        - Substituiu hardcoding de HPPC por template gen√©rico

        Args:
            phase_results: Lista de resultados de cada fase executada
            orch: Orchestrator com contexto acumulado e cache de URLs
            user_msg: Query original do usu√°rio (para detec√ß√£o de contexto)
            body: Body da requisi√ß√£o (para hist√≥rico de mensagens)
        """
        # Try Context Reducer first (direct tool invocation)
        if self.valves.ENABLE_CONTEXT_REDUCER and cr_callable:
            try:
                await _safe_emit(__event_emitter__, "**[S√çNTESE]** Context Reducer global...\n")
                yield "**[S√çNTESE]** Context Reducer global...\n"
                
                # Get accumulated context from global state or phase results
                accumulated_context = ""
                if global_state and "accumulated_context" in global_state:
                    accumulated_context = global_state["accumulated_context"]
                elif phase_results:
                    # Fallback: get from last phase result
                    last_result = phase_results[-1].get("result", {})
                    accumulated_context = last_result.get("accumulated_context", "")
                
                if not accumulated_context:
                    await _safe_emit(__event_emitter__, "**[INFO]** Nenhum contexto acumulado dispon√≠vel\n")
                    yield "**[INFO]** Nenhum contexto acumulado dispon√≠vel\n"
                    final = None
                else:
                    # Prepare context reducer input
                    all_queries = [
                        phase.get("query_sugerida", "") 
                        for phase in phase_results 
                        if "query_sugerida" in phase
                    ]
                    
                    context_params = {
                        "corpo": {
                            "ultrasearcher_result": {
                                "scraped_content": accumulated_context
                            }
                        },
                        "mode": self.valves.CONTEXT_MODE,
                        "queries": all_queries,
                        "tipo": "final",
                    }
                    
                    # Call Context Reducer tool
                    if asyncio.iscoroutinefunction(cr_callable):
                        final = await cr_callable(**context_params)
                    else:
                        final = await asyncio.to_thread(cr_callable, **context_params)
                    
                    # Extract result
                    if isinstance(final, dict):
                        final = final.get("final_markdown", accumulated_context)
                    elif not final:
                        final = accumulated_context

                if final:
                    await _safe_emit(__event_emitter__, f"\n{final}\n\n")
                    yield f"\n{final}\n\n"

                    # Get scraped cache from global state or phase results
                    scraped_cache = {}
                    if global_state and "scraped_cache" in global_state:
                        scraped_cache = global_state["scraped_cache"]
                    elif phase_results:
                        # Fallback: get from last phase result
                        last_result = phase_results[-1].get("result", {})
                        scraped_cache = last_result.get("scraped_cache", {})
                    
                    total_urls = len(scraped_cache)
                    total_phases = len(phase_results)
                    await _safe_emit(__event_emitter__, f"\n---\n**üìä Estat√≠sticas:**\n")
                    yield f"\n---\n**üìä Estat√≠sticas:**\n"
                    await _safe_emit(__event_emitter__, f"- Fases: {total_phases}\n")
                    yield f"- Fases: {total_phases}\n"
                    await _safe_emit(__event_emitter__, f"- URLs √∫nicas scraped: {total_urls}\n")
                    yield f"- URLs √∫nicas scraped: {total_urls}\n"
                    await _safe_emit(__event_emitter__, f"- Contexto acumulado: {len(accumulated_context)} chars\n")
                    yield f"- Contexto acumulado: {len(accumulated_context)} chars\n"
                    return
                    
            except TypeError as e:
                # Tool doesn't support these parameters
                yield f"**[WARNING]** Context Reducer params mismatch: {e}\n"
                final = None
            except Exception as e:
                yield f"**[ERRO]** Context Reducer: {e}\n"
                final = None

        # Fallback: deduplicar (merge+dedupe+MMR-lite) e depois sintetizar com UMA √öNICA chamada ao LLM
        await _safe_emit(__event_emitter__, "**[S√çNTESE]** S√≠ntese completa e detalhada (sem Context Reducer)...\n")
        yield "**[S√çNTESE]** S√≠ntese completa e detalhada (sem Context Reducer)...\n"

        def _paragraphs(text: str) -> List[str]:
            """Extrai par√°grafos do texto, suportando m√∫ltiplos formatos de separa√ß√£o

            Detecta automaticamente o formato baseado na densidade de par√°grafos:
            - Densidade baixa (>1000 chars/par√°grafo m√©dio) ‚Üí markdown com \n √∫nico
            - Densidade alta (<500 chars/par√°grafo m√©dio) ‚Üí formato normal com \n\n
            """
            if not text:
                return []

            # Tentar primeiro com duplo newline (formato padr√£o do accumulator)
            parts = [p.strip() for p in text.split("\n\n") if p.strip()]

            # Calcular densidade de par√°grafos (chars por par√°grafo)
            avg_paragraph_size = len(text) / max(len(parts), 1)

            # Se densidade √© muito baixa (par√°grafos muito grandes), provavelmente √© markdown com \n √∫nico
            # Threshold: >1000 chars/par√°grafo m√©dio indica blocos gigantes, n√£o par√°grafos reais
            if avg_paragraph_size > 1000:
                # Markdown do scraper/Context Reducer usa \n √∫nico
                # Agrupar linhas n√£o vazias em blocos (par√°grafos)
                lines = text.split("\n")
                paragraphs = []
                current_block = []

                for line in lines:
                    line_stripped = line.strip()
                    if line_stripped:
                        current_block.append(line_stripped)
                    else:
                        # Linha vazia = fim de par√°grafo
                        if current_block:
                            paragraph = " ".join(current_block)
                            if len(paragraph) > 20:  # Filtrar par√°grafos muito curtos
                                paragraphs.append(paragraph)
                            current_block = []

                # Adicionar √∫ltimo bloco
                if current_block:
                    paragraph = " ".join(current_block)
                    if len(paragraph) > 20:
                        paragraphs.append(paragraph)

                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(
                        f"[DEDUP] Densidade baixa detectada ({avg_paragraph_size:.0f} chars/par√°grafo)"
                    )
                    print(
                        f"[DEDUP] Usando extra√ß√£o linha-por-linha: {len(parts)} ‚Üí {len(paragraphs)} par√°grafos"
                    )

                # ‚úÖ v4.8.1: Se ainda houver par√°grafos gigantes, quebrar por senten√ßas/tamanho m√°ximo
                max_paragraph_chars = getattr(self.valves, "DEDUP_MAX_PARAGRAPH_CHARS", 1200)
                final_paragraphs: List[str] = []
                sentence_splitter = re.compile(r"(?<=[.!?])\s+")

                for paragraph in paragraphs:
                    if len(paragraph) <= max_paragraph_chars:
                        final_paragraphs.append(paragraph)
                        continue

                    sentences = [s.strip() for s in sentence_splitter.split(paragraph) if s.strip()]
                    chunk: List[str] = []
                    chunk_len = 0

                    for sentence in sentences:
                        sentence_len = len(sentence)
                        if chunk and chunk_len + sentence_len > max_paragraph_chars:
                            final_paragraphs.append(" ".join(chunk))
                            chunk = [sentence]
                            chunk_len = sentence_len
                        else:
                            chunk.append(sentence)
                            chunk_len += sentence_len + 1

                    if chunk:
                        final_paragraphs.append(" ".join(chunk))

                if getattr(self.valves, "VERBOSE_DEBUG", False) and len(final_paragraphs) != len(paragraphs):
                    print(
                        f"[DEDUP] Fragmenta√ß√£o adicional aplicada: {len(paragraphs)} ‚Üí {len(final_paragraphs)} par√°grafos"
                    )

                return [p for p in final_paragraphs if len(p) > 20]

            # Densidade normal: usar split padr√£o
            if getattr(self.valves, "VERBOSE_DEBUG", False):
                print(
                    f"[DEDUP] Densidade normal ({avg_paragraph_size:.0f} chars/par√°grafo)"
                )
                print(f"[DEDUP] Usando split por \\n\\n: {len(parts)} par√°grafos")

            return [p for p in parts if len(p) > 20]

        # Build context (with optional deduplication and size limit)
        # Get accumulated context from global state or phase results
        raw_context = ""
        if global_state and "accumulated_context" in global_state:
            raw_context = global_state["accumulated_context"]
        elif phase_results:
            # Fallback: get from last phase result
            last_result = phase_results[-1].get("result", {})
            raw_context = last_result.get("accumulated_context", "")
        
        if not raw_context:
            yield "**[INFO]** Nenhum contexto dispon√≠vel para s√≠ntese\n"
            return
        
        _emit_decision_snapshot(step="synthesis", vector={"context_chars": len(raw_context)}, reason="start")
        yield f"**[S√çNTESE]** Contexto bruto: {len(raw_context)} chars\n"

        if self.valves.ENABLE_DEDUPLICATION:
            raw_paragraphs = _paragraphs(raw_context)

            # v4.4: Usar Deduplicator centralizado (mesmo do Orchestrator)
            # S√≠ntese: SEM shuffle (fases n√£o s√£o cronol√≥gicas, ordem √© estrutural)
            deduplicator = Deduplicator(self.valves)
            
            # Usar estrat√©gia espec√≠fica da Synthesis
            algorithm = getattr(self.valves, "SYNTHESIS_DEDUP_ALGORITHM", "mmr")
            model_name = getattr(self.valves, "SYNTHESIS_DEDUP_MODEL", "sentence-transformers/paraphrase-MiniLM-L3-v2")
            threshold = getattr(self.valves, "DEDUP_SIMILARITY_THRESHOLD", 0.85)
            print(f"[S√çNTESE DEDUP] üß† Algoritmo: {algorithm.upper()}")
            print(f"[S√çNTESE DEDUP] üìè Threshold: {threshold}")
            print(f"[S√çNTESE DEDUP] üìä Input: {len(raw_paragraphs)} par√°grafos ‚Üí Target: {self.valves.MAX_DEDUP_PARAGRAPHS}")
            print(f"[S√çNTESE DEDUP] üîç Valves: ENABLE_DEDUPLICATION={self.valves.ENABLE_DEDUPLICATION}")
            print(f"[S√çNTESE DEDUP] üîç MAX_DEDUP_PARAGRAPHS: {self.valves.MAX_DEDUP_PARAGRAPHS}")
            print(f"[S√çNTESE DEDUP] üîç Type: {type(self.valves.MAX_DEDUP_PARAGRAPHS)}")
            
            # Extrair must_terms do contexto para context-aware dedup
            extracted_must_terms = []
            if hasattr(self, '_last_contract') and self._last_contract:
                # Tentar extrair must_terms do contract
                entities = self._last_contract.get("entities", {}).get("canonical", [])
                extracted_must_terms = entities[:5]  # Limitar a 5 termos mais relevantes
            
            dedupe_result = deduplicator.dedupe(
                chunks=raw_paragraphs,
                max_chunks=self.valves.MAX_DEDUP_PARAGRAPHS,
                algorithm=algorithm,
                threshold=threshold,
                preserve_order=self.valves.PRESERVE_PARAGRAPH_ORDER,  # Respeita valve
                preserve_recent_pct=0.0,  # S√≠ntese n√£o preserva recent (processa tudo igual)
                shuffle_older=False,  # SEM shuffle (ordem estrutural, n√£o cronol√≥gica)
                reference_first=False,  # SEM refer√™ncia (processa tudo igual)
                # NOVO: Context-aware parameters
                must_terms=extracted_must_terms,
                enable_context_aware=self.valves.ENABLE_CONTEXT_AWARE_DEDUP,
            )

            deduped_paragraphs = dedupe_result["chunks"]
            deduped_context = "\n\n".join(deduped_paragraphs)

            order_mode = (
                "ordem preservada"
                if self.valves.PRESERVE_PARAGRAPH_ORDER
                else "ordenado por tamanho"
            )
            await _safe_emit(__event_emitter__, f"**[S√çNTESE]** Deduplica√ß√£o ativa ({order_mode}, {dedupe_result['algorithm_used']}): {dedupe_result['original_count']} ‚Üí {dedupe_result['deduped_count']} par√°grafos ({dedupe_result['reduction_pct']:.1f}% redu√ß√£o)\n")
            yield f"**[S√çNTESE]** Deduplica√ß√£o ativa ({order_mode}, {dedupe_result['algorithm_used']}): {dedupe_result['original_count']} ‚Üí {dedupe_result['deduped_count']} par√°grafos ({dedupe_result['reduction_pct']:.1f}% redu√ß√£o)\n"
            await _safe_emit(__event_emitter__, f"**[S√çNTESE]** Tokens economizados: ~{dedupe_result['tokens_saved']}\n")
            yield f"**[S√çNTESE]** Tokens economizados: ~{dedupe_result['tokens_saved']}\n"
        else:
            deduped_context = raw_context
            await _safe_emit(__event_emitter__, f"**[S√çNTESE]** Deduplica√ß√£o desabilitada: usando todo o contexto\n")
            yield f"**[S√çNTESE]** Deduplica√ß√£o desabilitada: usando todo o contexto\n"

        # Aplicar limite de tamanho
        max_chars = self.valves.MAX_CONTEXT_CHARS
        if len(deduped_context) > max_chars:
            # Truncar mantendo par√°grafos completos
            truncated = deduped_context[:max_chars]
            last_paragraph = truncated.rfind("\n\n")
            if last_paragraph > max_chars * 0.8:  # Se n√£o perder muito
                deduped_context = truncated[:last_paragraph]
            else:
                deduped_context = truncated
            await _safe_emit(__event_emitter__, f"**[S√çNTESE]** Contexto truncado: {len(deduped_context)} chars (limite: {max_chars})\n")
            yield f"**[S√çNTESE]** Contexto truncado: {len(deduped_context)} chars (limite: {max_chars})\n"
            _emit_decision_snapshot(step="synthesis", vector={"context_chars": len(deduped_context), "limit": max_chars}, reason="after_truncation")
        else:
            await _safe_emit(__event_emitter__, f"**[S√çNTESE]** Contexto dentro do limite: {len(deduped_context)} chars\n")
            yield f"**[S√çNTESE]** Contexto dentro do limite: {len(deduped_context)} chars\n"
            _emit_decision_snapshot(step="synthesis", vector={"context_chars": len(deduped_context)}, reason="within_limit")
        
        try:
            # Log do contexto que ser√° usado
            if self.valves.DEBUG_LOGGING:
                await _safe_emit(__event_emitter__, f"**[DEBUG]** Contexto para s√≠ntese: {len(deduped_context)} chars\n")
                yield f"**[DEBUG]** Contexto para s√≠ntese: {len(deduped_context)} chars\n"
                await _safe_emit(__event_emitter__, f"**[DEBUG]** Primeiros 200 chars: {deduped_context[:200]}...\n")
                yield f"**[DEBUG]** Primeiros 200 chars: {deduped_context[:200]}...\n"

            # Coletar estat√≠sticas para incluir no prompt
            # Get scraped cache from global state or phase results
            scraped_cache = {}
            if global_state and "scraped_cache" in global_state:
                scraped_cache = global_state["scraped_cache"]
            elif phase_results:
                # Fallback: get from last phase result
                last_result = phase_results[-1].get("result", {})
                scraped_cache = last_result.get("scraped_cache", {})
            
            total_urls = len(scraped_cache)
            total_phases = len(phase_results)
            domains = set()
            for url in scraped_cache:
                try:
                    domain = url.split("/")[2]
                    domains.add(domain)
                except:
                    pass

            # Compactar HINTS dos Analistas (por fase)
            def _txt(x):
                if isinstance(x, dict):
                    return (
                        x.get("texto")
                        or x.get("text")
                        or x.get("claim")
                        or x.get("descricao")
                        or str(x)
                    )
                return str(x)

            hints_lines = []
            for idx, ph in enumerate(phase_results, 1):
                analysis = ph.get("analysis", {}) or {}
                summary = (analysis.get("summary") or "").strip()
                facts = analysis.get("facts", []) or []
                lacunas = analysis.get("lacunas", []) or []
                judgement = ph.get("judgement", {}) or {}
                verdict = judgement.get("verdict") or ph.get("verdict")
                reasoning = judgement.get("reasoning") or ph.get("reasoning")

                hints_lines.append(f"[FASE {idx}] {ph.get('query','')}")
                if summary:
                    hints_lines.append(f"- Resumo: {summary}")
                if facts:
                    # pegar at√© 3
                    for f in facts[:3]:
                        hints_lines.append(f"- Fato: {_txt(f)}")
                if lacunas:
                    for l in lacunas[:3]:
                        hints_lines.append(f"- Lacuna: {_txt(l)}")
                if verdict:
                    hints_lines.append(f"- Veredito: {verdict}")
                if reasoning:
                    hints_lines.append(f"- Justificativa: {reasoning}")
                hints_lines.append("")
            analyst_hints = "\n".join(hints_lines)[
                :4000
            ]  # limitar tamanho para seguran√ßa

            # USAR CONTEXTO CENTRALIZADO j√° detectado no in√≠cio do pipe
            if not self._detected_context:
                self._detected_context = await self._detect_unified_context(
                    user_msg, body
                )
            research_context = self._detected_context

            # Extrair KEY_QUESTIONS e RESEARCH_OBJECTIVES do detected_context
            key_questions = research_context.get("key_questions", [])
            research_objectives = research_context.get("research_objectives", [])

            # Extrair informa√ß√µes do contract (entidades, objetivos de fase)
            contract_entities = []
            phase_objectives = []
            if self._last_contract:
                entities = self._last_contract.get("entities", {})
                canonical = entities.get("canonical", [])
                aliases = entities.get("aliases", [])
                contract_entities = list(
                    dict.fromkeys(canonical + aliases)
                )  # Remove duplicatas

                # Coletar objetivos de cada fase
                for idx, phase in enumerate(self._last_contract.get("fases", []), 1):
                    phase_name = phase.get("name", f"Fase {idx}")
                    phase_obj = phase.get("objetivo", "")
                    if phase_obj:
                        phase_objectives.append(f"{idx}. {phase_name}: {phase_obj}")

            # REFACTORED (v4.3.1 - P1D): Usar fun√ß√£o consolidada para construir se√ß√µes
            sections = _build_synthesis_sections(
                key_questions=key_questions,
                research_objectives=research_objectives,
                entities=contract_entities,
                contract=self._last_contract or {},
            )

            # sections is a string, not a dict
            sections_text = sections

            # UMA √öNICA chamada ao LLM com prompt adaptativo baseado no contexto
            synthesis_prompt = f"""Voc√™ √© um analista executivo especializado em criar relat√≥rios executivos completos e narrativos.

**QUERY ORIGINAL DO USU√ÅRIO:**
{user_msg}

**PERFIL ADAPTATIVO:** {research_context['perfil_descricao']}
**OBJETIVO ADAPTATIVO:** Criar um relat√≥rio executivo profissional no estilo {research_context['estilo']}, rico em {research_context['foco_detalhes']}, baseado no contexto de pesquisa fornecido sobre {research_context['tema_principal']}.

{sections_text}
‚ö†Ô∏è **INSTRU√á√ïES CR√çTICAS DE S√çNTESE:**
1. **RESPONDA √ÄS KEY QUESTIONS**: O relat√≥rio DEVE responder diretamente √†s perguntas decis√≥rias listadas acima. Estruture se√ß√µes para responder cada uma.
2. **ALCANCE OS RESEARCH OBJECTIVES**: Cada objetivo de pesquisa final deve ser explicitamente endere√ßado com an√°lise e evid√™ncias.
3. **CUBRA TODAS AS ENTIDADES**: O relat√≥rio DEVE analisar TODAS as entidades espec√≠ficas listadas acima. Crie se√ß√µes/subse√ß√µes dedicadas para cada uma.
4. **ENTREGUE OS OBJETIVOS DAS FASES**: Cada objetivo de fase deve ser claramente respondido com evid√™ncias do contexto.
3. **AN√ÅLISE PROFUNDA**: Examine TODO o contexto fornecido (evid√™ncias, URLs, trechos) - identifique {research_context['foco_detalhes']}
4. **INTEGRA√á√ÉO ESTRAT√âGICA**: Integre os HINTS dos analistas (resumos, fatos e lacunas por fase) com o contexto principal
5. **NARRATIVA ESPEC√çFICA**: Crie um relat√≥rio {research_context['estilo']} sobre {research_context['tema_principal']}
6. **DADOS ESPEC√çFICOS**: Inclua n√∫meros, m√©tricas, projetos, tecnologias e indicadores relevantes para {research_context['tema_principal']}
7. **COBERTURA COMPLETA**: Para cada entidade espec√≠fica (empresa, produto, pessoa), detalhe:
   - Hist√≥rico e presen√ßa no mercado
   - Escopo de servi√ßos/produtos
   - Posicionamento e diferenciais
   - M√©tricas e indicadores (quando dispon√≠veis)
   - Cita√ß√µes e fontes (URLs)
8. **ESTRUTURA ESPECIALIZADA**: Use as se√ß√µes sugeridas: {', '.join(research_context.get('secoes_sugeridas', ['Resumo', 'An√°lise', 'Conclus√µes']))}
9. **FONTES ESPEC√çFICAS**: Cite fontes oficiais e t√©cnicas quando relevante (URLs entre par√™nteses)
10. **S√çNTESE ESTRAT√âGICA**: Integre informa√ß√µes sem repeti√ß√£o, focando em aspectos √∫nicos e {research_context['foco_detalhes']}
11. **FORMATO PROFISSIONAL**: Use Markdown narrativo com se√ß√µes bem estruturadas no estilo {research_context['estilo']}
12. **PRIORIZE DETALHAMENTO SOBRE BREVIDADE**: Prefira um relat√≥rio rico e detalhado a um gen√©rico e curto; use TODO o contexto dispon√≠vel

**ESTAT√çSTICAS DA PESQUISA:**
- Fases executadas: {total_phases}
- URLs analisadas: {total_urls}
- Dom√≠nios consultados: {len(domains)}
- Contexto processado: {len(deduped_context):,} caracteres

**HINTS DOS ANALISTAS (por fase):**
{analyst_hints}

**ESTRUTURA ADAPTATIVA BASEADA NO CONTEXTO DETECTADO:**

# üìã Relat√≥rio Executivo - {research_context['tema_principal']}

## üéØ Resumo Executivo
[Par√°grafo {research_context['estilo']} com vis√£o {research_context['foco_detalhes']} sobre {research_context['tema_principal']}]

## üîç Principais Descobertas
[An√°lise {research_context['estilo']} integrando as descobertas mais importantes sobre {research_context['tema_principal']}]

**DIRETRIZES ADAPTATIVAS PARA {research_context['tema_principal'].upper()}:**
- **FOCO**: {research_context['foco_detalhes']}
- **ESTILO**: {research_context['estilo']}
- **SE√á√ïES SUGERIDAS**: {', '.join(research_context.get('secoes_sugeridas', ['Resumo', 'An√°lise', 'Conclus√µes']))}
- **TOM**: Use linguagem apropriada para {research_context['perfil_descricao']}
- **N√çVEL DE DETALHE**: Seja espec√≠fico sobre dados, m√©tricas e evid√™ncias relevantes para {research_context['tema_principal']}
- **CONTEXTO**: Relacione descobertas com tend√™ncias e aspectos espec√≠ficos do setor

Agora, crie o relat√≥rio executivo completo baseado no contexto abaixo:

---

**CONTEXTO DE PESQUISA:**

{deduped_context}

---

**RELAT√ìRIO EXECUTIVO:**"""

            # P1: Exemplo de BOA vs M√Å s√≠ntese para calibrar sa√≠da do LLM
            synthesis_prompt += """
üí° EXEMPLO DE BOA vs M√Å S√çNTESE:
Query: "Volume de mercado de headhunting Brasil"
Key_question: "Qual volume anual?"
‚ùå M√Å s√≠ntese (gen√©rica):
"O mercado de headhunting no Brasil √© significativo e tem crescido nos √∫ltimos anos."
‚úÖ BOA s√≠ntese (espec√≠fica):
"O mercado brasileiro de executive search movimentou R$450-500M em 2023 (fonte: Relat√≥rio ABRH),
crescimento de 12% vs 2022. Spencer Stuart lidera com ~25% de participa√ß√£o (fonte: Valor Econ√¥mico),
seguida por Heidrick (18%) e Flow Executive (15%)."
‚Üí Diferen√ßa: n√∫meros concretos, fontes, nomes de players
"""

            # ‚úÖ GATE PREVENTIVO: Avisar sobre prompt gigante
            prompt_chars = len(synthesis_prompt)
            prompt_tokens_est = prompt_chars // 4  # Estimativa conservadora

            if prompt_tokens_est > 40000:  # ~160k chars
                yield f"**[‚ö†Ô∏è AVISO]** Prompt muito grande ({prompt_tokens_est:,} tokens estimados)!\n"
                yield f"**[SUGEST√ÉO]** S√≠ntese pode levar 5-10 minutos. Aguarde...\n"
                yield f"**[CONFIG]** Se houver timeout, aumente nas Valves:\n"
                yield f"   - LLM_TIMEOUT_SYNTHESIS (atual: {self.valves.LLM_TIMEOUT_SYNTHESIS}s)\n"
                yield f"   - HTTPX_READ_TIMEOUT (atual: {self.valves.HTTPX_READ_TIMEOUT}s)\n"
                yield f"**[ALTERNATIVA]** Reduza MAX_DEDUP_PARAGRAPHS (atual: {self.valves.MAX_DEDUP_PARAGRAPHS})\n"
                yield f"\n"

            if self.valves.DEBUG_LOGGING or self.valves.VERBOSE_DEBUG:
                yield f"**[DEBUG]** Prompt de s√≠ntese: {prompt_chars:,} chars (~{prompt_tokens_est:,} tokens estimados)\n"
                if prompt_tokens_est > 12000:
                    yield f"**[AVISO]** Prompt grande (>{prompt_tokens_est:,} tokens) pode causar truncamento ou resposta gen√©rica!\n"

            # Fazer UMA √∫nica chamada ao LLM para gerar o relat√≥rio completo
            # Usar modelo espec√≠fico para s√≠ntese se configurado (pode ser mais capaz)
            synthesis_model = self.valves.LLM_MODEL_SYNTHESIS or self.valves.LLM_MODEL
            llm = _get_llm(self.valves, model_name=synthesis_model)

            # Par√¢metros seguros para GPT-5/O1
            base_synthesis_params = {"temperature": 0.3}
            generation_kwargs = get_safe_llm_params(
                synthesis_model, base_synthesis_params
            )
            timeout_synthesis = self.valves.LLM_TIMEOUT_SYNTHESIS
            
            # üîß AUTO-ADJUST timeout for large context
            context_size = len(deduped_context)
            if context_size > 100000:  # >100k chars
                # Scale timeout based on context size
                min_timeout = 600  # 10 minutes minimum for large context
                max_timeout = 1800  # 30 minutes maximum
                # Linear scaling: 100k chars = 10min, 200k chars = 20min, 300k+ chars = 30min
                scaled_timeout = min(max_timeout, min_timeout + int((context_size - 100000) / 10000 * 600))
                timeout_synthesis = max(timeout_synthesis, scaled_timeout)
                if self.valves.DEBUG_LOGGING:
                    await _safe_emit(__event_emitter__, f"**[DEBUG]** Auto-ajuste timeout: {self.valves.LLM_TIMEOUT_SYNTHESIS}s ‚Üí {timeout_synthesis}s (contexto: {context_size:,} chars)\n")
                    yield f"**[DEBUG]** Auto-ajuste timeout: {self.valves.LLM_TIMEOUT_SYNTHESIS}s ‚Üí {timeout_synthesis}s (contexto: {context_size:,} chars)\n"

            # Log do modelo sendo usado
            if self.valves.DEBUG_LOGGING:
                await _safe_emit(__event_emitter__, f"**[DEBUG]** Modelo de s√≠ntese: {synthesis_model} (params: {generation_kwargs})\n")
                yield f"**[DEBUG]** Modelo de s√≠ntese: {synthesis_model} (params: {generation_kwargs})\n"

            _emit_decision_snapshot(step="synthesis", vector={"model": synthesis_model, "params": generation_kwargs, "timeout": timeout_synthesis}, reason="llm_call_start")
            out = await _safe_llm_run_with_retry(
                llm,
                synthesis_prompt,
                generation_kwargs,
                timeout=timeout_synthesis,
                max_retries=1,
            )
            if not out or not out.get("replies"):
                raise ValueError("LLM vazio na s√≠ntese final")
            report = (out["replies"][0] or "").strip()
            _emit_decision_snapshot(step="synthesis", vector={"ok": bool(report), "chars": len(report or "")}, reason="llm_call_end")
            if not report:
                raise ValueError("Relat√≥rio vazio na s√≠ntese final")
            await _safe_emit(__event_emitter__, f"\n{report}\n")
            yield f"\n{report}\n"

            # Generate PDF if enabled
            if getattr(self.valves, 'ENABLE_PDF_EXPORT', True):
                try:
                    # Convert markdown to HTML for PDF generation
                    import markdown
                    html_content = markdown.markdown(report, extensions=['tables', 'fenced_code'])
                    
                    # Add basic CSS styling
                    html_with_style = f"""
                    <!DOCTYPE html>
                    <html>
                    <head>
                        <meta charset="utf-8">
                        <title>Research Report</title>
                        <style>
                            body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
                            h1, h2, h3 {{ color: #333; }}
                            h1 {{ border-bottom: 2px solid #333; padding-bottom: 10px; }}
                            h2 {{ border-bottom: 1px solid #ccc; padding-bottom: 5px; }}
                            table {{ border-collapse: collapse; width: 100%; }}
                            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                            th {{ background-color: #f2f2f2; }}
                            code {{ background-color: #f4f4f4; padding: 2px 4px; border-radius: 3px; }}
                            blockquote {{ border-left: 4px solid #ccc; margin: 0; padding-left: 20px; }}
                        </style>
                    </head>
                    <body>
                        {html_content}
                    </body>
                    </html>
                    """
                    
                    pdf_b64 = self._generate_pdf_base64(html_with_style, "Research Report")
                    if pdf_b64:
                        pdf_link = f'<a href="data:application/pdf;base64,{pdf_b64}" download="report.pdf">Download PDF</a>'
                        await _safe_emit(__event_emitter__, f"\n\n**[PDF]** {pdf_link}\n")
                        yield f"\n\n**[PDF]** {pdf_link}\n"
                    else:
                        await _safe_emit(__event_emitter__, "\n\n**[PDF]** Export failed (weasyprint not installed)\n")
                        yield "\n\n**[PDF]** Export failed (weasyprint not installed)\n"
                except Exception as e:
                    await _safe_emit(__event_emitter__, f"\n\n**[PDF]** Export failed: {e}\n")
                    yield f"\n\n**[PDF]** Export failed: {e}\n"

            # Estat√≠sticas avan√ßadas
            # Get scraped cache from global state or phase results
            scraped_cache = {}
            if global_state and "scraped_cache" in global_state:
                scraped_cache = global_state["scraped_cache"]
            elif phase_results:
                # Fallback: get from last phase result
                last_result = phase_results[-1].get("result", {})
                scraped_cache = last_result.get("scraped_cache", {})
            
            total_urls = len(scraped_cache)
            total_phases = len(phase_results)
            domains = set()
            for url in scraped_cache:
                try:
                    domain = url.split("/")[2]
                    domains.add(domain)
                except:
                    pass

            await _safe_emit(__event_emitter__, f"\n---\n**üìä Estat√≠sticas da Pesquisa:** Fases={total_phases}, URLs √∫nicas={total_urls}, Dom√≠nios={len(domains)}, Contexto={len(raw_context):,} chars\n")
            yield f"\n---\n**üìä Estat√≠sticas da Pesquisa:** Fases={total_phases}, URLs √∫nicas={total_urls}, Dom√≠nios={len(domains)}, Contexto={len(raw_context):,} chars\n"

        except Exception as e:
            error_msg = str(e)
            is_timeout = (
                "timeout" in error_msg.lower() or "exceeded" in error_msg.lower()
            )

            if is_timeout:
                # üîß FIX: Calcular timeout HTTP real usado (n√£o mostrar PLANNER_REQUEST_TIMEOUT que √© irrelevante)
                effective_http_timeout = max(60, int(timeout_synthesis - 30))

                await _safe_emit(__event_emitter__, f"**[ERRO]** S√≠ntese completa falhou: {e}\n")
                yield f"**[ERRO]** S√≠ntese completa falhou: {e}\n"
                await _safe_emit(__event_emitter__, f"**[DICA]** Contexto muito grande ({len(deduped_context):,} chars). Sugest√µes:\n")
                yield f"**[DICA]** Contexto muito grande ({len(deduped_context):,} chars). Sugest√µes:\n"
                # Get max timeout value safely (Pydantic v1/v2 compatibility)
                try:
                    max_timeout = getattr(self.valves.__fields__['LLM_TIMEOUT_SYNTHESIS'], 'field_info', {}).get('extra', {}).get('le', 1800)
                except (AttributeError, KeyError):
                    max_timeout = 1800  # fallback
                await _safe_emit(__event_emitter__, f"  - Aumente LLM_TIMEOUT_SYNTHESIS nas valves (atual: {timeout_synthesis}s, m√°x: {max_timeout}s)\n")
                yield f"  - Aumente LLM_TIMEOUT_SYNTHESIS nas valves (atual: {timeout_synthesis}s, m√°x: {max_timeout}s)\n"
                await _safe_emit(__event_emitter__, f"  - üîç **Diagn√≥stico atual:**\n")
                yield f"  - üîç **Diagn√≥stico atual:**\n"
                await _safe_emit(__event_emitter__, f"    ‚Ä¢ timeout_synthesis (asyncio): {timeout_synthesis}s\n")
                yield f"    ‚Ä¢ timeout_synthesis (asyncio): {timeout_synthesis}s\n"
                await _safe_emit(__event_emitter__, f"    ‚Ä¢ request_timeout (HTTP): {effective_http_timeout}s (margem de 30s)\n")
                yield f"    ‚Ä¢ request_timeout (HTTP): {effective_http_timeout}s (margem de 30s)\n"
                await _safe_emit(__event_emitter__, f"    ‚Ä¢ HTTPX_READ_TIMEOUT (base): {self.valves.HTTPX_READ_TIMEOUT}s (n√£o usado na s√≠ntese)\n")
                yield f"    ‚Ä¢ HTTPX_READ_TIMEOUT (base): {self.valves.HTTPX_READ_TIMEOUT}s (n√£o usado na s√≠ntese)\n"
                await _safe_emit(__event_emitter__, f"  - ‚ö†Ô∏è **Se a resposta apareceu na OpenAI mas n√£o aqui:** pode ser timeout de conex√£o HTTP. Verifique logs para '[LLM_CALL]'.\n")
                yield f"  - ‚ö†Ô∏è **Se a resposta apareceu na OpenAI mas n√£o aqui:** pode ser timeout de conex√£o HTTP. Verifique logs para '[LLM_CALL]'.\n"
                await _safe_emit(__event_emitter__, f"  - Ou reduza MAX_DEDUP_PARAGRAPHS (atual: {self.valves.MAX_DEDUP_PARAGRAPHS}) para diminuir contexto\n")
                yield f"  - Ou reduza MAX_DEDUP_PARAGRAPHS (atual: {self.valves.MAX_DEDUP_PARAGRAPHS}) para diminuir contexto\n"
                await _safe_emit(__event_emitter__, f"  - Ou aumente DEDUP_SIMILARITY_THRESHOLD (atual: {self.valves.DEDUP_SIMILARITY_THRESHOLD}) para deduplicar mais agressivamente\n")
                yield f"  - Ou aumente DEDUP_SIMILARITY_THRESHOLD (atual: {self.valves.DEDUP_SIMILARITY_THRESHOLD}) para deduplicar mais agressivamente\n"
            else:
                await _safe_emit(__event_emitter__, f"**[ERRO]** S√≠ntese completa falhou: {e}\n")
                yield f"**[ERRO]** S√≠ntese completa falhou: {e}\n"

            await _safe_emit(__event_emitter__, f"**[FALLBACK]** Contexto completo ({len(raw_context):,} chars) dispon√≠vel para an√°lise manual.\n")
            yield f"**[FALLBACK]** Contexto completo ({len(raw_context):,} chars) dispon√≠vel para an√°lise manual.\n"

    def _validate_pipeline_state(self) -> None:
        """Valida consist√™ncia do estado interno do pipeline

        Verifica e corrige inconsist√™ncias entre estado interno.
        Chama este m√©todo no in√≠cio de pipe() para prevenir estados inv√°lidos.
        """
        # Check context lock consistency
        if hasattr(self, '_context_locked') and self._context_locked and not self._detected_context:
            logger.warning("Context locked but no detected context - resetting lock")
            self._context_locked = False

        # Check contract consistency
        if self._last_contract:
            if not isinstance(self._last_contract, dict):
                logger.error("Invalid contract type, clearing")
                self._last_contract = None
            elif "fases" not in self._last_contract:
                logger.error("Contract missing fases, clearing")
                self._last_contract = None
            elif not isinstance(self._last_contract.get("fases"), list):
                logger.error("Contract fases is not a list, clearing")
                self._last_contract = None

        # Check profile sync
        if self._detected_context and hasattr(self, '_intent_profile') and self._intent_profile:
            detected = self._detected_context.get("perfil")
            if detected and detected != self._intent_profile:
                logger.warning(f"Profile mismatch syncing to detected: {detected}")
                self._intent_profile = detected

    def _resolve_tool_deterministic(
        self,
        name_hint: str,
        available_keys: List[str],
        fallback_hints: List[str] = None,
    ) -> Optional[str]:
        """Resolve tool deterministically: nome exato > prefixo > substring"""
        # 1. Nome exato (via valves preferred tools)
        preferred = getattr(self.valves, "PREFERRED_TOOLS", {})
        if name_hint in preferred and preferred[name_hint] in available_keys:
            return preferred[name_hint]

        # 2. Nome exato direto
        if name_hint in available_keys:
            return name_hint

        # 3. Prefixo (starts with)
        for key in available_keys:
            if key.startswith(name_hint):
                return key

        # 4. Substring (contains)
        if fallback_hints:
            for hint in fallback_hints:
                for key in available_keys:
                    if hint.lower() in key.lower():
                        return key

        return None

    def _resolve_tools(self, __tools__: Dict[str, Dict[str, Any]]):
        """Resolve tools using deterministic heuristics"""
        if not __tools__:
            raise RuntimeError(
                "No tools available. Please configure tools in OpenWebUI."
            )

        keys = list(__tools__.keys())
        logger.debug("Available tools: %s", keys)

        # Resolu√ß√£o determin√≠stica: nome exato > prefixo > substring
        discover_key = self._resolve_tool_deterministic(
            "discovery", keys, ["discover", "search"]
        )
        scrape_key = self._resolve_tool_deterministic(
            "scraper", keys, ["scrape", "scraper"]
        )
        context_reducer_key = self._resolve_tool_deterministic(
            "context_reducer", keys, ["reduce", "context"]
        )

        # Check if we found the required tools
        if not discover_key:
            raise RuntimeError(f"Discovery tool not found. Available tools: {keys}")
        if not scrape_key:
            raise RuntimeError(f"Scraper tool not found. Available tools: {keys}")

        logger.debug("Using discovery tool: %s", discover_key)
        logger.debug("Using scraper tool: %s", scrape_key)
        if context_reducer_key:
            logger.debug("Using context reducer tool: %s", context_reducer_key)

        # Get the callables
        d_callable_raw = __tools__[discover_key]["callable"]
        s_callable = __tools__[scrape_key]["callable"]
        cr_callable = (
            __tools__[context_reducer_key]["callable"] if context_reducer_key else None
        )

        # Create wrapper for discovery tool to handle extra parameters
        async def d_callable_wrapper(
            query: str,
            must_terms: Optional[List[str]] = None,
            avoid_terms: Optional[List[str]] = None,
            time_hint: Optional[Dict[str, Any]] = None,
            lang_bias: Optional[List[str]] = None,
            geo_bias: Optional[List[str]] = None,
            source_bias: Optional[str] = None,
            min_domains: Optional[int] = None,
            official_domains: Optional[List[str]] = None,
            profile: Optional[str] = None,
            phase_objective: Optional[str] = None,
            **kwargs,
        ):
            """Wrapper to handle discovery tool parameters"""
            # Extract time_hint parameters
            after = None
            before = None
            if time_hint:
                after = time_hint.get("after")
                before = time_hint.get("before")

            # Build kwargs dynamically based on the callable's supported parameters
            import inspect

            supported_params = set()
            try:
                supported_params = set(
                    inspect.signature(d_callable_raw).parameters.keys()
                )
            except Exception as e:
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[WARNING][D_WRAPPER] Could not inspect discovery tool signature: {e}")
                # Assume basic params if inspection fails
                supported_params = {
                    "query",
                    "profile",
                    "after",
                    "before",
                    "whitelist",
                    "pages_per_slice",
                }

            # Check if tool supports minimum required params
            if "query" not in supported_params:
                logger.error("[D_WRAPPER] Discovery tool does not support 'query' parameter!")
                return {"urls": []}

            base_kwargs = {
                "query": query,
                "profile": profile or "general",
                "after": after,
                "before": before,
                "whitelist": "",
                "pages_per_slice": 2,
            }

            rails_kwargs = {
                "must_terms": must_terms,
                "avoid_terms": avoid_terms,
                "time_hint": time_hint,
                "lang_bias": lang_bias,
                "geo_bias": geo_bias,
                "min_domains": min_domains,
                "official_domains": official_domains,
                "phase_objective": phase_objective,
            }

            # Filter kwargs to only what the callable accepts
            final_kwargs = {}
            for k, v in base_kwargs.items():
                if k in supported_params:
                    if v is not None or k == "query":
                        final_kwargs[k] = v

            for k, v in rails_kwargs.items():
                if k in supported_params and v is not None:
                    final_kwargs[k] = v

            # Ensure query is never empty
            if not final_kwargs.get("query"):
                logger.error("[D_WRAPPER] Query is empty!")
                return {"urls": []}

            # ===== GRACEFUL DEGRADATION WITH FALLBACK =====
            try:
                result = await d_callable_raw(**final_kwargs)
                if result is None:
                    logger.warning("[D_WRAPPER] Discovery tool returned None")
                    return {"urls": []}
                return result
            except TypeError as e:
                # Tool doesn't support advanced parameters - try with basic query only
                if getattr(self.valves, "VERBOSE_DEBUG", False):
                    print(f"[D_WRAPPER] Advanced params failed: {e}")
                    print(f"[D_WRAPPER] Falling back to basic query only")
                
                try:
                    # Fallback: Only pass the essential query parameter
                    basic_kwargs = {"query": query}
                    result = await d_callable_raw(**basic_kwargs)
                    if result is None:
                        logger.warning("[D_WRAPPER] Discovery tool (fallback) returned None")
                        return {"urls": []}
                    
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(f"[D_WRAPPER] Fallback successful: {len(result.get('urls', []))} URLs found")
                    
                    return result
                except Exception as fallback_e:
                    logger.error(f"[D_WRAPPER] Even basic query failed: {fallback_e}")
                    return {"urls": []}
            except Exception as e:
                logger.error(f"[D_WRAPPER] Unexpected exception calling discovery tool: {e}")
                return {"urls": []}

        return d_callable_wrapper, s_callable, cr_callable

    async def _extract_contract_from_history(self, body: dict) -> Optional[dict]:
        """Extract contract from message history when 'siga' is called."""
        try:
            messages = body.get("messages", [])

            # Find the last assistant message that contains a plan
            for msg in reversed(messages):
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(f"[DEBUG] Checking message content: {content[:200]}...")

                    # Look for plan markers (both SDK and Manual formats)
                    has_fase = "Fase 1/" in content and "Objetivo:" in content
                    has_plano = "üìã Plano" in content
                    if getattr(self.valves, "VERBOSE_DEBUG", False):
                        print(f"[DEBUG] Plan markers: has_fase={has_fase}, has_plano={has_plano}")

                    if has_fase or has_plano:
                        # Try to extract contract using LLM
                        planner = PlannerLLM(self.valves)

                        # Create a prompt to extract the contract from the rendered plan
                        extract_prompt = f"""Extraia o contrato JSON do plano renderizado abaixo.
                        
PLANO RENDERIZADO:
{content}

Retorne APENAS o JSON do contrato no formato:
{{
  "intent_profile": "company_profile",
  "intent": "resumo do objetivo",
  "entities": {{"canonical": ["entidade principal"], "aliases": []}},
  "phases": [
    {{
      "name": "nome da fase",
      "objective": "objetivo da fase",
      "seed_query": "<3-6 palavras, sem operadores>",
      "must_terms": ["termo1", "termo2"],
      "avoid_terms": ["ru√≠do"],
      "time_hint": {{"recency": "1y", "strict": false}},
      "source_bias": ["oficial", "primaria", "secundaria"],
      "evidence_goal": {{"official_or_two_independent": true, "min_domains": 3}},
      "lang_bias": ["pt-BR", "en"],
      "geo_bias": ["BR", "global"]
    }}
  ],
  "quality_rails": {{"min_unique_domains": 2, "need_official_or_two_independent": true}},
  "budget": {{"max_rounds": 2}}
}}

INSTRU√á√ïES:
- Extraia o objetivo principal do plano
- Identifique as fases e seus objetivos
- Se faltar seed_query, gere com 3-6 palavras (sem operadores)
- Mantenha campos obrigat√≥rios com valores padr√£o
- Retorne APENAS o JSON v√°lido"""

                        # Filter params for GPT-5/O1 compatibility
                        safe_extract_params = get_safe_llm_params(
                            planner.model_name, planner.generation_kwargs
                        )
                        result = await _safe_llm_run_with_retry(
                            planner.llm,
                            extract_prompt,
                            safe_extract_params,
                            timeout=planner.valves.LLM_TIMEOUT_DEFAULT,
                            max_retries=1,
                        )
                        if result:
                            try:
                                contract = _extract_json_from_text(result)
                                if (
                                    contract
                                    and "phases" in contract
                                    and len(contract["phases"]) > 0
                                ):
                                    # Validate that we have the essential fields
                                    for phase in contract["phases"]:
                                        if not phase.get("objective") or not phase.get("seed_query"):
                                            if not phase.get("seed_query") and phase.get("objective"):
                                                try:
                                                    reask = f"Gere apenas uma seed query (3-6 palavras, sem operadores) para: '{phase.get('objective','')}'. Retorne s√≥ a seed."
                                                    safe_reask = get_safe_llm_params(
                                                        planner.model_name,
                                                        {"temperature": 0.2},
                                                    )
                                                    re = await _safe_llm_run_with_retry(
                                                        planner.llm,
                                                        reask,
                                                        safe_reask,
                                                        timeout=planner.valves.LLM_TIMEOUT_DEFAULT,
                                                        max_retries=1,
                                                    )
                                                    if re and re.get("replies"):
                                                        candidate = (
                                                            re["replies"][0]
                                                            .strip()
                                                            .strip('"')
                                                            .strip("'")
                                                        )
                                                        for op in [
                                                            "site:",
                                                            "filetype:",
                                                            "after:",
                                                            "before:",
                                                            "AND",
                                                            "OR",
                                                            '"',
                                                            "'",
                                                        ]:
                                                            candidate = candidate.replace(op, " ")
                                                        words = candidate.split()
                                                        if 3 <= len(words) <= 6:
                                                            phase["seed_query"] = " ".join(words)
                                                except Exception:
                                                    pass
                                            if not phase.get("objective"):
                                                phase["objective"] = f"An√°lise: {phase.get('seed_query', 't√≥pico')}"
                                    return contract
                            except (json.JSONDecodeError, KeyError, ValueError) as e:
                                logger.warning(f"Failed to parse extracted contract: {e}")
                            except Exception as e:
                                logger.error(f"Unexpected error parsing contract: {e}")
                                raise ContractValidationError(f"Contract parsing failed: {e}") from e

            return None
        except ContractValidationError:
            raise  # Re-raise specific errors
        except Exception as e:
            logger.warning(f"Failed to extract contract from history: {e}")
            return None

    async def _detect_unified_context(
        self, user_query: str, body: dict
    ) -> Dict[str, Any]:
        """DETEC√á√ÉO UNIFICADA DE CONTEXTO - Apenas LLM (heur√≠stica removida)

        Analisa a consulta do usu√°rio e hist√≥rico de mensagens para determinar:
        - Setor/ind√∫stria (10+ setores: sa√∫de, tech, finan√ßas, direito, etc.)
        - Tipo de pesquisa (acad√™mica, regulat√≥ria, t√©cnica, estrat√©gica, not√≠cias)
        - Perfil apropriado (company_profile, regulation_review, technical_spec, etc.)
        - Metadados adaptativos (estilo, foco, se√ß√µes sugeridas)

        ‚úÖ SIMPLIFICADO: Usa apenas LLM (heur√≠stica removida - era marginal)

        Returns:
            Dict com: setor, tipo, perfil, perfil_descricao, estilo, foco_detalhes,
                     tema_principal, secoes_sugeridas, detec√ß√£o_confianca, fonte_deteccao
        """

        # Preparar contexto do hist√≥rico
        text_sample = user_query.lower()
        messages = body.get("messages", [])
        if messages:
            for msg in messages[:-1]:  # Excluir √∫ltima mensagem (query atual)
                content = msg.get("content", "")
                if content:
                    text_sample += " " + content.lower()

        # ===== LLM √öNICO (heur√≠stica removida) =====
        contexto_enriquecido = None
        try:
            from datetime import datetime

            current_date = datetime.now().strftime("%Y-%m-%d")

            # Prompt JSON-only sem CoT exposto (anti-recusa/anti-disclaimer)
            detect_prompt = f"""Voc√™ √© um consultor de estrat√©gia s√™nior.

Pense passo a passo INTERNAMENTE, mas N√ÉO exponha o racioc√≠nio.
Retorne SOMENTE JSON v√°lido no schema abaixo.

‚ö†Ô∏è IMPORTANTE:
- N√ÉO use markdown ou c√≥digo fence (```json)
- N√ÉO escreva nada fora do JSON
- N√ÉO inclua pedidos de desculpa, men√ß√µes a pol√≠ticas ou resumos narrativos
- Apenas JSON v√°lido conforme schema

CONSULTA: {user_query}

CONTEXTO DO HIST√ìRICO:
{text_sample[:1000]}

Data atual: {current_date}

TAREFA:
Analise a consulta e produza:
- setor_principal (espec√≠fico, n√£o "geral")
- tipo_pesquisa (acad√™mica, mercado, t√©cnica, regulat√≥ria, not√≠cias)
- perfil_sugerido: CRITICAL - escolha baseado no OBJETIVO PRINCIPAL:
  * company_profile: an√°lise de MERCADO/EMPRESAS/COMPETI√á√ÉO/NEG√ìCIOS (ex: "estudo de mercado", "an√°lise competitiva", "players", "reputa√ß√£o")
  * regulation_review: an√°lise REGULAT√ìRIA/LEGAL/COMPLIANCE (ex: "marco legal", "normas", "regulamenta√ß√£o")
  * technical_spec: an√°lise T√âCNICA/OPERACIONAL/IMPLEMENTA√á√ÉO (ex: "arquitetura", "stack t√©cnico", "processos operacionais")
  * literature_review: revis√£o ACAD√äMICA/CIENT√çFICA (ex: "estado da arte", "revis√£o sistem√°tica", "papers")
  * history_review: an√°lise HIST√ìRICA/TEMPORAL (ex: "evolu√ß√£o hist√≥rica", "contexto cultural")
  
- 5-10 key_questions (perguntas de DECIS√ÉO que precisam resposta, n√£o queries de busca)
- entities_mentioned (APENAS empresas/produtos/pessoas/marcas espec√≠ficas mencionadas EXPLICITAMENTE, incluir aliases)
- research_objectives (3-5 objetivos de pesquisa espec√≠ficos e mensur√°veis)

SCHEMA JSON:
{{
  "setor_principal": "string",
  "tipo_pesquisa": "string", 
  "perfil_sugerido": "string",
  "key_questions": ["string"],
  "entities_mentioned": [{{"canonical": "string", "aliases": ["string"]}}],
  "research_objectives": ["string"],
  "perfil_descricao": "string",
  "estilo": "string",
  "foco_detalhes": "string",
  "tema_principal": "string",
  "secoes_sugeridas": ["string"],
  "detec√ß√£o_confianca": 0.85,
  "fonte_deteccao": "llm"
}}"""

            # Ensure LLM components are initialized (handles case with no valves override)
            if self.analyst is None:
                self.analyst = AnalystLLM(self.valves)
            if self.judge is None:
                self.judge = JudgeLLM(self.valves)
            if self.planner is None:
                self.planner = PlannerLLM(self.valves)
            if self.deduplicator is None:
                self.deduplicator = Deduplicator(self.valves)

            safe_params = get_safe_llm_params(self.valves.LLM_MODEL, {"temperature": 0.1})
            
            result = await _safe_llm_run_with_retry(
                llm,
                detect_prompt,
                safe_params,
                timeout=self.valves.LLM_TIMEOUT_DEFAULT,
                max_retries=1,
            )

            if result and result.get("replies"):
                try:
                    contexto_enriquecido = _extract_json_from_text(result["replies"][0])
                    if contexto_enriquecido:
                        # Validar campos obrigat√≥rios
                        required_fields = ["setor_principal", "tipo_pesquisa", "perfil_sugerido"]
                        if all(field in contexto_enriquecido for field in required_fields):
                            return contexto_enriquecido
                except Exception as e:
                    logger.warning(f"Failed to parse context detection result: {e}")

        except Exception as e:
            logger.warning(f"Context detection failed: {e}")

        # Fallback: contexto gen√©rico
        return {
            "setor_principal": "geral",
            "tipo_pesquisa": "analise_mercado",
            "perfil_sugerido": "company_profile",
            "key_questions": [f"Quais s√£o os principais aspectos de {user_query}?"],
            "entities_mentioned": [],
            "research_objectives": [f"Analisar {user_query}"],
            "perfil_descricao": "an√°lise de mercado",
            "estilo": "executivo",
            "foco_detalhes": "dados e m√©tricas",
            "tema_principal": user_query,
            "secoes_sugeridas": ["Resumo", "An√°lise", "Conclus√µes"],
            "detec√ß√£o_confianca": 0.5,
            "fonte_deteccao": "fallback"
        }